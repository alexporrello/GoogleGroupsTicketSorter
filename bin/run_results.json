{
 "topics": [
  {
   "name": [
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Aaron Friesz (CTR)",
     "body": "Hi,\nWe've recently tried to add and aggregate a new NASA VIIRS dataset to OPeNDAP but ran into a problem. These datasets are in HDF-EOS5 format. We were able to add them individually to OPeNDAP without any apparent problems. However, when we temporally aggregated them, we were only able to request data for the first time period in the aggregation (e.g. time[0:1:0]). If we included any other time periods, for example time[1:1:1] or time[0:1:4], then the return would be empty.\nI noticed that some of the other HDF-EOS5 products that we have properly working in OPeNDAP did not contain the CoreMetadata.0 attributes within the dataset that we were working with. We found that if we removed the CoreMetadata.0 attributes from the HDF files we were working with, that we were able to properly aggregate and extract data from the datasets.\nMy question is, does the HDF5 handler not support the addition of the CoreMetadata.0 attributes? or is there something about the CoreMetadata.0 attributes in this product that's throwing OPeNDAP off?\nI've added a couple files to my dropbox (https://www.dropbox.com/sh/vn001ssxz42vfvo/AADVkrtA4gDJJ4-PMa_sl1XMa?dl=0)\nThanks,\nAaron\n--\nAaron FrieszGeospatial Data ScientistInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)USGS/EROS CenterSioux Falls, SD 57198\nPhone: 605-594-6526 Email: aaron.fr...@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.]"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Kent Yang",
     "body": "Aaron,\nIf individual HDF-EOS5 file works with the OPeNDAP, that means the HDF5 handler works. The issue is mostly in the aggregation part, the NcML handler, which belongs to the OPeNDAP folks. The information in the CoreMetadata.0 may make the NcML handler fail, that’s my guess. OPeNDAP team should tell you more.\nKent\nFrom: Aaron Friesz (CTR) [mailto:aaron.fr...@usgs.gov]\nSent: Wednesday, June 20, 2018 4:05 PM\nTo: OPeNDAP Support; James Gallagher; Kent Yang\nCc: Schroeder (CTR), Bradley\nSubject: OPeNDAP HDF5 Issue\n- show quoted text -\nThanks,\nAaron\n--\n- show quoted text -\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.]"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Nathan Potter",
     "body": "Hi Aaron,\nI’m having a look at this. I got your DropBox download which contains 3 h5 files.\nCould you also share the NcML file that you used to do the aggregation you described?\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Bradley (Contractor) Schroeder",
     "body": "Hi Nathan,\nThese 3 files were a subset of a larger data set. I modified the ncml file to reflect just these 3 files, and attached it to this email.\nI also verified that this subset of 3 files gives us the same functionality as the full data set.\nBrad\nBrad Schroeder\nSenior Software Engineer\nStinger Ghaffarian Technologies (SGT)Technical Support Services Contractor to U.S. Geological SurveyEarth Resources Observation & Science Center47914 252nd Street NESioux Falls, South Dakota 57198\n(605) 594-2685\n**Note: I have updated my email address to bschr...@contractor.usgs.gov"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Nathan Potter",
     "body": "Hi Brad & Aaron,\nI out the test data and the ncml file here:\nhttp://test.opendap.org/opendap/viirs/\nYou can get the ncml file with this URL:\nhttp://test.opendap.org/opendap/viirs/test_agg.ncml\nAnd it replicates your problem. At least I think it’s your problem, there certainly are problems with that aggregation.\nAnd it’s got a ticket: https://opendap.atlassian.net/browse/HYRAX-792\nI don’t see an obvious NcML thing to correct so I’ll have to look more closely at the issue.\nSincerely,\nNathan\n- show quoted text -\n> <h10v04.ncml>\n- show quoted text -"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Bradley (Contractor) Schroeder",
     "body": "Thanks Nathan,\nYes, your instance looks like it behaves exactly the same as ours is behaving.\nOne other note that Aaron put in the original email, but I want to re-iterate. When we were trouble shooting this issue, Aaron took a few of these .h5 files and made copies with the CoreMetadata.0 removed. Then I created an NcML file that pointed at those 3 modified files. That aggregation of the modified .h5 files works as expected. The only change in the NcML file is it points to different .h5 files.\nBrad\nBrad Schroeder\nSenior Software Engineer\nStinger Ghaffarian Technologies (SGT)Technical Support Services Contractor to U.S. Geological SurveyEarth Resources Observation & Science Center47914 252nd Street NESioux Falls, South Dakota 57198\n(605) 594-2685\n**Note: I have updated my email address to bschr...@contractor.usgs.gov"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Nathan Potter",
     "body": "Hi Brad,\nThanks! I was eyeing that facet of this problem and noticed an interesting anomaly in how the HDF5 files are being interpreted by the hdf5_handler.\nIn the DDS response for one of the HDF5 files:\nhttp://test.opendap.org/opendap/viirs/VNP13A1.A2018145.h10v04.001.2018161235134.h5.dds\nthere are these variables:\nString ArchiveMetadata_0;\nString CoreMetadata_0;\nString StructMetadata_0;\nWhich I believe are being used as an organizational convention for organizing the metadata.\nWhat I noticed is that in the metadata (DAS) response for the same file:\nhttp://test.opendap.org/opendap/viirs/VNP13A1.A2018145.h10v04.001.2018161235134.h5.das\nThe variables have different names:\nArchiveMetadata {\n}\nCoreMetadata {\n}\nStructMetadata {\n}\nI am pretty sure that this is never supposed to happen.\nAnd I wonder if the NcML code the handles the aggregation could be stumbling over this.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Bradley (Contractor) Schroeder",
     "body": "That could be,\nWe have been a little suspicious of those 3 variables as well.\nBrad\nBrad Schroeder\nSenior Software Engineer\nStinger Ghaffarian Technologies (SGT)Technical Support Services Contractor to U.S. Geological SurveyEarth Resources Observation & Science Center47914 252nd Street NESioux Falls, South Dakota 57198\n(605) 594-2685\n**Note: I have updated my email address to bschr...@contractor.usgs.gov"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Kent Yang",
     "body": "Nathan,\nI don't think this is the issue.\nThe different names of ArchiveMetadata etc at DAS and DDS are normal and legal for DAP.\nThe HDF-EOS5 use an HDF5 dataset(a variable) to store ArchiveMetadata_0... . So the handler maps these variables to DAP DDS.\nThe DAS output of ArchiveMetdata, CoreMetadata, StructMetadata are added by the handler since NASA asks to see the information inside in the DAS rather than as data.\nSo we provided them in DAS. The different names between DAS and DAS are due to the complexity of HDF-EOS conventions. I remembered I explained to you on why we chose the names like this before. So won't repeat here.\nBut anyhow, I don't think this should be the reason that causes the NcML failed.\nFrom DAP point of view, DDS is for variable shape and type, DAS is for attributes or containers. DDS has a set of string variable ArchiveMertadata_0 .. DAS has some similar attribute names like ArichveMetadata ... They are legal for DAP. It shouldn't offend any DAP implementation. Just like I can have a variable fake1 and have an attribute or container fake1_0. Will you say that this should never happen for DAP?\nSo the different names should not be the reason that causes the NcML failed. My guess is that the failure is due to the contents inside the CoreMetadata attribute. The String CoreMetadata is really complicated. The failure is also possible caused by the CoreMetadata_0, which is unlikely. Somehow NcML has an internal check that is not happy with the contents. It may also be due to the others.\nFolks at LP DAAC may replace the content of the current CoreMetadata attribute with a trivial string and see if it still causes the problem.\nKent\n- show quoted text -"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Kent Yang",
     "body": "BTW, I used the Hyrax's filenetCDF-4 to downloaded 3 HDF5 files as netCDF-4 from http://test.opendap.org/opendap/viirs/ without any problems. I checked the nc4 files. They can be opened by Panoply and HDFView. All the contents are kept.\nHowever, I don’t understand why filenetCDF-4 chops the coremetadata(partial attached below) into many attributes.\nCoreMetadata.INVENTORYMETADATA.GROUPTYPE: MASTERGROUP\nCoreMetadata.INVENTORYMETADATA.ECSDATAGRANULE.LOCALGRANULEID.NUM_VAL: 1\n........\nFrom http://test.opendap.org/opendap/viirs/VNP13A1.A2018153.h10v04.001.2018169191321.h5.das\nCoreMetadata {\nINVENTORYMETADATA {\nString GROUPTYPE \"MASTERGROUP\";\nECSDATAGRANULE {\nLOCALGRANULEID {\nInt32 NUM_VAL 1;\nString VALUE \"VNP13A1.A2018153.h10v04.001.2018169191321.h5\";\n}\nPRODUCTIONDATETIME {\nInt32 NUM_VAL 1;\nString VALUE \"2018-06-18T15:13:21.000Z\";\n}\nDAYNIGHTFLAG {\nInt32 NUM_VAL 1;\nString VALUE \"DAY\";\n}\nREPROCESSINGACTUAL {\nInt32 NUM_VAL 1;\nString VALUE \"reprocessed\";\n}\nLOCALVERSIONID {\nInt32 NUM_VAL 1;\nString VALUE \"2.2.4\";\n}\nREPROCESSINGPLANNED {\nInt32 NUM_VAL 1;\nString VALUE \"further update is anticipated\";\n}\n}\nMEASUREDPARAMETER {\nMEASUREDPARAMETERCONTAINER {\nString CLASS \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\";\nPARAMETERNAME {\nString CLASS \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\";\nInt32 NUM_VAL 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1;\nString VALUE \"500 m 16 days NDVI\", \"500 m 16 days EVI\", \"500 m 16 days EVI2\", \"500 m 16 days VI Quality\", \"500 m 16 days red reflectance\", \"500 m 16 days NIR reflectance\", \"500 m 16 days blue reflectance\", \"500 m 16 days green reflectance\", \"500 m 16 days SWIR1 reflectance\", \"500 m 16 days SWIR2 reflectance\", \"500 m 16 days SWIR3 reflectance\", \"500 m 16 days view zenith angle\", \"500 m 16 days sun zenith angle\", \"500 m 16 days relative azimuth angle\", \"500 m 16 days composite day of the year\", \"500 m 16 days pixel reliability\";\n}\nQAFLAGS {\nString CLASS \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\";\nSCIENCEQUALITYFLAG {\nString CLASS \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\";\nInt32 NUM_VAL 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1;\nString VALUE \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\", \"Not Investigated\";\n-----Original Message-----\nFrom: Kent Yang\nSent: Tuesday, June 26, 2018 3:22 PM\nTo: 'Nathan Potter'; Bradley (Contractor) Schroeder\nCc: n...@opendap.org; Aaron Friesz; User Support; James Gallagher; Bradley Schroeder (CTR)\nSubject: RE: [EXTERNAL] Re: [support] OPeNDAP HDF5 Issue\nNathan,\nI don't think this is the issue.\nThe different names of ArchiveMetadata etc at DAS and DDS are normal and legal for DAP.\nThe HDF-EOS5 use an HDF5 dataset(a variable) to store ArchiveMetadata_0... . So the handler maps these variables to DAP DDS.\nThe DAS output of ArchiveMetdata, CoreMetadata, StructMetadata are added by the handler since NASA asks to see the information inside in the DAS rather than as data.\nSo we provided them in DAS. The different names between DAS and DAS are due to the complexity of HDF-EOS conventions. I remembered I explained to you on why we chose the names like this before. So won't repeat here.\nBut anyhow, I don't think this should be the reason that causes the NcML failed.\nFrom DAP point of view, DDS is for variable shape and type, DAS is for attributes or containers. DDS has a set of string variable ArchiveMertadata_0 .. DAS has some similar attribute names like ArichveMetadata ... They are legal for DAP. It shouldn't offend any DAP implementation. Just like I can have a variable fake1 and have an attribute or container fake1_0. Will you say that this should never happen for DAP?\nSo the different names should not be the reason that causes the NcML failed. My guess is that the failure is due to the contents inside the CoreMetadata attribute. The String CoreMetadata is really complicated. The failure is also possible caused by the CoreMetadata_0, which is unlikely. Somehow NcML has an internal check that is not happy with the contents. It may also be due to the others.\nFolks at LP DAAC may replace the content of the current CoreMetadata attribute with a trivial string and see if it still causes the problem.\nKent\n-----Original Message-----\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Tuesday, June 26, 2018 2:48 PM\nTo: Bradley (Contractor) Schroeder\nCc: n...@opendap.org; Aaron Friesz; User Support; James Gallagher; Kent Yang; Bradley Schroeder (CTR)\nSubject: Re: [EXTERNAL] Re: [support] OPeNDAP HDF5 Issue\n- show quoted text -"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Bradley (Contractor) Schroeder",
     "body": "Aaron and I just ran the test that Kent suggested.\nAaron modified the CoreMetadata attribute on 3 files and changed it to Hello OPeNDAP 123, Hello OPeNDAP 456, & Hello OPeNDAP 789.\nThen I created an NcML file to aggregate those 3 .h5 files and those seem to return data for all three time steps as expected.\nBrad\nBrad Schroeder\nSenior Software Engineer\nStinger Ghaffarian Technologies (SGT)Technical Support Services Contractor to U.S. Geological SurveyEarth Resources Observation & Science Center47914 252nd Street NESioux Falls, South Dakota 57198\n(605) 594-2685\n**Note: I have updated my email address to bschr...@contractor.usgs.gov"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Aaron Friesz (CTR)",
     "body": "Here is a link to the sample data with the modified CoreMetadata attribute.\nhttps://www.dropbox.com/s/vafopnsp5k5jr7u/VNP13A1_CoreMetadata_MOD_SampleFiles.zip?dl=0\nThanks,\nAaron"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Kent Yang",
     "body": "Aaron and Brad,\nGood. It seems that the contents of CoreMetadata cause the issue. While Nathan is looking into why NcML is not happy with CoreMetadata, if your center doesn’t care about making the CoreMetadata publish to the users, I may provide a BES key in the HDF5 handler not to output the CoreMetadata etc.\nThe default setting of the handler will still provide the mapping of these attributes. However, you can change the BES key at your center so that these attributes will not be shown in the DAP output. It may also improve the performance a little bit. If you are interested, please share with me with my email address only since this is solely an enhancement of the HDF5 handler for the NASA DAACs.\nRegards,\nKent\nFrom: Aaron Friesz (CTR) [mailto:aaron.fr...@usgs.gov]\nSent: Tuesday, June 26, 2018 4:02 PM\nTo: Schroeder (CTR), Bradley\nCc: Kent Yang; n...@opendap.org; OPeNDAP Support; James Gallagher; Schroeder (CTR), Bradley\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n> **Note: I have updated my email address to bsch...@contractor.usgs.gov\n>\n>\n> On Tue, Jun 26, 2018 at 1:50 PM Nathan Potter <n...@opendap.org> wrote:\n> Hi Brad & Aaron,\n>\n> I out the test data and the ncml file here:\n>\n> http://test.opendap.org/opendap/viirs/\n>\n> You can get the ncml file with this URL:\n>\n> http://test.opendap.org/opendap/viirs/test_agg.ncml\n>\n> And it replicates your problem. At least I think it’s your problem, there certainly are problems with that aggregation.\n>\n> And it’s got a ticket: https://opendap.atlassian.net/browse/HYRAX-792\n>\n> I don’t see an obvious NcML thing to correct so I’ll have to look more closely at the issue.\n>\n>\n> Sincerely,\n>\n> Nathan\n>\n>\n> > On Jun 26, 2018, at 7:15 AM, Bradley (Contractor) Schroeder <bschr...@contractor.usgs.gov> wrote:\n> >\n> > Hi Nathan,\n> >\n> > These 3 files were a subset of a larger data set. I modified the ncml file to reflect just these 3 files, and attached it to this email.\n> >\n> > I also verified that this subset of 3 files gives us the same functionality as the full data set.\n> >\n> > Brad\n> >\n> > Brad Schroeder\n> > Senior Software Engineer\n> > Stinger Ghaffarian Technologies (SGT)\n> > Technical Support Services Contractor to U.S. Geological Survey\n> > Earth Resources Observation & Science Center\n> > 47914 252nd Street NE\n> > Sioux Falls, South Dakota 57198\n> > (605) 594-2685\n> >\n> > **Note: I have updated my email address to bsch...@contractor.usgs.gov\n- show quoted text -\n--\n- show quoted text -\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly."
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Nathan Potter",
     "body": "I have updated the test server by adding the modified files and the associated aggregation ncml.\nhttp://test.opendap.org/opendap/viirs/\nIt does seem to work fine.\nThanks,\nNathan\n- show quoted text -\n> > **Note: I have updated my email address to bschr...@contractor.usgs.gov\n> >\n> >\n> > On Tue, Jun 26, 2018 at 1:50 PM Nathan Potter <n...@opendap.org> wrote:\n> > Hi Brad & Aaron,\n> >\n> > I out the test data and the ncml file here:\n> >\n> > http://test.opendap.org/opendap/viirs/\n> >\n> > You can get the ncml file with this URL:\n> >\n> > http://test.opendap.org/opendap/viirs/test_agg.ncml\n> >\n> > And it replicates your problem. At least I think it’s your problem, there certainly are problems with that aggregation.\n> >\n> > And it’s got a ticket: https://opendap.atlassian.net/browse/HYRAX-792\n> >\n> > I don’t see an obvious NcML thing to correct so I’ll have to look more closely at the issue.\n> >\n> >\n> > Sincerely,\n> >\n> > Nathan\n> >\n> >\n> > > On Jun 26, 2018, at 7:15 AM, Bradley (Contractor) Schroeder <bschr...@contractor.usgs.gov> wrote:\n> > >\n> > > Hi Nathan,\n> > >\n> > > These 3 files were a subset of a larger data set. I modified the ncml file to reflect just these 3 files, and attached it to this email.\n> > >\n> > > I also verified that this subset of 3 files gives us the same functionality as the full data set.\n> > >\n> > > Brad\n> > >\n> > > Brad Schroeder\n> > > Senior Software Engineer\n> > > Stinger Ghaffarian Technologies (SGT)\n> > > Technical Support Services Contractor to U.S. Geological Survey\n> > > Earth Resources Observation & Science Center\n> > > 47914 252nd Street NE\n> > > Sioux Falls, South Dakota 57198\n> > > (605) 594-2685\n> > >\n> > > **Note: I have updated my email address to bschr...@contractor.usgs.gov\n- show quoted text -"
    },
    {
     "page": "OPeNDAP HDF5 Issue",
     "name": "Aaron Friesz (CTR)",
     "body": "Hi Kent,\nWe'd be interested in modifying(?) the BES Key to get around this problem. The CoreMetadata attribute is not used in any of our applications that leverage OPeNDAP that I know of. Would this be a long-term solution? Or just until this bug get fixed. Also, if the bug fix does not make it into the next release of Hyrax and we update anyway, would the BES Key change still be valid?\nI'll send a separate email to only you, as requested, for the BES Key change information.\nThanks,\nAaron"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Zero byte download file",
     "name": "phuon...@gmail.com",
     "body": "Hello,\nWe are using Hyrax 1.14.0 docker image to serve our .nc files. Some of them will return 0 byte files if we click to \"Get as NetCDF 3\" or \"Get as NetCDF 4\" button, other buttons work. However, if we select several Variables (see picture attached), it works. It seems that is not data size problem because some other big files (>1 GB) still work well.\nCould you please take a look into the attached nc file (3.5 MB) if you have a chance to figure out what happened?\nThanks,\nPhuong"
    },
    {
     "page": "Zero byte download file",
     "name": "Nathan Potter",
     "body": "Hi Phuong,\nI am able to replicate your issue. I have a copy of the data on our test server:\nhttp://test.opendap.org/opendap/phuong/nwm.t00z.short_range.channel_rt.f001.conus.nc.html\nAnd it behaves as you describe.\nI made a ticket:\nhttps://opendap.atlassian.net/browse/HYRAX-764\nAnd we may know more soon.\nThanks!\nNathan\n- show quoted text -\n> <Auto Generated Inline Image 1.png>\n> <nwm.t00z.short_range.channel_rt.f001.conus.nc><Auto Generated Inline Image 1.png>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Zero byte download file",
     "name": "Phuong Doan"
    },
    {
     "page": "Zero byte download file",
     "name": "Phuong Doan",
     "body": "Hi Nathan,\nSorry for disturb, is there any update for this case?\nThanks,\nPhuong"
    },
    {
     "page": "Zero byte download file",
     "name": "Nathan Potter",
     "body": "Hi Phuong,\nI have not had a chance to look into this more closely. It’s still on the list. My apologies for the delay.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Zero byte download file",
     "name": "Phuong Doan",
     "body": "Thanks Nathan for your quick reply,\nIt’s no problem, I only need to make sure I do not miss any update 😂\nBr,\nPhuong\n- show quoted text -"
    }
   ]
  },
  {
  },
  {
  },
  {
   "name": [
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Hi,\nI’m interested in hourly MERRA-2 data (M2T1NXSLV), and I tried to download it with MATLAB from the OPENDAP server (https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/). I can get information about the NC4 files with ncdisp, but ncread returns the following error\nError using netcdflib\nThe NetCDF library encountered an error during execution of 'getVarDouble' function - 'Authorization failure (-78)'.\nWhen I try to download the whole nc file from the data server (https://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2T1NXSLV.5.12.4/), there’s no problem. But I need to download a subset for a long period. Could you help me?\nI’m using Matlab R2017a. The NetCDF library version is 4.3.3.1 and the Operating system is Windows 10 Enterprise 1803 (17134.48).\nThanks in advance.\nJamal"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Hi Jamal,\nThe errors you are encountering are probably due to the fact that NASA now requires a login step in order to access data. This isn’t typically a problem for a browser as you (a user) are on hand to navigate the authentication steps. In order for Matlab to work you have to do some extra configuration.\nWe have general instructions in section 6.3 of our Hyrax Guide:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_authentication_for_dap_clients.\nFor NASA data you will be utilizing their Earthdata Login system so you will need to read and perform the tasks outlined in section 6.3.2\nOnce that is completed you should follow the specific instructions for Matlab in section 6.3.9 (more details in section 6.3.5 - because Matlab utilizes the NetCDF-C library to perform it’s DAP day access steps)\nPleas let me know whether or not this resolves your issue.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Hi Nathan,\nThanks for your answer.\nIn what refers to section 6.3.2, I have already Earthdata credentials. I've approved NASA GESDISC DATA ARCHIVE and GES DISC (attached picture). In the whole applications list, i can't find any mention of a DAP server for the GES DISC.\nAs for the section 6.3.9, the .netrc file in my home directory contains:\nmachine urs.earthdata.nasa.gov\nlogin jamalchi\npassword *****\nmachine goldsmr4.gesdisc.eosdis.nasa.gov\nlogin jamalchi\npassword *****\nand the .dodsrc file contains:\nHTTP.COOKIEJAR=C:\\Users\\Jamal\\.urs_cookies\nHTTP.NETRC=C:\\Users\\Jamal\\.netrc\nThe .urs_cookies file is empty.\nin spite of that, I'm still receiving the Authorization failure error.\nThanks again for your help.\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailto:n...@opendap.org]\nEnviado el: martes, 19 de junio de 2018 12:46\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>; sup...@opendap.org\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Hi Nathan,\nThanks for your answer.\nIn what refers to section 6.3.2, I have already Earthdata credentials. I've approved NASA GESDISC DATA ARCHIVE and GES DISC (attached picture). In the whole applications list, i can't find any mention of a DAP server for the GES DISC.\nAs for the section 6.3.9, the .netrc file in my home directory contains:\nmachine urs.earthdata.nasa.gov\nlogin jamalchi\npassword *****\nmachine goldsmr4.gesdisc.eosdis.nasa.gov\nlogin jamalchi\npassword *****\nand the .dodsrc file contains:\nHTTP.COOKIEJAR=C:\\Users\\Jamal\\.urs_cookies\nHTTP.NETRC=C:\\Users\\Jamal\\.netrc\nThe .urs_cookies file is empty.\nin spite of that, I'm still receiving the Authorization failure error.\nThanks again for your help.\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailto:n...@opendap.org]\nEnviado el: martes, 19 de junio de 2018 12:46\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>; sup...@opendap.org\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Hi Jamal,\nWhat you have seems like it should work.\nOne thing I noticed is that in .netrc you have an entry for goldsmr4.gesdisc.eosdis.nasa.gov\nI don’t think you need it.\nThe only server that you would need to send URS credentials to is urs.earthdata.nasa.gov\nHere are some thing to check for the authentication issue:\nThe file system permissions for the .netrc should be set to rw for your user and none everywhere else:\n-rw-------@ 1 ndp staff 474 Jun 19 09:38 /Users/ndp/.netrc\nIf that looks ok (or if you correct it and Matlab still doesn’t work)then I think we should try some command line stuff.\nFirst with curl (if you ghave it) and then with the netcdf-lib.\nIf you can, run this curl command:\ncurl -k -n -c ursCookies -b ursCookies -L --url \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4.ascii?lat\"\nIf your .netrc contains valid URS credentials and it’s permissions are set as indicated above, this really should work.\nIf the curl thing fails there must be an issue with .netrc.\nIf it works that’s a good sign.\nThen I think try the netcdf-lib utility “ncdump”.\nTry this:\nncdump \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4?lat”\nThat should return metadata regardless, but fail at the “data:” section if the authentication doesn’t work.\nPlease let me know how that goes.\nSincerely,\nNathan\n- show quoted text -\n> <authorized_apps.png>\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Hi Nathan,\nUsing the curl command, i get\nHTTP Basic: Access denied.\nUsing the ncdump command, I get\ncurl error details:\nncdump: https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4?lat: NetCDF: I/O failure\nI'm sure that the .netrc file is rw for me (only user).\nIt's strange because i can download data with this credentials with the browser.\nRegards,\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailto:n...@opendap.org]\nEnviado el: jueves, 21 de junio de 2018 14:13\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Hi Jamal,\nI think understanding the problem with curl is going to lead us to the Matlab issue. Let’s see if curl will tell us anything of value.\nTry clearing any cookies, add “-v” to the curl command, and capture the output (stdin & stderr) in a file like this:\nrm -f ursCookies\ncurl -v -k -n -c ursCookies -b ursCookies -L --url \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4.ascii?lat\" > curl.log 2>&1\n(Not sure if all that translates in Windows. The idea is to get all the verbose curl output in a file.)\nPlease send me the file.\nHopefully curl will say why it’s unhappy.\nI’ve attached the log from my successful attempt to this message.\nCheers,\nNathan"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Hi Nathan,\nYou have attached the log file.\nThank you so much for your patience.\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailto:n...@opendap.org]\nEnviado el: jueves, 21 de junio de 2018 16:21\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>; sup...@opendap.org\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Hi Jamal,\nIn your log files I see this:\n* Couldn't find host urs.earthdata.nasa.gov in the _netrc file; using defaults\nWhich is surely the source of at least the curl headache and possibly Matlab too.\nThe name of the file “_netrc” got me thinking so I googled and found this:\nhttps://stackoverflow.com/questions/6031214/git-how-to-use-netrc-file-on-windows-to-save-user-and-password\n(Note the second line of the first reply)\nIt looks to me that if you rename your .netrc file to _netrc and then retry the curl command we might get further.\nLet me know.\nNathan\n- show quoted text -\n> <curl.log>\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Hi Nathan,\nI've renamed the file to _netrc. After, I used the command\nsetx HOME %USERPROFILE%\nfinally, i've run the cmd as administrator and worked out of my home directory (in C:\\), and now it's working.\nI tried with curl and wget and it's working fine with both, but not with Matlab.\nI tried to put _netrc, _dodsrc and the ursCookies in my Matlab work folder, but without result.\nThanks so much.\nKind regards,\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailto:n...@opendap.org]\nEnviado el: jueves, 21 de junio de 2018 18:16\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Hi Jamal,\nSo you where able to use curl and wget, but only as the administrator?\nIf that’s the case then it's probably not a good sign, as I suspect Matlab runs as your user.\nI am wondering about this:\n> setx HOME %USERPROFILE%\nI think you might try being very specific. Try doing this:\nsetx HOME C:\\Users\\Jamal\nAnd see if that makes it so that you can run curl without being the administrator.\nNathan\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Note below…\n> On Jun 21, 2018, at 2:17 PM, Nathan Potter <n...@opendap.org> wrote:\n>\n> Hi Jamal,\n>\n> So you where able to use curl and wget, but only as the administrator?\n>\n> If that’s the case then it's probably not a good sign, as I suspect Matlab runs as your user.\n>\n> I am wondering about this:\n>\n>> setx HOME %USERPROFILE%\n>\n>\n> I think you might try being very specific. Try doing this:\n>\n> setx HOME C:\\Users\\Jamal\n>\nAnd of course making sure that .netrc and .dodsrc ar in the directory C:\\Users\\Jamal\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: \"Jamal Chioua\" <jama...@gmail.com>\nSubject: RE: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nDate: June 22, 2018 at 1:46:21 AM PDT\nTo: \"'Nathan Potter'\" <n...@opendap.org>\nGood morning Nathan,\nYou were right. Now, curl and wget are working good without need to run it as administrator.\nYesterday I tried to make Matlab use the cookies file created with curl to connect to the opendap server. it could easily be done in the case of a conventional HTTP, using the urlread or webread command. But, to read netcdf files on an opendap server, you can only use the ncread command, and don't have the option to use a cookies file, nor to use username and password directly in the command.\nwith the previous security configuration, one could use, for example: 'http://',username':'password,'@site/', but not anymore.\nCan you think of another method?\nCheers,\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailt...@opendap.org]\nEnviado el: jueves, 21 de junio de 2018 22:19\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>; sup...@opendap.org\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nNote below…\nOn Jun 21, 2018, at 2:17 PM, Nathan Potter <n...@opendap.org> wrote:\nHi Jamal,\nSo you where able to use curl and wget, but only as the administrator?\nIf that’s the case then it's probably not a good sign, as I suspect Matlab runs as your user.\nI am wondering about this:\nsetx HOME %USERPROFILE%\nI think you might try being very specific. Try doing this:\nsetx HOME C:\\Users\\Jamal\nAnd of course making sure that .netrc and .dodsrc ar in the directory C:\\Users\\Jamal\nAnd see if that makes it so that you can run curl without being the administrator.\nNathan\nOn Jun 21, 2018, at 11:44 AM, Jamal Chioua <jama...@gmail.com> wrote:\nHi Nathan,\nI've renamed the file to _netrc. After, I used the command setx HOME\n%USERPROFILE% finally, i've run the cmd as administrator and worked\nout of my home directory (in C:\\), and now it's working.\nI tried with curl and wget and it's working fine with both, but not with Matlab.\nI tried to put _netrc, _dodsrc and the ursCookies in my Matlab work folder, but without result.\nThanks so much.\nKind regards,\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailt...@opendap.org] Enviado el: jueves, 21 de\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nDate: June 22, 2018 at 3:26:44 AM PDT\nTo: Jamal Chioua <jama...@gmail.com>\nCc: Nathan Potter <n...@opendap.org>\nHi Jamal,\nWhat I think I know is that Matlab uses the netcdf-c library underneath the hood.\nIf we can get the netcdf command line utility “ncdump\" working like wget and curl then I think Matlab will work.\nNow that you have wget and curl working, have you tried ncdump on the same request URL?\nFrom a shell/commandline try this:\nncdump https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4?lat\nThis works on my system and I think it will work for you if you have HOME set correctly and the .netrc and .dodsrc files organized as discussed previously.\nLet me know if that works, if it does then we can sort out Matlab\nSincerely,\nNathan\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: \"Jamal Chioua\" <jama...@gmail.com>\nSubject: RE: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nDate: June 22, 2018 at 4:05:12 AM PDT\nTo: \"'Nathan Potter'\" <n...@opendap.org>\nHi Nathen,\nNcdump is giving the same error:\ncurl error details:\nncdump: https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4?lat: NetCDF: I/O failure\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailt...@opendap.org]\nEnviado el: viernes, 22 de junio de 2018 11:27\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\ne -on-windows-to-save-user-and-password\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nDate: June 22, 2018 at 4:22:37 AM PDT\nTo: Jamal Chioua <jama...@gmail.com>\nCc: Nathan Potter <n...@opendap.org>\nHi Jamal,\nThere are some version differences between our systems (I have netcdf library version 4.4.1.1 of May 21 2018, you have 4.3.3.1) but I don’t think that’s an issue.\nWhen I run this:\nncdump https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4?lat\nI get the correct response, no error.\nThis is pretty clearly about the netcdf library code not being able to locate the .netrc and .dodsrc files.\nFrom the command line are you able to issue a command that will show you the currently set environment variables?\nCan you see if HOME is set correctly in your current shell?\nBecause ncdump should work fine and since it is using the same underlying API that Matlab is I think that if we can get ncdump sorted then we will be very close to getting Matlab too.\nHere’s what I am thinking - When you run curl from the command line it runs in the current shell “environment”, so if HOME is set correctly in your current shell everything should be fine.\nAnd that seems to be the case based on what we have seen so far.\nBut when you launch Matlab it’s not being launched from your shell, but rather by the OS and the “environment” that it inherits is set by the OS from your “default” profile, not from the “environment” from an open shell window.\nI think what has to happen for Matlab is that you have to set “HOME” using the Control Panel application.\nThis should cause HOME to be set for applications launched by the system on your behalf.\nDoes that make sense?\nHere’s something that might be of use:\nhttps://winaero.com/blog/how-to-see-names-and-values-of-environment-variables-in-windows-10/\nDon’t despair, I am pretty sure we can sort this out.\nSincerely,\nNathan\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: \"Jamal Chioua\" <jama...@gmail.com>\nSubject: RE: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nDate: June 22, 2018 at 5:01:22 AM PDT\nTo: \"'Nathan Potter'\" <n...@opendap.org>\nHi Nathan,\nThe netcdf library version 4.3.3.1 is what i'm using under Matlab R2017a.\nTo use ncdump, I've installed netCDF 4.6.1 from https://www.unidata.ucar.edu/downloads/netcdf/index.jsp. Once installed, I've added the netcdf 4.6.1 path to the environment variables, so I can execute ncdump everywere.\nWhen I type >%HOME% from cmd, I get \"C:\\Users\\Jamal\".\nSame thing if typing\necho %userprofile%\necho %home%\nthe result is always C:\\Users\\Jamal\nIn Matlab, typing\ngetenv('HOME')\nans =\n'C:\\Users\\Jamal'\nAnd surprise!!!! Matlab is working when i execute it without administrator privileges.\nI will run the whole script in Matlab that I prepared to download and analyse MERRA-2 Data and i tell you\nJamal\n-----Mensaje original-----\nDe: Nathan Potter [mailt...@opendap.org]\nEnviado el: viernes, 22 de junio de 2018 12:23\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nHi Jamal,\n- show quoted text -\n- show quoted text -\nDe: Nathan Potter [mailto:n...@opendap.org] Enviado el: viernes, 22 de\n- show quoted text -\n- show quoted text -\nDe: Nathan Potter [mailto:n...@opendap.org] Enviado el: jueves, 21 de\n- show quoted text -\n- show quoted text -\n- show quoted text -\nDe: Nathan Potter [mailto:n...@opendap.org] Enviado el: jueves, 21\nde junio de 2018 18:16\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>; sup...@opendap.org\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via\nOPENDAP server\nHi Jamal,\nIn your log files I see this:\n* Couldn't find host urs.earthdata.nasa.gov in the _netrc file;\nusing defaults\nWhich is surely the source of at least the curl headache and possibly Matlab too.\nThe name of the file “_netrc” got me thinking so I googled and found this:\nhttps://stackoverflow.com/questions/6031214/git-how-to-use-netrc-fi\nl e -on-windows-to-save-user-and-password\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: \"Jamal Chioua\" <jama...@gmail.com>\nSubject: RE: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nDate: June 22, 2018 at 10:34:11 AM PDT\nTo: \"'Nathan Potter'\" <n...@opendap.org>\nHi Nathan,\nFinally, I can confirm that is working well in Matlab right now.\nI've been a bit slow to confirm since I've been setting up other computers following all the steps we have taken, and now I think I already know what the problem was.\nUnder normal conditions, changing the .netrc to _netrc should be enough for it to work, but I've seen that to define HOME in the environnement variables using set, it disappears with every cmd restart, so you need to define it again. This is arranged using setx instead of set, but it does not work on all computers (possibly different versions of cmd).\nThis problem is not completely resolved on computers that do not accept setx, and for the script to work, the necessary steps are:\n- Start cmd and type set HOME= C:\\Users\\Jamal,\n- keeping cmd open, start Matlab\n- type setenv('HOME','C:\\Users\\Jamal')\n-run the script\nThanks for everything.\nJamal\n-----Mensaje original-----\nDe: Nathan Potter <n...@opendap.org>\nEnviado el: Friday, June 22, 2018 2:31 PM\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\nSo it seems to work, nice!\nOn Jun 22, 2018, at 5:01 AM, Jamal Chioua <jama...@gmail.com> wrote:\nHi Nathan,\n- show quoted text -\n- show quoted text -\n2 .4/1996/07/MERRA2_200.tavg1_2d_slv_Nx.19960708.nc4?lat\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\ni l e -on-windows-to-save-user-and-password\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Nathan Potter",
     "body": "Jamal,\nGreat news! Thanks for the detailed information. I don’t have access to a Windows 10 system which means this kind of information (now in our support email system) can be very helpful to others down the road.\nI’m glad we got it sorted out.\nCheers,\nNathan\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Problem to access MERRA2 data with MATLAB via OPENDAP server",
     "name": "Jamal Chioua",
     "body": "Nathan,\nIt would have been impossible without your help. Thanks again\nSincerely,\nJamal\n-----Mensaje original-----\nDe: Nathan Potter <n...@opendap.org>\nEnviado el: Friday, June 22, 2018 6:39 PM\nPara: Jamal Chioua <jama...@gmail.com>\nCC: Nathan Potter <n...@opendap.org>; User Support <sup...@opendap.org>\nAsunto: Re: [support] Problem to access MERRA2 data with MATLAB via OPENDAP server\n- show quoted text -"
    }
   ]
  },
  {
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi Executive Director, I hope you’re having a great day! I came across Open Source Project For A Network again a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a technology service specifically built for small to medium-sized nonprofits. We help organizations save time and money by automating your fundraising interactions, in one place, with no technical work required on your end.\nWith your Flipcause subscription, you'll receive every feature and service we have to offer regardless of budget or size.\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you're open to a quick conversation, take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you're not interested, let me know and you'll never hear from me again. :)\nThank you for your time and I hope to hear from you soon!\nJoan Ramirez\nOutreach Representative\nFlipcause Inc.\n283 4th Street, Suite 101 Oakland, CA 94607\n(800) 523-1950\nUNSUBSCRIBE: If you would like to stop receiving further messages, reply \"Stop\" to this email.\nHere are all of the features and services that come with your Flipcause subscription (we know, it's a lot)…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nMobile app with event and volunteer check-in\nPeer to peer fundraising\nTeam fundraising\nFull payment processing and refund system\nCredit card swiper/card reader\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated Success Team (with unlimited concierge services included at no additional cost)\nFully built website and web hosting services\nSocial media integration\nUnlimited webmaster services (at no additional cost)\nSSL certification and installation\nFully customizable fundraising pages to market your campaigns\nContact database to keep track of your supporters in one place\nEasy to transfer fundraising activity into Quickbooks"
    }
   ]
  },
  {
   "name": [
    {
     "page": "bes getopt() patch",
     "name": "Orion Poplawski",
     "body": "getopt() returns int.\nSee attached patch.\n--\nOrion Poplawski\nManager of NWRA Technical Systems 720-772-5637\nNWRA, Boulder/CoRA Office FAX: 303-415-9702\n3380 Mitchell Lane or...@nwra.com\nBoulder, CO 80301 https://www.nwra.com/"
    }
   ]
  },
  {
  },
  {
  },
  {
  },
  {
   "name": [
    {
     "page": "How to make plot of rainfall using netCDF4 data.",
     "name": "Dr. Md. Nazmul Ahasan",
     "body": "Dear Sir,\nI downloaded netCDF4 formatted daily TRMM data named \"3B42RT_Daily.20010701.7.nc4.nc4\".\nI want to make plot for rainfall using GrADS.\nHow can I do this.\nPls help me in details.\nRegards\n-MNA"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Fucoxantin Products",
     "name": "William Zhang",
     "body": "Dear Sir,\nWe are the manufacturer of Fucoxantin, Shandong Jiejing Group Corporation established in 1968.\nThere are top production technique and equipment at our factory and we can supply the Fucoxantin 1% and 10% powder with stable regular production quantity.\nWe'd very like to provide more information about our products. Please feel free to reply us and we can answer any questions about the Fucoxantin.\nWelcome to visit our factory in Rizhao, China at any time!\n2018-06-07\nBest Regards,\nWilliam Zhang\nShandong Jiejing Group Corporation\nNo. 98, Shenzhen West Road, Rizhao Economic Development Zone, Rizhao City, Shandong Province, China\nMobile: +86 158 5821 2758\nhttp://www.china-jiejing.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi. I expressed to you the concern before but we have probably reached a point to rethink Hyrax support for converting whole granules as it is increasingly a usability issue for some of our large datasets. First, the utility of attaching a single response such as .ascii or .nc to an OPeNDAP granule URL, and the corresponding “Get as” tabs on top of an .html form of a granule, while being great features in the past, are now becoming the sources of burden for our Hyrax server. Trouble is our data granules keep getting bigger, and these facilities make it too easy for users converting whole-data. I also believe these facilities can actually send a wrong message about what OPeNDAP really is.\nCan we make it a configuration option for a Hyrax server to disable the facility until a constraint expression is included? This way, servers with small datasets can still choose to keep the easy conversions.\nSecond, some patient users can still click through (or write) to include all datasets for a full-granule conversion. There are probably already some constraints (string length in a constraint expression, amount of data to convert, etc.) in Hyrax but they are a bit ambiguous to me. Can we implement such that users can get clear messages that they have reached the limits? This again can be a configuration option.\nOPeNDAP is #1 a subsetting facility, #2 a conversion facility. It is never mean to be a download facility although downloading can be provided after #1 and 2. I think by doing the above we can convey the correct message, and guide users to better use the server.\n-Fan"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Peter Cornillon",
     "body": "Hi Fan,\nQuestion in-line below.\n—\nPeter Cornillon\n215 South Ferry Road Telephone: (401) 874-6283\nGraduate School of Oceanography\nUniversity of Rhode Island\nNarragansett, RI 02882 USA\nOn Jan 19, 2018, at 1:30 PM, Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC] <fan.f...@nasa.gov> wrote:\nHi. I expressed to you the concern before but we have probably reached a point to rethink Hyrax support for converting whole granules as it is increasingly a usability issue for some of our large datasets. First, the utility of attaching a single response such as .ascii or .nc to an OPeNDAP granule URL, and the corresponding “Get as” tabs on top of an .html form of a granule, while being great features in the past, are now becoming the sources of burden for our Hyrax server. Trouble is our data granules keep getting bigger, and these facilities make it too easy for users converting whole-data. I also believe these facilities can actually send a wrong message about what OPeNDAP really is.\nCan we make it a configuration option for a Hyrax server to disable the facility until a constraint expression is included?\nDo you mean ‘to disable the facility UNLESS a constraint expression is included?’ ? If so, I agree, that using OPeNDAP like this is not the way OPeNDAP was intended to be used unless, there is no other access option.\nPeter\nThis way, servers with small datasets can still choose to keep the easy conversions.\nSecond, some patient users can still click through (or write) to include all datasets for a full-granule conversion. There are probably already some constraints (string length in a constraint expression, amount of data to convert, etc.) in Hyrax but they are a bit ambiguous to me. Can we implement such that users can get clear messages that they have reached the limits? This again can be a configuration option.\nOPeNDAP is #1 a subsetting facility, #2 a conversion facility. It is never mean to be a download facility although downloading can be provided after #1 and 2. I think by doing the above we can convey the correct message, and guide users to better use the server.\n-Fan"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Nathan Potter",
     "body": "> On Jan 19, 2018, at 10:30 AM, Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC] <fan.f...@nasa.gov> wrote:\n>\n> Hi. I expressed to you the concern before but we have probably reached a point to rethink Hyrax support for converting whole granules as it is increasingly a usability issue for some of our large datasets. First, the utility of attaching a single response such as .ascii or .nc to an OPeNDAP granule URL, and the corresponding “Get as” tabs on top of an .html form of a granule, while being great features in the past, are now becoming the sources of burden for our Hyrax server. Trouble is our data granules keep getting bigger, and these facilities make it too easy for users converting whole-data. I also believe these facilities can actually send a wrong message about what OPeNDAP really is."
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Thanks Nathan and Peter – yes I meant UNLESS.\nI think a limit for response size would be the most direct fix, although coming up with a good limit can be a bit tricky. It sounds like a component in BES is dormant or missing.\nWe have separate HTTP servers where users can download whole original granules. On our service front we will make that service more prominent so that users can see that before they see anything opendap. Here I want to thank you guys to make the option in configuration to disable the download in Hyrax – that prevented a lot server trouble for us. On the other hand, adventurous and knowledgeable users can still make the full-granule conversion to get around this, which is our headache. We want to education these users in various ways, one of which being making this difficult or impossible for them.\n-Fan\n- show quoted text -"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nI just had a fairly length exchange with a woman that was intent on doing just that. It took some work to get her to directed to the file download part of the site.\nOne thought: Rather than disabling the file download feature maybe we could cook up a way so that if they ask for the “file” from hyrax we simply redirect the request to the file download service?\nN\n- show quoted text -"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nThat’s a valid idea. However given the different file download services offered at even the different NASA DAACs, this would have to be rule-based and configurable, and it could be hard to capture all possibilities. For us it would be a simple regex replacement of “/opendap/” by something like “/data/s4pa/” in a granule OPeNDAP URL to redirect to the HTTPS download URL. It may not be that straightforward for other data centers.\n-Fan\n- show quoted text -"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nYeah, I was thinking that might be the case. Not much uniformity in the URL paths between the centers.\nJames and I’ll will have a look at the max response size thing and see why that’s not operational.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nAre we implementing or planning this for a future Hyrax release? In either case recording it with a ticket would be great.\n-Fan\n- show quoted text -"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nThe max_response size feature was implemented quite a while ago. Somewhere along the way it was broken.\nIn January when we had our previous exchange about this problem, I made a ticket for the issue:\nhttps://opendap.atlassian.net/browse/HYRAX-594\nRecently James indicated to me that he thought he had fixed it, but I am not sure as the correction was made during a different effort.\nRegardless the intention is to repair the feature and roll that into the next Hyrax release.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "whole-granule conversion in Hyrax",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "That's great news Nathan! I look forward to testing the feature with your next release. Thanks.\n-Fan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Question about NEX-GDDP dataset",
     "name": "Daneshvar, Fariborz",
     "body": "Hi,\nI have the following questions about NEX-GDDP dataset, available on the OPeNDAP Dataset Access Form:\n- Regarding the time: this dataset is based on 365 day calendar. What would happen for leap years?\n- Regarding the longitude: it ranges from 0 to 360. How does it match with longitudes ranging from -180 to +180?\nThanks for your time,\nFariborz\n---\nFariborz Daneshvar, Ph.D.\nPost-Doctoral Research Associate\nDepartment of Agricultural and Biological Engineering\nPurdue University\nWest Lafayette, IN 47907"
    },
    {
     "page": "Question about NEX-GDDP dataset",
     "name": "Nathan Potter",
     "body": "Hi Fariboz,\nIn order to assist you I need the URL of the OPeNDAP Dataset Access Form you are viewing.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Question about NEX-GDDP dataset",
     "name": "Daneshvar, Fariborz",
     "body": "Hi Nathan,\nHere is the URL of a dateset that I mentioned before:\nhttps://dataserver.nccs.nasa.gov/thredds/dodsC/bypass/NEX-GDDP/bcsd/rcp85/r1i1p1/pr/inmcm4.ncml.html\nThanks for your help.\nFariborz\n---\nFariborz Daneshvar, Ph.D.\nPost-Doctoral Research Associate\nDepartment of Agricultural and Biological Engineering\nPurdue University\nWest Lafayette, IN 47907\n________________________________________\nFrom: Nathan Potter <n...@opendap.org>\nSent: Friday, May 25, 2018 1:13 PM\nTo: Daneshvar, Fariborz\nCc: Nathan Potter; sup...@opendap.org\nSubject: Re: [support] Question about NEX-GDDP dataset\n- show quoted text -"
    },
    {
     "page": "Question about NEX-GDDP dataset",
     "name": "Nathan Potter",
     "body": "Hi Fairboz,\nThe dataset metadata response:\nhttps://dataserver.nccs.nasa.gov/thredds/dodsC/bypass/NEX-GDDP/bcsd/rcp85/r1i1p1/pr/inmcm4.ncml.das\nReveals some of what you asked about:\n> - Regarding the time: this dataset is based on 365 day calendar. What would happen for leap years?\nThis data set appears to be the output of the BCSD model:\n> String references \"BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314.\n> Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php),\n> based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.\";\nAnd it is possible that there are simply no leap years. Climate/Forecast models sometimes operate on a 365 day\nyear to simplify the model computation.\nThe metadata also contain contact information for two principles that would likely know the answer:\n> String contact \"Dr. Rama Nemani: rama....@nasa.gov, Dr. Bridget Thrasher: bri...@climateanalyticsgroup.org\";\nAnd I am cc’ing them in the hopes that they will chime in on your calendar question.\n> - Regarding the longitude: it ranges from 0 to 360. How does it match with longitudes ranging from -180 to +180?\n>\nThe longitude metadata:\n> String standard_name \"longitude\";\n> String long_name \"longitude\";\n> String units \"degrees_east\";\n> String axis \"X\";\n> Int32 _ChunkSizes 1440;\n>\nIndicate that the units are “degrees_east”.\nSo this data set starts at 0 deg (Greenwich Line) and goes east from there.\nDatasets whose longitude spans -180 to 180 with units “degrees_east” start at the International Date Line (180 degrees West of the Greenwich Line)\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Question about NEX-GDDP dataset",
     "name": "Bridget Thrasher",
     "body": "Thanks, Nathan. I responded to Fariborz earlier today...\nBest,\nBridget"
    },
    {
     "page": "Question about NEX-GDDP dataset",
     "name": "Daneshvar, Fariborz",
     "body": "Hi Nathan and Bridget,\nThanks for your responses. I really appreciate it.\nBest,\nFariborz\n---\nFariborz Daneshvar, Ph.D.\nPost-Doctoral Research Associate\nDepartment of Agricultural and Biological Engineering\nPurdue University\nWest Lafayette, IN 47907\n________________________________________\nFrom: Nathan Potter <n...@opendap.org>\nSent: Tuesday, May 29, 2018 6:38 PM\nTo: Daneshvar, Fariborz; rama....@nasa.gov; bri...@climateanalyticsgroup.org\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Call for UGC Approved Journal (Impact Factor- 5.070, Research ID/Thomson)",
     "name": "arti...@gjesr.com",
     "body": "Dear Author/Researcher,\nGlobal Journal of Engineering Science and Researches, invites you to submit your research paper for publishing in Volume 5, Issue 6 (June- 2018).\nIndexing\nResearch ID/Thomson Reuters, Academic Edu, SIS, GIF, I20R, Zenodo (DOI) etc. For more details go the below link\nhttp://gjesr.com/index.html\nImpact Factor- 5.070 (Click here to verify)\nUGC Approved\nSubject Category:\nCover all branches of Engineering and Science\nImportant Dates:\nPaper Submission: 31 May 2018\nReview Results (Acceptance/Rejection) Notification: Within 02 Days after submitting the paper.\nPublished Online: 05 June 2018\nPublication Charges:\nRs 1500/Article ( Upto 3 Authors & 15 pages. Rs 500/Author Above 2 Authors and Rs 50/page above 15 pages).\nRs 300/Author for Hard Copy of Certificate(Optional).\n$80 Author out of India (Upto 5 Authors).\nSubmission Procedure:\nManuscript are invited in MS Word format and to be submitted via mail on gjesrj...@gmail.com or arti...@gjesr.com, Please note that Review Papers/Articles/Thesis/Posters are also acceptable.\nOnce a paper is accepted, authors are assumed to cede copyrights of the paper over to GJESR. All papers will be acknowledged and referred.\nFINAL PAPER SUBMISSION:\nEach final full-text paper (.doc) along with the corresponding signed copyright transfer form should be submitted by email to gjesrj...@gmail.com or articles@gjesr.com.\nRegards,\nGlobal Journal of Engineering Science & Research\narti...@gjesr.com\nwww.gjesr.com"
    },
    {
     "page": "Call for UGC Approved Journal (Impact Factor- 5.070, Research ID/Thomson)",
     "name": "arti...@gjesr.com",
     "body": "- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "olfs returns 400 for some requests",
     "name": "Golpayegani, Navid (GSFC-6190)",
     "body": "Hi,\nSome of our users are reporting that some requests to our opendap are returning a 400 to them. For example this link returns a 400 error:\nhttps://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0:1:359]\nWe consistently get a 400 error on that link but if you remove the XDim parameter the link works. Additionally, the response seems to be slightly different. For example with curl we simply receive a 400 HTTP code with no data while with Chrome browser we get a 400 code along with message:\nError {\ncode = 400;\nmessage = \"libdap exception building response: error_code = 1005: Failed to get values as ascii: Constraint expression parse error: syntax error\";\n}\nSince this is a live system with a lot of requests it’s hard to associate error messages from tomcat with a particular request but I think when we make this request we receive this error message from tomcat:\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | 24-May-2018 21:55:17.313 INFO [http-nio-8080-exec-4] org.apache.coyote.http11.Http11Processor.service Error parsing HTTP request header\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | Note: further occurrences of HTTP header parsing errors will be logged at DEBUG level.\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | java.lang.IllegalArgumentException: Invalid character found in the request target. The valid characters are defined in RFC 7230 and RFC 3986\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.coyote.http11.Http11InputBuffer.parseRequestLine(Http11InputBuffer.java:479)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:687)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:790)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1468)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\nOPeNDAP_olfs.1.q7u7ndf72w94@dockr002 | at java.lang.Thread.run(Thread.java:748)\nI partially suspect that the log is unrelated to this error.\nWe’re running olfs version 1.17.0, libdap and bes version 3.19.1-1.\nAny help?\nNavid"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Nathan Potter",
     "body": "Hi Navid,\nI looked at this and noticed a couple of things:\n1) Any use of the square bracket notation in the query string causes the \"400 BAD REQUEST” error, regardless of what is requested:\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0:1:1359]\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0]\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?Solar_Zenith_Mean[0]\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dmr?Solar_Zenith_Mean[0]\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dds?Solar_Zenith_Mean[0]\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dods?Solar_Zenith_Mean[0]\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dap?Solar_Zenith_Mean[0]\n2) Dropping the square brackets entirely makes everything cool:\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?Solar_Zenith_Mean\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dmr?Solar_Zenith_Mean\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dds?Solar_Zenith_Mean\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dods?Solar_Zenith_Mean\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.dap?Solar_Zenith_Mean\n3) And replacing the square brackets with their equivalent HTTP escape value works too:\ncurl -i -g https://ladsweb.modaps.eosdis.nasa.gov/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim%5B0%5D\nConclusions:\nAt this point my thinking is that since you are the only person reporting this issue with Hyrax-1.14.0 (which is pretty widely deployed) I am thinking that there must be some kind of security layer in your cloud deployment that is getting rankled but the square brackets in the constraint. I think this especially because none headers returned with the 400 error:\nHTTP/1.1 400\nServer: openresty\nDate: Thu, 24 May 2018 22:40:53 GMT\nContent-Type: application/octet-stream\nTransfer-Encoding: chunked\nConnection: keep-alive\nSet-Cookie: tisSession=a0a52f7f-9e77-4c3e-82b1-5faa85f0c0cb; Path=/; HttpOnly\nStrict-Transport-Security: max-age=31536000; includeSubDomains\nAre generated from Hyrax. In other words, I don’t think the request is getting to Hyrax.\nThe headers make me think that your deployment is in the OpenResty cloud, (possibly using NGINX), which is an environment with which I have zero experience.\nCan you tell me about the environment you are running in?\nPossibilities:\n- Some upstream component (NGINX?) is returning the error or munging the URL\n- There are new security rules in place that we have not yet encountered that may require that we adjust the Data Request Form to do the escaping on the way out.\nI don’t think the stuff in your error log is noise. I think that’s probably related.\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Golpayegani, Navid (GSFC-6190)",
     "body": "Hi Nathan,\nThanks for the quick reply. Sorry I should have included that information as well. We're running opendap in a docker environment. And you're right we've been running 1.14.0 in docker for months now without any issues. For an unknown reason this started last week.\nWe are running a 3 container setup with 1 container being bes and one container being olfs. The third container is nginx (openresty) acting as a load balancer which proxies requests to olfs if the endpoint is /opendap. The opendap endpoint in nginx is very simple:\nlocation /opendap{\nproxy_pass http://opendap/opendap;\n}\nThe olfs container is setup to expose the internal port 8080 as an external port 3001. I tested if it's our nginx server by directly connecting to the container and bypassing nginx. I still get the same result:\n# curl -v -g 'http://dockr002:3001/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0:1:1359]'\n* About to connect() to dockr002 port 3001 (#0)\n* Trying 192.168.55.72...\n* Connected to dockr002 (192.168.55.72) port 3001 (#0)\n> GET /opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0:1:1359] HTTP/1.1\n> User-Agent: curl/7.29.0\n> Host: dockr002:3001\n> Accept: */*\n>\n< HTTP/1.1 400\n< Date: Fri, 25 May 2018 14:42:45 GMT\n< Connection: close\n<\n* Closing connection 0\nAlso to make sure it's not the docker firewall rules causing problems I connected directly into the docker container and ran the command directly against the tomcat running inside the docker container and the problem is still there:\n$ curl -v -g 'http://localhost:8080/opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0:1:1359]'\n* Trying 127.0.0.1...\n* TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 8080 (#0)\n> GET /opendap/allData/6/MOD08_D3/2016/122/MOD08_D3.A2016122.006.2016123095613.hdf.ascii?XDim[0:1:1359] HTTP/1.1\n> Host: localhost:8080\n> User-Agent: curl/7.52.1\n> Accept: */*\n>\n< HTTP/1.1 400\n< Date: Fri, 25 May 2018 14:44:16 GMT\n< Connection: close\n<\n* Curl_http_done: called premature == 0\n* Closing connection 0\nAnd you're right as soon as I remove the brackets or escape them the error seems to go away.\nNavid\n- show quoted text -"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Nathan Potter",
     "body": "Hi Navid,\nSo can you tell me more about the docker containers?\nWhat Tomcat version?\nWhat OS for the OLFS container?\nWhat OS for the BES container?\nThanks,\nN\n- show quoted text -"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Golpayegani, Navid (GSFC-6190)",
     "body": "The olfs container is based off of the tomcat:8-jre8 image. The exact tomcat version is 8.5.31.0. The base OS is Debian 9.4.\nThe bes container is based off of centos:7 image. The bes image is identical to https://github.com/OPENDAP/hyrax-docker/blob/master/hyrax-1.14.0/besd/Dockerfile\nNavid\n- show quoted text -"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Nathan Potter",
     "body": "Hi Navid,\nI understand the problem. I can reproduce it on my dev system, and I think I have a work around for you until the next release arrives.\nI have opened a ticket in our JIRA system for this: https://opendap.atlassian.net/browse/HYRAX-767\nThe Work Around:\nIn the Tomcat configuration file: $CATALINA_HOME/conf/server.xml add the following attribute to your Connector element:\nrelaxedQueryChars=\"&lt;&gt;[\\]{|}\"\nSo you end up with something like:\n<Connector port=\"8080\" protocol=\"HTTP/1.1\"\nconnectionTimeout=\"20000\"\nredirectPort=\"8443\"\nrelaxedQueryChars=\"&lt;&gt;[\\]{|}\"\n/>\nObviously you may have a very different Connector instance, but just poke that one attribute in there and you should be good to go.\nHere’s the relevant Tomcat doc: https://tomcat.apache.org/tomcat-8.5-doc/config/http.html\nI think the fix will make it into our next release. We are in discussion about a date for that, but have not set one yet.\nI hope this gets you up and running!\nSincerely.,\nNathan\n- show quoted text -"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Golpayegani, Navid (GSFC-6190)",
     "body": "Hi Nathan,\nThank you. That was indeed the problem and it fixed it.\nJust out of curiosity. Are you aware if there's any reason why that would suddenly stop working for us or why nobody else has encountered this before? Our server.xml file is version controlled and I just verified it was last modified 4 months ago so it's a bit of a mystery why it would suddenly break.\nThanks again for debugging the problem for us,\nNavid\n- show quoted text -"
    },
    {
     "page": "olfs returns 400 for some requests",
     "name": "Nathan Potter",
     "body": "Hi Navid,\nMy thought is that you might have a look at the Dockerfile used to build the OLFS container. Near/At the top there will typically be a “FROM” statement like this:\nFROM unidata/tomcat-docker:8\nor\nFROM tomcat:8.5-jre8\nEvery time the docker container is built, the FROM statement is evaluated.\nIf the underlying reference image has been updated it is pulled into the build.\nSince both of the above examples reference a sort of “latest” release image as opposed to a specific version like this:\nFROM tomcat:8.5.31-jre8\nThis means that as Apache rolls out new code under Tomcat-8.x (first example) or Tomcat-8.5.x (2nd example) their containers get updated and so do the dependent containers.\nThis is a great plan for security patches, which is exactly what we are encountering here. But, the sudden behavioral changes can be disconcerting to say the least!\nI think in general there has been a big push to be more strict about URL encodings as these unescaped characters have been used to force arbitrary code injection into poorly implemented servers. Apparently the front-line step to mitigate this kind of attack is to utilize correct URL encoding in requests and to reject requests that are not correctly encoded.\nhttps://security.elarlang.eu/request-uri-query-string-and-url-encoding.html\nhttps://perishablepress.com/stop-using-unsafe-characters-in-urls/\nI would be interested to know if the Dockerfile in question references a specific Tomcat version or some variant of “latest” in the FROM line.\nDoes all that make sense to you?\nSincerely,\nNathan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Questions about SST data",
     "name": "kui li",
     "body": "Hi,\nI am a research at UCSB, I would like to download the SST data from your website, from 2002 to 2018. I try to figure out a way to subset only the southern California region instead of downloading the global data. Use this website as an example: https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/JPL/MUR/v4.1/2002/152/20020601090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.html\nIf I want to select only the latitude between 32 to 34 and longitude between -118 to -121. How should I specify the boxes ? Currently, the latitude box seems to allow value between 0 to 17998, then what is the 32 to 34 degree landed in your range? I try to put in 32:1:34, it doesn’t work.\nPlease advise! Thank you\nLi Kui,\nResearch Specialist\n3405 Marine Science Institute\nUniversity of California\nSanta Barbara, CA 93106-6150"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi ,\nI work for Flipcause. We’re a technology company committed to making it easier than ever for nonprofit leaders to increase supporter engagement and advance their missions through technology that was tailored to them.\nWith little to no technical experience or resources required, you can use Flipcause products and services for online and mobile donations, event, program and volunteer registrations, crowdfunding and peer-to-peer fundraising campaigns, and manage your supporter database from a single location.\nA few things that make us different\nFast account setup and integration into any website. We can also create a new website for you if you need one for free\nBest value in the industry\nAll core features and services are included at every pricing level.\nThe lowest guaranteed effective transaction rate of 1.5% for online payments\nUnlimited campaigns and admin users\nNo annual contracts- Risk-Free money back guarantee\nA dedicated success team included with your subscription to tackle time-consuming tasks like creating and launching your campaigns\nIf you're open to a quick conversation, take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you're not interested, let me know and you'll never hear from me again. :)\nThank you for your time and I hope to hear from you soon!\nJoan Ramirez\nOutreach Representative\nFlipcause Inc.\n283 4th Street, Suite 101 Oakland, CA 94607\n(800) 523-1950\nUNSUBSCRIBE: If you would like to stop receiving further messages, reply \"Stop\" to this email.\nHere are all of the features and services that come with your Flipcause subscription (we know, it's a lot)…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nMobile app with event and volunteer check-in\nPeer to peer fundraising\nTeam fundraising\nFull payment processing and refund system\nCredit card swiper/card reader\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated Success Team (with unlimited concierge services included at no additional cost)\nFully built website and web hosting services\nSocial media integration\nUnlimited webmaster services (at no additional cost)\nSSL certification and installation\nFully customizable fundraising pages to market your campaigns\nContact database to keep track of your supporters in one place\nEasy to transfer fundraising activity into Quickbooks"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Call for UGC Approved Journal for (Impact Factor- 5.070)",
     "name": "GJESR",
     "body": "Dear Author/Researcher,\nGlobal Journal of Engineering Science and Researches, invites you to submit your research paper for publishing in Volume 5, Issue 5 (May- 2018).\nIndexing\nResearch ID/Thomson Reuters, Academic Edu, SIS, GIF, I20R, Zenodo (DOI) etc. For more details go the below link\nhttp://gjesr.com/index.html\nImpact Factor- 5.070 (Click here to verify)\nUGC Approved\nSubject Category:\nCover all branches of Engineering and Science\nImportant Dates:\nPaper Submission: 25 May 2018\nReview Results (Acceptance/Rejection) Notification: Within 02 Days after submitting the paper.\nPublished Online: 30 May 2018\nPublication Charges:\nRs 1500/Article ( Upto 3 Authors & 15 pages. Rs 500/Author Above 2 Authors and Rs 50/page above 15 pages).\nRs 300/Author for Hard Copy of Certificate(Optional).\n$80 Author out of India (Upto 5 Authors).\nSubmission Procedure:\nManuscript are invited in MS Word format and to be submitted via mail on gjesrj...@gmail.com or arti...@gjesr.com, Please note that Review Papers/Articles/Thesis/Posters are also acceptable.\nOnce a paper is accepted, authors are assumed to cede copyrights of the paper over to GJESR. All papers will be acknowledged and referred.\nFINAL PAPER SUBMISSION:\nEach final full-text paper (.doc) along with the corresponding signed copyright transfer form should be submitted by email to gjesrj...@gmail.com or articles@gjesr.com.\nRegards,\nGlobal Journal of Engineering Science & Research\narti...@gjesr.com\nwww.gjesr.com"
    },
    {
     "page": "Call for UGC Approved Journal for (Impact Factor- 5.070)",
     "name": "GJESR",
     "body": "- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Gridmet URL stopped working 5/14",
     "name": "Jeff LaBarge",
     "body": "Hello -\nThis URL for downloading Gridmet precip data has been working great for nearly a year, and then suddenly stopped working on Monday 5/14:\nhttp://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_met_pr_1979_CurrentYear_CONUS.nc.ascii?precipitation_amount[14378:14379][58][265]\nNow it always returns the fill value of -32767.0\nDid the URL syntax change or is the service down?\nWe would love to get this fixed as quickly as possible. Please advise.\nThanks!\nJeff LaBarge"
    },
    {
     "page": "Gridmet URL stopped working 5/14",
     "name": "Sheneman, Lucas (sheneman@uidaho.edu)",
     "body": "Thanks Jeff –\nI will make some inquiries and get back to you very soon.\n-Luke\n——\nLuke Sheneman, Ph.D.\nDirector, Northwest Knowledge Network (NKN)\nUI XSEDE Campus Champion\nUniversity of Idaho\nhttps://www.northwestknowledge.net\nshen...@uidaho.edu\nOffice: 208.885.4228 Mobile: 208.669.2248\nFrom: Jeff LaBarge <je...@tuletechnologies.com>\nDate: Thursday, May 17, 2018 at 5:27 PM\nTo: \"ad...@northwestknowledge.net\" <ad...@northwestknowledge.net>, \"sup...@opendap.org\" <sup...@opendap.org>\nCc: jeremy labarge <jeremy...@gmail.com>\nSubject: Gridmet URL stopped working 5/14\n- show quoted text -"
    },
    {
     "page": "Gridmet URL stopped working 5/14",
     "name": "Jeff LaBarge",
     "body": "Thanks for the response! We look forward to hearing back after you've made your inquiries."
    }
   ]
  },
  {
   "name": [
    {
     "page": "DockerCon Next Month",
     "name": "DockerCon",
     "body": "DockerCon is 27 days away!\nDockerCon is right around the corner! Don't miss out on this must-attend IT conference in San Francisco June 12-15. Come gain real world advice from container thought leaders and network with peers on best practices for developing and refining your IT strategy.\nBook this week and make sure to take advantage of our negotiated discount hotel rates for attendees. These rates will expire on Friday May 18th!\nBook Hotel\nNEW THIS YEAR : Registered DockerCon attendees will need to reserve seats for breakout sessions and workshops. The two day training courses have already sold out, but you still have a chance to reserve sessions and the hands-on, instructor led workshops included in the standard full conference pass.\nDon't miss out on top content - register and reserve your sessions and workshops!\nIf you have already registered - build your agenda here!\nRegister Today\nHands-on Labs\nCheck out the Hands-on Labs for this year. Included in Full Conference pass and no RSVP required.\nBring A Colleague\nIt's not too late to bring a coworker or friend to DockerCon! Inquire about group discounts.\nGet Certified\nDockerCon is a great place to become a Docker Certified Associate!\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Download all the time series data using OpenDAP",
     "name": "johnny.ch...@gmail.com",
     "body": "Dear all,\nI am trying to download all the time step from a model but I don't know what is the maximum number of the time step.\nIs there anyway that I can download the without specify the maximum time step of the model?\nThank you very much.\nThis is an example of what I want to achieve:\nNormally i write sth like this to download the data:\nhttps://noresg.norstore.no/thredds/dodsC/esg_dataroot/cmor/CMIP5/output1/NCC/NorESM1-M/rcp26/day/atmos/day/r1i1p1/v20110901/pr/pr_day_NorESM1-M_rcp26_r1i1p1_20060101-20551231.nc.ascii?pr[0:18249][58:69][41:49]\nBut i want to change:\nhttps://noresg.norstore.no/thredds/dodsC/esg_dataroot/cmor/CMIP5/output1/NCC/NorESM1-M/rcp26/day/atmos/day/r1i1p1/v20110901/pr/pr_day_NorESM1-M_rcp26_r1i1p1_20060101-20551231.nc.ascii?pr[all time step][58:69][41:49]\nRegards,\nJohnny"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Error: libcurl was built with SSL disabled, https: not supported!",
     "name": "陈旭",
     "body": "Dear Mr. / Ms.,\nI have installed loaddap.exe in my cumputer successsfully. But when i typt the command of “ x=loaddap('-A', 'http://esgf-data1.diasjp.net/thredds/dodsC/esg_dataroot/cmip5/output1/MRI/MRI-CGCM3/rcp85/day/atmos/day/r1i1p1/v20120701/hfls/hfls_day_MRI-CGCM3_rcp85_r1i1p1_20560101-20651231.nc');” in the MATLAB, the error of \"Error: libcurl was built with SSL disabled, https: not supported!\" appeared. Can you tell me what is the cause of this mistake.\nYours faithfully,\nXu Chen"
    },
    {
     "page": "Error: libcurl was built with SSL disabled, https: not supported!",
     "name": "Nathan Potter",
     "body": "Hi Xu Chen,\nBefore we can be of much assistance we will need to know a couple of things:\n- What version of Matlab are you using?\n- What operating system are you using?\nIt may be that if you have a recent version of Matlab (2012a or newer) that you can use the built in NetCDF interface to access OPeNDAP data.\nThere is a brief explanation and example here:\nhttps://opendap.github.io/documentation/UserGuideComprehensive.html#NetCDFTools\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Error: libcurl was built with SSL disabled, https: not supported!",
     "name": "Nathan Potter",
     "body": "Hi Chen Xu,\nI tried using “curl” to access the dataset and I noticed that the server returned a 302 redirect to the OpenID login (A single sign-on authentication service) endpoint. You’ll need to get authentication credentials from Earth System Grid Foundation (the “esfg” in http://esgf-data1.diasjp.net) and use them to configure your system so that the authentication can happen automatically when you use Matlab.\nI am pretty sure that if you follow the instructions in our master guide you’ll get it working.\nThe relevant part is Section 6.3: Authentication For DAP Clients:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_authentication_for_dap_clients\nIn particular Section 6.3.9: Matlab, Ferret, Other applications that use NetCDF C\nwill have instructions that pertain to accessing resources that require authentication for access.\nLet me know how that goes.\nSincerely,\nNathan\nTo see the redirect response I used curl having it show the http header inline.\ncurl -i http://esgf-data1.diasjp.net/thredds/dodsC/esg_dataroot/cmip5/output1/MIROC/MIROC-ESM-CHEM/historical/day/atmos/day/r1i1p1/v20120710/psl/psl_day_MIROC-ESM-CHEM_historical_r1i1p1_18500101-20051231.nc.dds\nTo get the login page and the authentication challenge (401) I used curl again and had it follow the redirects, save cookies, and finally return the login endpoint page with a status of 401 which means that the server is asking you to authenticate.\ncurl -i -k -n -c cookies -b cookies -L --url http://esgf-data1.diasjp.net/thredds/dodsC/esg_dataroot/cmip5/output1/MIROC/MIROC-ESM-CHEM/historical/day/atmos/day/r1i1p1/v20120710/psl/psl_day_MIROC-ESM-CHEM_historical_r1i1p1_18500101-20051231.nc.dds\n> On May 10, 2018, at 5:16 AM, 陈旭 <chen...@126.com> wrote:\n>\n> Dear Nathan\n> The version of MATLAB is 2012b and the operating system is Windown 7 (32 bit). In order to read the OPeNDAP data, the loaddap_3.7.0.exe has been intalled. The RUL is http://esgf-data1.diasjp.net/thredds/dodsC/esg_dataroot/cmip5/output1/MIROC/MIROC-ESM-CHEM/historical/day/atmos/day/r1i1p1/v20120710/psl/psl_day_MIROC-ESM-CHEM_historical_r1i1p1_18500101-20051231.nc. The erroe is \"Error: libcurl was built with SSL disabled, https: not supported!\".\n> Following your suggestion, I have attempted to to access OPeNDAP data by NetCDF interface in the MATLAB 2016a. The command is \"modis='http://esgf-data1.diasjp.net/thredds/dodsC/esg_dataroot/cmip5/output1/MIROC/MIROC-ESM-CHEM/historical/day/atmos/day/r1i1p1/v20120710/psl/psl_day_MIROC-ESM-CHEM_historical_r1i1p1_18500101-20051231.nc';\" and \"ncid = netcdf.open ( modis );\". A new erroe appeared. \"Error using netcdflib. Unable to open file. File may be corrupt or filename may have invalid characters. If the data source was an OPeNDAP URL, see the OPeNDAP. Troubleshooting section in the Users Guide.\"\n> Thank you for your patience.\n>\n> Sincerely,\n> Chen Xu\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: OPeNDAP Inquiry",
     "name": "jgallagher",
     "body": "On May 2, 2018, at 21:00, Benedict Castro <benedict...@gmail.com> wrote:\nGood day!\nThis is Benedict again, and I just want to ask about file handling in OPeNDAP Hyrax. I just installed it on my PC but can't seem to find a way to upload my files.\nHyrax does not support uploading files. Question: How did you install Hyrax on your PC? Using Docker?\nI'm currently working on netcdf files.\nTo serve netCDF file, just copy them to the machine running hyrax into a directory in or under the Data root directory.\nAlso, how can I access the OPeNDAP server from other workstations?\nUsing HTTP, in a web browser, etc. In addition to Hyrax, you must have Tomcat. The Hyrax server will be accessible by appending ‘opendap’ to the URL that points to your Tomcat server.\nJames\nSorry if these queries may sound basic but I hope you can help me on this matter. Thank you very much.\nBest regards,\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Updates to Atlassian's Privacy Policy",
     "name": "Atlassian",
     "body": "We're updating our Privacy Policy\nThanks for using Atlassian collaboration tools! We're constantly striving to provide a seamless, integrated experience to help your team work smarter and faster. To that end, we want to give you an overview of our updated privacy policy. It's more user-friendly and addresses new data regulations (including GDPR).\nWant to read the full policy? Check it out here.\nLooking for a quick summary of the updates? Read on:\n• Better navigation and user-friendly language. To make the policy easier to understand, we use clear, plain language and examples that illustrate our activities. We reformatted our privacy policy page with active links, so you can quickly find the information that matters most to you.\n• How we integrate our products. We're always improving our products to give you a frictionless and customized experience. The updates to our policy describe the tools we’ve built to make our products smarter and allow you to move seamlessly from one Atlassian product to another.\n• More control over your information. We make it easy for you to control the information you provide to us. Our policy explains how you can make choices about your information, and the measures we’ve put in place to keep your information secure.\n• Using our products for work. Many users have access to our services through their organizations (e.g., their employers), who control their accounts or use of our services. The updated policy clarifies our relationship to these users and explains the tools available to administrators of these users.\nThe new privacy policy goes into effect on May 25, 2018. If you have any questions about these changes, take a look at the FAQs. For questions not addressed by the FAQs, please reach out to us using the contact information provided in the privacy policy.\nWe're proud to be part of your team!\nPrivacy policy • Contact us • Read our blog\nCopyright 2018 Atlassian Pty Ltd. All rights reserved. We are located at 341 George Street, Sydney, NSW, 2000, Australia"
    }
   ]
  },
  {
   "name": [
    {
     "page": "download problem on OPeNDAP",
     "name": "汗粒粒",
     "body": "Dear Sir/Madam,\nI am HLC and I request your help.\nI would like to know the way to download a big amount of data just by one click, if there is.\nI want to download the data from the OPeNDAP. However, the satellite data has too many links, one link per day and I need the data over 9 years, which makes the download work a heavy thing.\nI am looking forward to your Email. Thank you for your time.\nBest regards,\nHLC"
    },
    {
     "page": "download problem on OPeNDAP",
     "name": "Nathan Potter",
     "body": "Hi HLC,\nIn order to help you by providing examples I need to have the URL of a dataset, or of the collection that your are working with.\nIn general I think you will find that “downloading” 9 years of satellite data will be impractical as the data volume will likely be on the order of terabytes. The DAP protocol is designed to allow server side subsetting of the data so that you can request only that which is relevant to your work. Thus, if you have an area of interest on the globe, you can make a calculation to determine the parts of the data arrays that carry the information you want and request just those values. This will greatly decrease the transported data volume and make your data download task more manageable.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "download problem on OPeNDAP",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: \"汗粒粒\" <hlch...@foxmail.com>\nSubject: 回复： [support] download problem on OPeNDAP\nDate: April 26, 2018 at 8:59:32 AM PDT\nTo: \"Nathan Potter\" <n...@opendap.org>\nHi Nathan,\nI appreciate your kind help. However, maybe these days I am unfortunate and as a result, I find a problem again. Although it could be boresome, I would still like to request your help.\nBecause I is using a PC of Win, so I used the VMware to run the shell script (CentOS).\nBut when I used the command \"bash scipt-name\", boom! （I think this is relevant to the black cat. Today, it flashed across in front of me. Meanwhile, I was walking feebly like a mummy）\ndate: invalid option -- 'j'\nTry 'date --help' for more information.\nmytest_sh.sh: line 29: +*86400: syntax error: operand expected (error token is \"*86400\")\ndate: invalid option -- 'j'\nTry 'date --help' for more information.\nmytest_sh.sh: line 29: +*86400: syntax error: operand expected (error token is \"*86400\")\nmytest_sh.sh: line 29: $'\\342\\200\\235': command not found\nmytest_sh.sh: line 31: ..: Is a directory\nmytest_sh.sh: line 32: syntax error near unexpected token `done'\nmytest_sh.sh: line 32: ` done'\nI suppose this command may have something to modify but I am not good at Linux and have no idea about how to do. I have searched on the web but nothing useful.\nfileDate=`date -j -f %s $(($(date -j 010001${year} +%s)+${day}*86400)) +%m%d`\nThank you for your time. I am looking forward to your Email.\nSincerely,\nHLC\n------------------ 原始邮件 ------------------\n发件人: \"Nathan Potter\"<n...@opendap.org>;\n发送时间: 2018年4月26日(星期四) 凌晨1:08\n收件人: \"汗粒粒\"<hlch...@foxmail.com>;\n抄送: \"Nathan Potter\"<n...@opendap.org>;\n主题: Re: [support] download problem on OPeNDAP\nHi HLC,\nWell, using the data request form in the way you describe would be very tedious.\nMost people that want to do this kind of thing employ a small shell script or program to do this work.\nThe challenge here appears to be that the files are organized by year-day (1-365) but the files are named using year, month, and date.\nFor example 2010/172/20100621-JPL_OUROCEAN-L4UHfnd-GLOB-v01-fv01_0-G1SST.nc.bz2\nIt’s easy enough to a simple script to loop through the years and year days, but then each year/year-day pair must be converted into\nmonth and day of month.\n#!/bin/bash\n# Collection URL (base URL) of the data collection\ncollectionUrl=\"https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/L4/GLOB/JPL_OUROCEAN/G1SST”\n# Use this to pick the variable(s) and array subset(s) you wish to retrieve.\nfeature=\"analysed_sst[0][753:1:800][9001:1:9731]”\n# Use this to control the encoding of the response.\n# You can ask for nc (NetCDF3), nc4 (NetCDF-4), ascii (ascii values), dods (DAP2 Data), and json (JSON encoded data)\nresponseType=“ascii\"\n# In this collection 2010 and 2018 are incomplete years and need to be handled differently.\n# Here I only address the complete years\nfor year in {2011..2017}; do\n# You may need to do something extra for leap year, or not depending.\nfor dnum in {1..366}; do\nif [[ $dnum -lt 100 ]] ; then\nif [[ $dnum -lt 10 ]] ; then\nday=\"00${dnum}\";\nelse\nday=\"0${dnum}\";\nfi\nfi\n# determine the dat string for the filename\nfileDate=`date -j -f %s $(($(date -j 010001${year} +%s)+${day}*86400)) +%m%d`\n# Build the request URL\nrequestUrl=\"${collectionUrl}/${year}/${day}/${year}${fileDate}-JPL_OUROCEAN-L4UHfnd-GLOB-v01-fv01_0-G1SST.nc.bz2.${responseType}?${feature}\"\necho \"URL: ${requestUrl}”\n# Get the data\ncurl -s -g $requestUrl > $year.$day.$responseType\ndone\ndone\nCheers,\nNathan\n> On Apr 25, 2018, at 9:11 AM, 汗粒粒 <hlch...@foxmail.com> wrote:\n>\n> Hi Nathan,\n>\n> The following is the URL for the data access.\n> https://podaac.jpl.nasa.gov/dataset/JPL_OUROCEAN-L4UHfnd-GLOB-G1SST?ids=ProcessingLevel:GridSpatialResolution&values=*4*:[0.0%20TO%200.05]&search=GHRSST\n>\n> Because of some troubles, I cannot just put the URL of the dataset on the OPeNDAP. Sorry.\n>\n> I know the way to download wanted data only and that is why I choose the OPeNDAP. What makes me puzzled is that I have to click the small circles to choose variables and insert numbers for ranges, when I select data, which is really boring and time-consuming.\n> And I had tried to change the URL which was usable (URLs have the regular pattern) and download by Downthemall (a downloader of firefox) but my effort failed.\n>\n> Sincerely,\n>\n> HLC\n>\n>\n> ------------------ 原始邮件 ------------------\n> 发件人: \"Nathan Potter\"<n...@opendap.org>;\n> 发送时间: 2018年4月25日(星期三) 晚上9:00\n> 收件人: \"汗粒粒\"<hlch...@foxmail.com>;\n> 抄送: \"Nathan Potter\"<n...@opendap.org>; \"support\"<sup...@opendap.org>;\n> 主题: Re: [support] download problem on OPeNDAP\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "download problem on OPeNDAP",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] download problem on OPeNDAP\nDate: April 26, 2018 at 9:26:24 AM PDT\nTo: 汗粒粒 <hlch...@foxmail.com>\nCc: Nathan Potter <n...@opendap.org>\nHi HLC,\nTry replacing:\nfileDate=`date -j -f %s $(($(date -j 010001${year} +%s)+${day}*86400)) +%m%d`\nWith:\nfileDate=`date -d \"${year}-1-1 +${day} days\" +%m%d`\nI got the idea here: https://stackoverflow.com/questions/42023382/convert-from-year-day-of-year-to-date-in-bash\nIf that fails, read the man page for date ( man date ) and see if you can make sense of it.\nThe point is to convert the year and day-of-year to month (%m) and date-of0month (%d)\nSincerely,\nNathan\n- show quoted text -\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "\"Parent Directory\"",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi. There is a “Parent Directory” showing up at the top level from some of our Hyrax servers, e.g.\nhttps://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/\nwhile others don’t, e.g.\nhttps://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/\nOur servers are the same version (1.13.4). Wonder if there is a quick configuration item somewhere we can use to get rid of it?\nThe link used to show up at subdirectory levels but in the later Hyrax versions they are gone. Wonder if there is a reason for that?\nThanks.\n-Fan"
    },
    {
     "page": "\"Parent Directory\"",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nThis server:\n> https://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/\nAppears to be down, and as you said, the goldsmr4.gesdisc.eosdis.nasa.gov system doesn’t have the link.\nI remember dropping the “Parent Directory” link for some technical reason a long time ago, but\nI can not recall what that reason was..\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "\"Parent Directory\"",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nI finally got a response from goldsmr5 and I do see the \"Parent Directory” link. I think this might be an indication that that server has a mix of 1.13.4 files along with some older ones.\nI think you might have some luck if you try the following:\n1) Get a fresh copy of the OLFS web archive file:\nhttps://www.opendap.org/pub/olfs/olfs-1.16.3-webapp.tgz\n2) Unpack the .tgz file and locate the opendap.war file within.\n3) Shutdown Tomcat\n4) rm -r $CATALINA_HOME/webapps/opendap*\n5) If there is a directory /etc/olfs/docs then move it to /etc/olfs/docs.OLD\n6) Copy the opendap.war file from the download to $CATALINA_HOME/webapps\n7) Start Tomcat\nIf the link is gone then it worked.\nIf you renamed the directory /etc/olfs/docs in step 5, try changing it back to the old name and restarting Tomcat.\nIf the link comes back then that we need to do more work.\nMinimally you might at that this point:\n1) Move /etc/olfs/docs to /etc/olfs/docs.OLD\n2) copy the directory $CATALINA_HOME/webapps/opendap/docs to /etc/olfs\n3) Restart Tomcat\nBut let me know if you get to this point, let’s hope the first bit resolves it.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "\"Parent Directory\"",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Thanks Nathan - we'll try that. We took the server down for a brief while for some testing - sorry.\n-Fan\n- show quoted text -"
    },
    {
     "page": "\"Parent Directory\"",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nWe cleared out the BES cache and it appeared to have removed the link. We did use the same area for BES cache in a few upgrades so it is possible that some old cached stuff were there, but thought restarting BES would generally clean the cache.\nI'll let you know if this happens again before our next upgrade. Thanks.\n-Fan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Hi,\nI would like to configure a server in order to made available some\nmetocean data.\nI can compute .nc files and verify them with some software (i.e.\nPanoply, netcdf python modules...) and access them from remote computer.\nBut now i would like to get (from a remote computer) just few parameters\nand/or grid points. And I have to prepare associates .das .dds and .dods\nfiles.\nIf .das and .dds are easy to prepare (Even if for the moment I don't\nprepare these files automatically with the .nc), my problem is to\nprepare the .dods file, and I don't know how tp proceed.\nMay be my question is very basic (and idiotic ) but please could you\ntell me how to proceed or which software is useful for that.\nThank you in advance\nBest regards\nDenis BONICEL\nGlobOcean Sarl"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Hi Denis,\nAre you invested in making your own implementation of a DAP server, or would you be willing to utilize existing software to meet you needs?\nIf your answer “yes” to the latter then you might consider utilizing the Hyrax Data Server or the THREDDS Data Server to serve your NetCDF files. Both of these servers can provide all of the nominal DAP2 responses (.das, .dds, .dods, and more) for your NetCDF encoded data.\nYou can find more about them here:\nHyrax: https://www.opendap.org/software/hyrax/1.14.0\nTDS: https://www.unidata.ucar.edu/software/thredds/current/tds/\nIf on the other hand you are interested in writing your own software we can help guide with this too.\nPlease let me know if this was helpful!\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Hi Nathan,\nand thank you very much for your prompt answer and your help.\nThe idea would be to write our own software, as we have deployed an\napache server.\nWe have encoded metocean data into netcdf files using Fortran programs,\nand we will put these files on our server.\nIf you can help me to write a software able to compute associated .das\n.dds and .dods files it would be very useful.\nAs i said in my previous e-mail, .das and .dds files are not really a\nproblem, but I have no idea of what is written in the .dods file and how\nto proceed to obtain it.\nThe format of .dods file seems to be a first part similar to the .dds\nfile, then two lines in binary format (I don't knwon what it is\ncontained in theses lines) and a the last part similar to the .nc file\n(Except the lines containing the metadata informations).\nBut may be it is more simple to use Hyrax or Thredds software ?\nThanks again\nSincerely\nDenis\n- show quoted text -"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Hi Denis,\nHere is the specification document for DAP2:\nhttps://cdn.earthdata.nasa.gov/conduit/upload/512/ESE-RFC-004v1.1.pdf\nWhich contains a formal definition of the content of the DAP2 Data Response (aka “.dods”).\nHowever, I think it may be more complex than you realize, as supporting the constraint syntax which allows users to request subsets of the data from the server is something you may also desire. Without it every request will return the entire contents of the source data file which may not be optimal for users (gettin more data than they want/need) and for your service (sending more data to the user than required). Additionally, the various DAP enabled client software packages will probably have difficulty working with your service if they cannot request specific ranges of specific variables.\nBoth Hyrax and the TDS work well with the Apache server. Since we at OPeNDAP write and support Hyrax I will limit my discussion to it, although the TDS is similar, at least w.r.t. how it integrates with Apache.\nYou can see our online documentation for Hyrax here:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html\nAnd the Apache server integration is covered in this section:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#apache-integration\nIf you still want to write your own server then we should discuss which language(s) you wish to work in, as there are a number of available DAP libraries that may do a lot of the more difficult work for you.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Thank you Nathan,\nyes you are right we don't want return the entire contents at each\nrequest, but only subsets of data.\nAnd yes I suppose it's more complex than i expected...\nFirst I have to read all the links you sent to me and then I come back\nto you.\nThank you very much for your help !\nSincerely\nDenis\n- show quoted text -\n--"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "James,\nDoes he need to build from source on Ubuntu?\nN\nBegin forwarded message:\nFrom: Denis <denis....@globocean.fr>\nSubject: Re: [support] Information request\nDate: March 29, 2018 at 5:32:39 AM PDT\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\njust to say I am trying to install Hyrax on a Ubuntu 16.04 OS, but I can't install libdap-3.19.0-1.el6.x86_64.rpm\nSome librairies are required, but when I check with the command \"locate\" most of these librairies are already installed...\nSo i have to check this problem before being able to install and use Hyrax.\nI send you the screenshot of my problem, maybe you have the key to resolve it !\n- show quoted text -\n--\nDenis BONICEL GlobOcean Sarl\nEmail : denis....@globocean.fr\nMobile : +33 (0) 699 077 520\nPhone : +33 (0) 494 876 347\nGlobOcean - 45, avenue de la première armée, Espace Euros, Draguignan, 83300, France\nwww.globocean.com"
    },
    {
     "page": "Information request",
     "name": "jgallagher",
     "body": "On Apr 1, 2018, at 12:01, Nathan Potter <n...@opendap.org> wrote:\nJames,\nDoes he need to build from source on Ubuntu?\nThere’s something odd about this - why/how is he using yum and RPMs on Ubuntu? I thought that was a apt-get/deb (only) system. When we build on Ubuntu (travis uses that) we always load otehr packages in with apt-get and those packages are Debian (.deb) packages, not RPMs.\nHere’s some info on the web: https://askubuntu.com/questions/2988/how-do-i-install-and-manage-rpms\nSo, it might be that he does need to build from source. Or use a CentOS/RedHat/Fedora. Since the system is running in virtual box, it seems he could do that.\nJames\n- show quoted text -\n<InstallHyrax.txt>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Hi Nathan,\nI try to install Hyrax, but I have many difficulties. I try to install it on Ubuntu 16.04.\nIs it possible to have your help to have a dedicated program (Fortran ? Python ?) to build .dods files corresponding to our .nc files ?\nSincerely\nDenis\n- show quoted text -\n--\nDenis BONICEL GlobOcean Sarl\nEmail : denis....@globocean.fr\nMobile : +33 (0) 699 077 520\nPhone : +33 (0) 494 876 347\nGlobOcean - 45, avenue de la première armée, Espace Euros, Draguignan, 83300, France\nwww.globocean.com"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Hi Denis,\nThe RPM binaries that we offer are suitable for use with Redhat/Fedora systems like CentOS. The RPM files don't work with Ubuntu. However the Hyrax software is regularly built on Ubuntu by our continuous integration testing - doing so yourself should not be too difficult.\nIf you wish we have detailed instruction for building on a CentOS system here:\nhttp://docs.opendap.org/index.php/Hyrax_GitHub_Source_Build\nI recommend that you do the “manual build” described in section 3.\nOn Ubuntu you’ll need to use \"apt-get” and not “yum” to load up the system dependencies. You might have a look at the YaML file that drives our continuous integration tests for both libdap4 and the BES to see, on lines 32-39, what packages get installed before the build:\nLibdap: https://github.com/OPENDAP/libdap4/blob/master/.travis.yml\nBes: https://github.com/OPENDAP/bes/blob/master/.travis.yml\nGetting our production rules to generate Ubuntu packages in addition to RPM files is definitely on our list of things we would like to do.\n> On Apr 5, 2018, at 2:12 AM, Denis <denis....@globocean.fr> wrote:\n>\n> Hi Nathan,\n>\n> I try to install Hyrax, but I have many difficulties. I try to install it on Ubuntu 16.04.\n>\n> Is it possible to have your help to have a dedicated program (Fortran ? Python ?) to build .dods files corresponding to our .nc files ?\nTo be honest getting a server built and installed is your best path to this goal. If getting Hyrax sorted out on Ubuntu is too cumbersome you might try the TDS:\nhttps://www.unidata.ucar.edu/software/thredds/current/tds/\nWhich may be simpler to install and will, like Hyrax, read your NetCDF files.\nSincerely,\nNathan\n- show quoted text -\n> Email : denis....@globocean.fr Mobile : +33 (0) 699 077 520 Phone : +33 (0) 494 876 347\n>\n> GlobOcean - 45, avenue de la première armée, Espace Euros, Draguignan, 83300, France\n>\n> www.globocean.com\n>\n>\n- show quoted text -"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Hi Denis,\nWhere you able to make any progress?\nAre you stuck? Can I help?\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Hi Nathan,\nI have tried in vain to install Hyrax on few system (Using virtual box): on ubuntu, CentOS6.9, CentOS7\nI have tried automatic, semi-automatic and manual build, and I have had always something wrong (Librairies and/or files missing).\nAs I am not used enough with this kind of installation I asked someone to do it, and he would do that next week.\nI will tell you if it's ok or if we will need your help.\nOnce build, what will we have to do to prepare .das . dds and .dods files associated to our .nc ?\nSincerely\nDenis\n- show quoted text -\n--\nDenis BONICEL GlobOcean Sarl\nEmail : denis....@globocean.fr\nMobile : +33 (0) 699 077 520\nPhone : +33 (0) 494 876 347\nGlobOcean - 45, avenue de la première armée, Espace Euros, Draguignan, 83300, France\nwww.globocean.com"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Hi Denis,\nWell - that is sad news that you have been unsuccessful with building Hyrax. We routinely build and delpy to CentOS-6.x and CentOS-7.x using the instructions posted here:\nhttp://docs.opendap.org/index.php/Hyrax_GitHub_Source_Build\nSeeing as that has not been easy for you, would a Docker image be useful?\nWe have one for our current release here:\nhttps://hub.docker.com/r/opendap/hyrax/\nAnd instructions for how to use it to service your data here:\nhttps://github.com/OPENDAP/hyrax-docker/blob/master/README.md\nAnd I will be happy to engage with the person who is working on building the code, as you wish.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Hi Nathan,\nI have installed the docker image on a CentOS7\nPlease could you explain me what is the procedure to compute .dods files ?\nSincerely\nDenis\n- show quoted text -\n--\nDenis BONICEL GlobOcean Sarl\nEmail : denis....@globocean.fr\nMobile : +33 (0) 699 077 520\nPhone : +33 (0) 494 876 347\nGlobOcean - 45, avenue de la première armée, Espace Euros, Draguignan, 83300, France\nwww.globocean.com"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Hi Denis,\nI think you may have be holding a misconception about how the server operates: The responses ending in .dds, .das, .dods, and so on are dynamically generated. There are no files to generate, the server software builds each response at the time the user requests it.\nIn this document:\nhttps://github.com/OPENDAP/hyrax-docker/blob/master/README.md\nThe section titled \"Using Hyrax docker to serve your data” provides a simple example of how to mount part of the host systems local file system (in the example the local filesystem directory is \"/home/mydata\") onto the docker container so that it becomes the data volume for the server:\ndocker run --hostname hyrax --port 8080:8080 --volume /home/mydata:/usr/share/hyrax --name=hyrax_container hyrax_image\nIf your data files are in NetCDF format, and the files names end in “.nc”, then the server will recognize them and will serve them and all of their associated DAP responses without further configuration.\nDoes that make sense?\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Information request",
     "name": "Denis",
     "body": "Hi Nathan,\nyes i thought i need to compute .dods (and .das .dds) files to be able to operate the server and i was wrong.\nWe don't have installed (for the moment) Hyrax on our server.\nI have installed pydap + apache on it, and it seems it works fine, so for the moment we start our server with this solution.\nThank you very much for all your advices and for being available !\nSincerely\n- show quoted text -\n--\nDenis BONICEL GlobOcean Sarl\nEmail : denis....@globocean.fr\nMobile : +33 (0) 699 077 520\nPhone : +33 (0) 494 876 347\nGlobOcean - 45, avenue de la première armée, Espace Euros, Draguignan, 83300, France\nwww.globocean.com"
    },
    {
     "page": "Information request",
     "name": "Nathan Potter",
     "body": "Denis,\nThat’s great news! I’m glad you got that sorted out.\nIf you have questions about the DAP protocol, or you choose to work with Hyrax in the future please let us know if we can help.\nSincerely,\nNathan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "We are missing some important information about - Open Dap in www.montana-webbusiness.com",
     "name": "Montana Business Directory - Support team",
     "body": "Hello!\nWe are missing some important information about your business: Open Dap in our FREE business directory http://www.montana-webbusiness.com/company-Open-Dap_92757 - Please take five minutes to update and improve your company listing (This action is Free).\nMore detailed and precise information (Business description, services, products) get better visibility on search engines. We have more than 50.000 visits per day from Searchs Engines. This will help you to find thousands of potential customers on the internet.\nThis is the form that allows you to improve your listing:\nhttp://www.montana-webbusiness.com/edit.php?company_id=92757\nThis is your current profile on line: http://www.montana-webbusiness.com/company-Open-Dap_92757\nThank you!\nMontana Business Directory Support Team.\nhttp://www.montana-webbusiness.com\nin...@montana-webbusiness.com\nEarth Business Directory is a Free human edited Business Directory ( working in 25 Countries ):\nArgentina - Australia - Belgium - Bolivia - Brasil - Canada - Chile - Colombia - Costa Rica - Ecuador - France - India - Ireland - Mexico - New Zealand - Paraguay - Peru - Portugal - South Africa - Spain - Switzerland - United Kingdom - United States of America - Uruguay - Venezuela\nFor more information visit http://www.earthbusinessdirectory.com"
    },
    {
     "page": "We are missing some important information about - Open Dap in www.montana-webbusiness.com",
     "name": "Montana Business Directory - Support team",
     "body": "- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Please, I just would like to download the MSWEP data. Why is it so complicated? I have read notes, ALL yours \"read me\", installed plenty of I do not know what, but I cannot obtain the data. Are you sure that is the easiest way like it is informed? Don't you have an FTP, please?",
     "name": "Victor Paca",
     "body": "Dear,\nPlease, I just would like to download the MSWEP data.\nWhy is it so complicated?\nI have read notes, ALL yours \"read me\", installed plenty of I do not know what, but I cannot obtain the data. Are you sure that is the easiest way like it is informed?\nDon't you have an FTP, please?\nWith kind regards,\nVictor Paca"
    },
    {
     "page": "Please, I just would like to download the MSWEP data. Why is it so complicated? I have read notes, ALL yours \"read me\", installed plenty of I do not know what, but I cannot obtain the data. Are you sure that is the easiest way like it is informed? Don't you have an FTP, please?",
     "name": "Victor Paca",
     "body": "I managed here.\nMy apologies, and thank you for providing this valuable data.\nWith kind regards,\nVictor Paca\n- show quoted text -"
    },
    {
     "page": "Please, I just would like to download the MSWEP data. Why is it so complicated? I have read notes, ALL yours \"read me\", installed plenty of I do not know what, but I cannot obtain the data. Are you sure that is the easiest way like it is informed? Don't you have an FTP, please?",
     "name": "jgallagher",
     "body": "On Apr 23, 2018, at 07:12, 'Victor Paca' via User Support <sup...@opendap.org> wrote:\nI managed here.\nMy apologies, and thank you for providing this valuable data.\nNo worries; Glad it’s working for you.\nJames\nWith kind regards,\nVictor Paca\nOn Monday, April 23, 2018, 5:03:58 AM GMT+2, Victor Paca <victo...@yahoo.com> wrote:\nDear,\nPlease, I just would like to download the MSWEP data.\nWhy is it so complicated?\nI have read notes, ALL yours \"read me\", installed plenty of I do not know what, but I cannot obtain the data. Are you sure that is the easiest way like it is informed?\nDon't you have an FTP, please?\nWith kind regards,\nVictor Paca\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "download SODA 2.2.4",
     "name": "victor estella-perez",
     "body": "Hi,\nmy name is Victor Estella-Perez, a post-doc researcher at LOCEAN in France.\nI am trying to download soda 2.4.4 data set with the following command line:\nwget http://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_c71f_e12b_37f8.nc\nBut the error I am receiving is the following:\nResolving apdrc.soest.hawaii.edu... 128.171.151.240\nConnecting to apdrc.soest.hawaii.edu|128.171.151.240|:80... connected.\nHTTP request sent, awaiting response... 500\n2018-04-23 11:30:03 ERROR 500: (no description).\nAny idea what am I doing wrong?\nBest,\nVictor"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Error in reading OPeNDAP file through MATLAB",
     "name": "faak...@colorado.edu",
     "body": "Hello\nI need help with opendap server access at CYGNSS server among other NASA web servers. My code works fine for the example codes provided at your MATLAB documentation page and for other ‘modis’ examples. It also runs fine for a local netCDF file access but does not work for the CYGNSS data that I’m trying to access remotely from their opendap server. Thanks for your help.\nI’m trying to access CYGNSS files on available on this opendap data URL:\nhttps://podaac-opendap.jpl.nasa.gov:443/opendap/allData/cygnss/L1/v2.0/2018/001/cyg01.ddmi.s20180101-000000-e20180101-235959.l1.power-brcs.a20.d20.nc\nThe entire data can be accessed at the server at:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/cygnss/L1/v2.0/\nI used the instructions at your page:\nhttps://www.opendap.org/about/news/matlab-2012a-support\nand at the CYGNSS data access website for a netCDF matlab reader downloaded from:\nftp://podaac.jpl.nasa.gov/common/sw/generic_nc_readers/matlab/\nMy code is an exact copy (except for file names) of the MATLAB reader from the ftp server above. The code is copied below:\n%File 1: Calls read_nc_file_struct.m also given below\nfilename = 'https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/cygnss/L1/v2.0/2018/001/cyg01.ddmi.s20180101-000000-e20180101-235959.l1.power-brcs.a20.d20.nc'\n[finfo outstrct]=read_nc_file_struct(filename)\n%read_nc_file_struct.m file\nfunction [finfo outstrct]=read_nc_file_struct(fnam)\n%FILENAME: read_nc_file_struct\n%\n%DESCRIPTION:\n%This file contains one Matlab program that is a high level NetCDF reader. It will read any NetCDF file\n%inputted. If the file is properly formatted, CF or another standard format, it will apply offsets and scaling\n%automatically and will replace the fill values with NaNs. Otherwise you will need to do this manually.\n%This program will output all of the variables in the file into a\n%structure.\n%\n%USAGE: To run this program, use the following command:\n% >> [finfo outstrct] = read_nc_file('filename');\n%\n%INPUTS:\n%fnam = The filename of the file you would like to read, with the full path,\n%as a string\n%\n%OUTPUTS:\n%1. finfo = File information, as a structure, with the Global and Variable Attributes\n%\n%2. outstrct is a structure containing all of the variables within the\n% specified file. The first field in the structure is the filename\n%\n%3. This function will output Variable and Global Attributes to the screen. If you do\n% not want this feature comment line 38.\n%\n%NOTES:\n%This code was written with Matlab Version 7.13.0.564 (R2011b)\n%\n%19 April 2012\n%======================================================================\n% Copyright (c) 2012, California Institute of Technology\n%======================================================================\nncdisp(fnam) %outputs Variable and Global Attributes to the screen\nfinfo=ncinfo(fnam); %gets Variable and Global Attribute information from the file\nminargs=1; %make sure that the right number of fields are inputted and outputted\nmaxargs=2;\noutstrct=struct('filename',fnam); %first field in the structure is the filename\nnargoutchk(minargs,maxargs) %check if more than just finfo is declared but make sure that the number of variables declared is not more than what is in the file\nfor i=1:length(finfo.Variables) %loop over the number of variables in the file\neval(['a=ncread(fnam,''' finfo.Variables(i).Name ''');']) %reads in variable data\noutstrct=setfield(outstrct,finfo.Variables(i).Name,a);\nclear a\nend\nend %ends function\n%End of read_nc_file_struct.m file\nThanks for your help.\nFarah Akhtar\nGraduate Student\nCenter for Environmental Technology\nDepartment of Electrical Engineering\nUniversity of Colorado, Boulder\n425 UCB, Boulder CO-80309\nPh # 720-338-5106"
    },
    {
     "page": "Error in reading OPeNDAP file through MATLAB",
     "name": "Nathan Potter",
     "body": "Hi Farah,\nI could use some additional information:\n- What version of Matlab are you running?\n- Was there an error message or some kind of error output (pop-up window?) that you can forward to me?\nAlso, you might try changing URL to change from the https protocol to http by modifying the URL like this:\nhttp://podaac-opendap.jpl.nasa.gov/opendap/allData/cygnss/L1/v2.0/2018/001/cyg01.ddmi.s20180101-000000-e20180101-235959.l1.power-brcs.a20.d20.nc\nAnd see if that helps.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Error in reading OPeNDAP file through MATLAB",
     "name": "Dan Holloway",
     "body": "Nathan,\nThe current revision of Matlab can access this dataset, and the DAS states its a Netcdf-4.3.3.1 which I wasn’t able to access using an older revision of Matlab. Looks like this might become more of an issue and we need to update the document he referenced. As always dealing with version incompatibilities is a PITA.\nDan\n- show quoted text -"
    },
    {
     "page": "Error in reading OPeNDAP file through MATLAB",
     "name": "Peter Cornillon",
     "body": "> On Apr 19, 2018, at 4:11 PM, Dan Holloway <dholloway@OPeNDAP.org> wrote:\n>\n> Nathan,\n>\n> The current revision of Matlab can access this dataset, and the DAS states its a Netcdf-4.3.3.1 which I wasn’t able to access using an older revision of Matlab. Looks like this might become more of an issue and we need to update the document he referenced. As always dealing with version incompatibilities is a PITA.\nThat’s not Rhode Islandeese for Peter is it?\n- show quoted text -\n—\nPeter Cornillon\n215 South Ferry Road Telephone: (401) 874-6283\nGraduate School of Oceanography\nUniversity of Rhode Island\nNarragansett, RI 02882 USA"
    }
   ]
  },
  {
   "name": [
    {
     "page": "info request",
     "name": "Jorge Magalhães",
     "body": "Dear Sirs,\nI’m trying to download mursst data for a particular region between 2012 and 2014, hence avoiding the very large amounts of data in the global netcdfs. To do so I’m using MATLAB and the ‘customized’ URLs available from your opendap Server Dataset Access Form. However, even when trying to download just ‘time’ the URL:\nhttps://opendap.jpl.nasa.gov/opendap/hyrax/allData/ghrsst/data/GDS2/L4/GLOB/JPL/MUR/v4.1/2018/107/20180417090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc?time[0:1:0]\nseems to try and download the all thing instead. Can you please advise.\nThank you very much for your efforts in advance.\nKind Regards,\nJorge Magalhaes.\nhttp://jmagalhae0.wixsite.com/internal-waves-/contact"
    }
   ]
  },
  {
   "name": [
    {
     "page": "[IDV #LKR-160339]: Why no love?",
     "name": "Unidata netCDF Java Support",
     "body": ">\n> I got the error (below) when trying to access a Level 2 data set at PODAAC.\n>\n> With the exception of \"time\" all of the variables in the dataset are\n> Grids with a single Map of \"time”.\n>\n> https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc.dds\n>\n> Could this arragement may be the source of the issue?\n>\n> Otherwise I am concerned that the error message \"handshake alert:\n> unrecognized_name\" may point to some deeper issue with protocol\n> compatibility that we may need to examine.\n>\n> Thanks,\n>\n> Nathan\n>\n> There was an error loading the data:\n> Error creating data source:file.grid with:\n> https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\n>\n> Grid data source failed making data set:\n> https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\n>\n> handshake alert: unrecognized_name\n>\n> = = =\n> Nathan Potter\nGreetings Nathan,\nIt looks like there is an issue with the SSL certificate being used by the PO.DAAC. I\nthink the main thing is that the name in the SSL certificate doesn’t match with url domain,\nwhich starting with Java 7, this kind of error will occur. If they are using apache, I think this can\nbe addressed by using a ServerAlias in the Apache config.\nAs a test, I added the -Djsse.enableSNIExtension=false flag when starting up toolsUI, I can\nhit the dataset without any issues.\nCheers,\nSean\nTicket Details\n===================\nTicket ID: LKR-160339\nDepartment: Support netCDF Java\nPriority: Normal\nStatus: Open\n===================\nNOTE: All email exchanges with Unidata User Support are recorded in the Unidata inquiry tracking system and then made publicly available through the web. If you do not want to have your interactions made available in this way, you must let us know in each email you send to us."
    },
    {
     "page": "[IDV #LKR-160339]: Why no love?",
     "name": "Nathan Potter",
     "body": "Hi Sean,\nThanks for the reply - I forwarded your response to PODAAC and we’ll see what they figure out. What you said makes sense to me - I figured the root cause to be some issue with the https configuration as everything seems to work fine.\nThanks again!\nCheers,\nNathan\n- show quoted text -\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Introducing Docker Enterprise Edition 2.0",
     "name": "Docker",
     "body": "Hello ,\nWe are excited to share that Docker Enterprise Edition 2.0 (Docker EE), the most advanced enterprise-ready container platform in the market, is now available.\nDocker EE 2.0 is a significant leap forward in our enterprise-ready container platform, designed to give you the broadest choice around orchestrators, application types, operating systems, and clouds to support the requirements of global organizations. Read the blog to learn more or start the hosted trial.\nJoin us and thousands of your peers for the Virtual Event: Unveiling Docker Enterprise Edition 2.0 in your time zone on:\nWednesday, April 25th at 11AM PDT\nThursday, April 26th at 9AM BST\nAttend this exclusive event to learn:\nHow enterprises are leveraging Docker EE 2.0 to accelerate their digital and multi-cloud initiatives\nHow Docker can help you on your containerization journey\nRegister now* to chat live with Docker experts, view the latest demos and leave the event with a blueprint for your containerization strategy that can support your ever-changing business environment.\nThe Docker Team\n*Space is Limited. Be sure to select the event in your local time zone.\nIf you previously registered for the Docker Enterprise Container Platform Virtual Event, it has been renamed to: Unveiling Docker Enterprise Edition 2.0.\nRegister Now\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "An odd issue...",
     "name": "Nathan Potter",
     "body": "Greetings,\nI have had two support requests in the past 24 hours in which a user was unable to utilize a DAP client to retrieve data from:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/\nIn my tests I could not retrieve data using Panoply, IDV, Tools-UI, or Matlab R2016b\nHowever, Matlab R2017b and R2018a both work.\nAnd I think the problem lies in the HTTPS/SSL/TLS stack.\nFor example, this dataset:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nCauses Panoply to issue this error message:\nThere was an error loading the data:\nError creating data source:file.grid with: https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nGrid data source failed making data set: https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nhandshake alert: unrecognized_name\nAnd if I change the the protocol to HTTP:\nhttp://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nIt works fine in Panoply. Which is deeply perplexing as this URL simply returns a 302 redirect to the HTTPS endpoint for the dataset.\nI am wondering if a deployment configuration change at PODAAC could have exposed a bug in the NetCDF code.\nCommand line curl works just fine for all of this which might also point to the UNIDATA code simply because, in my experience, curl is pretty intolerant of misconfigured servers…\nAlthough I am not really in a position to take any action to fix this, I would really appreciate it if I could be kept in the loop for the solution, if one materializes.\nThanks,\nNathan\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "An odd issue...",
     "name": "PO.DAAC User Services",
     "body": "Hi Nathan,\nwhat version of Panoply are you using?\nI could not re-produce the problem you reported on my side.\nFor this example,\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nSee screenshot attached.\nThanks,\nSandra\n---------------------------------------------------------\nJet Propulsion Laboratory / Physical Oceanography DAAC\nEmail: pod...@podaac.jpl.nasa.gov\nWebsite: http://podaac.jpl.nasa.gov\n---------------------------------------------------------\nTicket History\nNathan Potter (Client) Posted On: 13 April 2018 02:05 PM\n- show quoted text -\nTicket Details\nTicket ID: 82994\nDepartment: PO.DAAC\nType: Feedback\nStatus: Waiting for User\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?"
    },
    {
     "page": "An odd issue...",
     "name": "Unidata netCDF Support",
     "body": "The following ncdump command also appears to work using the latest\ngithub master for netcdf-c.\nncdump -h https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nCan anyone indicate what version of the netcdf-c library is producing these\nfailures?\n> Hi Nathan,\n>\n> what version of Panoply are you using?\n>\n> I could not re-produce the problem you reported on my side.\n>\n> For this example,\n>\n> https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\n>\n> See screenshot attached.\n>\n> Thanks,\n> Sandra\n>\n>\n> ---------------------------------------------------------\n> Jet Propulsion Laboratory / Physical Oceanography DAAC\n> Email: pod...@podaac.jpl.nasa.gov\n> Website: http://podaac.jpl.nasa.gov\n> ---------------------------------------------------------\n>\n> Ticket History\n> ===================\n> Nathan Potter (Client) Posted On: 13 April 2018 02:05 PM\n>\n> ===============================================================\n- show quoted text -\n> ---------------------------------\n> Ticket ID: 82994\n> Department: PO.DAAC\n> Type: Feedback\n> Status: Waiting for User\n> Priority: Normal\n>\n> Helpdesk: https://support.earthdata.nasa.gov/index.php?\n>\n>\n=Dennis Heimbigner\nUnidata\nTicket Details\n===================\nTicket ID: JVY-917863\nDepartment: Support netCDF\nPriority: Normal\nStatus: Open\n===================\nNOTE: All email exchanges with Unidata User Support are recorded in the Unidata inquiry tracking system and then made publicly available through the web. If you do not want to have your interactions made available in this way, you must let us know in each email you send to us."
    },
    {
     "page": "An odd issue...",
     "name": "Nathan Potter",
     "body": "Hi Sandra,\nThe good folks at UNIDATA sent this comment - which makes sense to me as being the root issue:\n> Greetings Nathan,\n>\n> It looks like there is an issue with the SSL certificate being used by the PO.DAAC. I\n> think the main thing is that the name in the SSL certificate doesn’t match with url domain,\n> which starting with Java 7, this kind of error will occur. If they are using apache, I think this can\n> be addressed by using a ServerAlias in the Apache config.\n>\n> As a test, I added the -Djsse.enableSNIExtension=false flag when starting up toolsUI, I can\n> hit the dataset without any issues.\n>\n> Cheers,\n>\n> Sean\nThanks,\nNathan\n> On Apr 13, 2018, at 11:26 AM, PO.DAAC User Services <support...@earthdata.nasa.gov> wrote:\n>\n> Hi Nathan,\n>\n> what version of Panoply are you using?\n>\n> I could not re-produce the problem you reported on my side.\n> For this example,\n> https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\n>\n> See screenshot attached.\n>\n> Thanks,\n> Sandra\n>\n>\n> ---------------------------------------------------------\n> Jet Propulsion Laboratory / Physical Oceanography DAAC\n> Email: pod...@podaac.jpl.nasa.gov\n> Website: http://podaac.jpl.nasa.gov\n> ---------------------------------------------------------\n>\n> Ticket History\n> Nathan Potter (Client) Posted On: 13 April 2018 02:05 PM\n>\n>\n- show quoted text -\n> Ticket ID: 82994\n> Department: PO.DAAC\n> Type: Feedback\n> Status: Waiting for User\n> Priority: Normal\n>\n> Helpdesk: https://support.earthdata.nasa.gov/index.php?\n> <Screen Shot 2018-04-13 at 11.16.10 AM.png>\n- show quoted text -"
    },
    {
     "page": "An odd issue...",
     "name": "Nathan Potter",
     "body": "Hi Sandra,\nI neglected to answer your question: I have been using Panoply 4.6.1 (build G9W2)\nBut I think Sean’s point (in my previous response) should be investigated.\nThanks,\nNathan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Discover Your Containerization Strategy at DockerCon 2018",
     "name": "DockerCon",
     "body": "Learn how Docker, Kubernetes, serverless and AI come together at DockerCon 2018\nDockerCon is back and better than ever as we come home to San Francisco June 12-15 for a weeks full of all things Docker!\nRegardless of where you are in your containerization journey, DockerCon is the ultimate place to discover what the Docker platform, industry leaders, and experts can do for you, your team, and your company.\nAt DockerCon, you’ll get access to:\nOne-on-one plus group sharing experiences and expert advice on your containerization journey and how to advance to the next stage\nDocker’s unique community of developers, IT operators and leaders who can help you determine what Docker can do for you and your company\nOpportunities to attend subject specific breakouts relevant to you, Hallway Tracks, Trainings, Certifications, Hands on Labs and more regardless of your experience\nAccess to Docker team, industry professionals and container experts who can help you determine and develop your containerization strategy\nRegistration is now open and we’re excited to welcome container enthusiasts both old and new to DockerCon 2018. Let’s get this journey started! Use code ContainThis for 10% your registration until May 1st, 2018.\nRegister Now\nDockerCon is the best place to get fully up to speed on all things Docker and Container Industry related. Send your team and take advantage of our group discounts of up to 20% off.\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "[#82991]: Re: [support] Water Depth-Western Basin of Lake Erie",
     "name": "PO.DAAC User Services",
     "body": "Hi\nThere is an issue with Matlab R2016b and OPeNDAP. You can find out more at https://podaac.jpl.nasa.gov/forum/viewtopic.php?f=5&t=603\nIf that is not the issue it could be that dataset is swath data. Other than lat and lon along the path of the satellite there is no dimensional information with that point as the footprint varies size with the distance the beam has to travel. Many plotting programs have issues rendering it. Since the data are not regularly spaced that also can cause issues with subsetting. You can try using PO.DAAC's web services to subset as we have all the lat and lon already index in our database. You can find recipes at https://podaac.jpl.nasa.gov/forum/viewforum.php?f=85.\nAlso note that this dataset is designed for oceans. While there are measurements over inland water special algorithms and handling need to be taken as those measurements are likely land contaminated.\nSincerely,\nJessica Hausman\nPO.DAAC Data Engineer\n---------------------------------------------------------\nJet Propulsion Laboratory / Physical Oceanography DAAC\nEmail: pod...@podaac.jpl.nasa.gov\nWebsite: http://podaac.jpl.nasa.gov\n---------------------------------------------------------\nTicket History\nNathan Potter (Client) Posted On: 11 April 2018 08:10 PM\nHi Aidin,\nCan you send me the URL of the the data collection or individual dataset that you wish to better understand?\nI’m sorry, but without it I cannot be of much assistance.\nThanks,\nNathan\n> On Apr 11, 2018, at 1:07 PM, Aidin Jabbari wrote:\n>\n> Dear Podaac Support:\n>\n> I am Aidin Jabbari, a postdoctoral scientist in University of Guelph working on the water quality in the western basin of Lake Erie. I am trying to find the water depth in that region during Aug-Sep 2017.\n>\n> Looking at the data in SARAL/AltiKa I have a hard time to understand the time, location, and water depth. Specially the units are confusing. I appreciate any advise on the data and their units to find the water depth in that period.\n>\n>\n> Sincerely,\n> Aidin Jabbari, PhD\n> Integrative Biology\n> Science Complex\n> University of Guelph\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317\nNathan Potter (Client) Posted On: 13 April 2018 01:45 PM\n> On Apr 13, 2018, at 10:08 AM, Aidin Jabbari wrote:\n>\n> Hi Nathan,\n>\n> couple of questions:\n>\n> -The longitude is east longitude relative to Greenwich meridian. So it means 82.611 W in these URLs is 360-82.54=277.46?\nThat seems correct, but honestly I’m just the software engineer. The folks at PODAAC will know how to get you in touch with someone with domain experience for that data. (I am cc’ing them)\nAlso the dataset metadata:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc.das\nContains contact information:\nString contact \"CNES av...@oceanobs.com, EUMETSAT o...@eumetsat.int, ISRO TBC\";\nI am cc’ing them also as they are probably the most likely to know the ins and outs of this dataset collection.\n>\n> -Looks like every lat corresponds to a specific lon at a specific time. The lat and lon at each URL is different from others, that I think it is because of satellite's locations. So if I am interested at a specific location, I need to download all the URLs and search the data (in my case by MATLAB) for that location? This means that location may or may not be in some URLs?\nI am afraid this is the case. Such is the travails of Level 2 data. My understanding (which is often flawed) is that the satellite circles the earth looking down and collecting what it sees. These Level 2 datasets are essentially the track of the satellite over a time period. If you work with Level 3 or Level 4 processed data then you’ll get a gridded view of the earth. These L3 & L4 collections typically have many files over time and the same coordinate index in each of the collections files is the same location. The trade off is that the spatial resolution in Level 3 and Level 4 is less that what you are currently looking at - although the vertical resolution will probably be the same, or possibly better due to averaging.\nSincerely,\nNathan\n>\n> Aidin\n> From: Aidin Jabbari\n> Sent: Friday, April 13, 2018 8:51:06 AM\n> To: Nathan Potter\n> Subject: Re: [support] Water Depth-Western Basin of Lake Erie\n>\n> I am using MATLAB 2018a.\n> From: Nathan Potter\n> Sent: Friday, April 13, 2018 8:10:19 AM\n> To: Aidin Jabbari\n> Cc: Nathan Potter; OPeNDAP User Support\n> Subject: Re: [support] Water Depth-Western Basin of Lake Erie\n>\n>\n> Hi Aidin,\n>\n> I have a mother user attempting to use Matlab to access a different dataset at the PODAAC server and they have run into some connection issues.\n>\n> Where you able to get access with Matlab or did you have issues? What version of Matlab are you using?\n>\n>\n> Thanks,\n>\n> Nathan\n>\n>\n> This is what the other user reported:\n>\n>\n> > In the event that this problem comes up again, I’ve dug a bit farther. Specifically, this URL\n> >\n> > FJ = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc’;\n> >\n> > works on my laptop running this version of Matlab:\n> >\n> >>> ver\n> > -----------------------------------------------------------------------------------------------------\n> > MATLAB Version: 9.3.0.713579 (R2017b)\n> > MATLAB License Number: 442299\n> > Operating System: Mac OS X Version: 10.13.4 Build: 17E199\n> > Java Version: Java 1.8.0_121-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n> > -----------------------------------------------------------------------------------------------------\n> > MATLAB Version 9.3 (R2017b)\n> > Image Processing Toolbox Version 10.1 (R2017b)\n> > Mapping Toolbox Version 4.5.1 (R2017b)\n> > Signal Processing Toolbox Version 7.5 (R2017b)\n> >\n> > while the same URL\n> >\n> > FJ = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc’;\n> >\n> > does not work on my desktop running this version of Matlab:\n> >\n> >>> ver\n> > ----------------------------------------------------------------------------------------------------\n> > MATLAB Version: 9.1.0.441655 (R2016b)\n> > MATLAB License Number: 225793\n> > Operating System: Mac OS X Version: 10.12.6 Build: 16G1212\n> > Java Version: Java 1.7.0_75-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n> > ----------------------------------------------------------------------------------------------------\n> > MATLAB Version 9.1 (R2016b)\n> > Simulink Version 8.8 (R2016b)\n> > Curve Fitting Toolbox Version 3.5.4 (R2016b)\n> > Database Toolbox Version 7.0 (R2016b)\n> > Fuzzy Logic Toolbox Version 2.2.24 (R2016b)\n> > Global Optimization Toolbox Version 3.4.1 (R2016b)\n> > Image Processing Toolbox Version 9.5 (R2016b)\n> > MATLAB Compiler Version 6.3 (R2016b)\n> > Mapping Toolbox Version 4.4 (R2016b)\n> > Neural Network Toolbox Version 9.1 (R2016b)\n> > Optimization Toolbox Version 7.5 (R2016b)\n> > Parallel Computing Toolbox Version 6.9 (R2016b)\n> > Partial Differential Equation Toolbox Version 2.3 (R2016b)\n> > Signal Processing Toolbox Version 7.3 (R2016b)\n> > Statistics and Machine Learning Toolbox Version 11.0 (R2016b)\n> > Symbolic Math Toolbox Version 7.1 (R2016b)\n> > Wavelet Toolbox Version 4.17 (R2016b)\n> >>>\n> >\n> > Finally, I vaguely remember something about some low level configuration issue related to Matlab so it may not be a version problem.\n> >\n>\n>\n>\n>\n>\n>\n>\n>\n> > On Apr 12, 2018, at 12:46 PM, Aidin Jabbari wrote:\n> >\n> > Hi Nathan,\n> >\n> > Here is some detail:\n> >\n> > - What are you actually trying to get? (When you say water depth, I translate that to bathymetry )\n> >\n> > I am interested in the changes in the water depth or surface level at the western basin of the Lake Erie (41.94833 N, -82.5425W) during May-Sep 2017. The water depth (the distance from the bed to the water surface) can change due to advection of water from the center basin to the western basin, so it can be time dependent.\n> >\n> >\n> > - How do you want it? (comma separated values? netcdf? tiff? something else?.)\n> >\n> > I work with MATLAB, so comma separated would be better.\n> >\n> > - What software will you use to view/explore/manipulate/plot it?\n> >\n> > MATLAB\n>\n> = = =\n> Nathan Potter ndp at opendap.org\n> OPeNDAP, Inc. +1.541.231.3317\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317\nTicket Details\nTicket ID: 82991\nDepartment: PO.DAAC\nType: Data/Science\nStatus: Waiting for User\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Aidin Jabbari",
     "body": "Dear Podaac Support:\nI am Aidin Jabbari, a postdoctoral scientist in University of Guelph working on the water quality in the western basin of Lake Erie. I am trying to find the water depth in that region during Aug-Sep 2017.\nLooking at the data in SARAL/AltiKa I have a hard time to understand the time, location, and water depth. Specially the units are confusing. I appreciate any advise on the data and their units to find the water depth in that period.\nSincerely,\nAidin Jabbari, PhD\nIntegrative Biology\nScience Complex\nUniversity of Guelph"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Hi Aidin,\nCan you send me the URL of the the data collection or individual dataset that you wish to better understand?\nI’m sorry, but without it I cannot be of much assistance.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Aidin Jabbari <ajab...@uoguelph.ca>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 11, 2018 at 5:18:56 PM PDT\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\nHere is one of the data sets I am trying to get water depth data from (close to Peele Island in the western basin of Lake Erie).\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c111/SRL_OPRSSHA_2PTS111_0832_20170822_152441_20170822_170421.EUM.nc.html\nIs ssha the water depth?\nAidin\nFrom: Nathan Potter <n...@opendap.org>\nSent: Wednesday, April 11, 2018 8:07:17 PM\nTo: Aidin Jabbari\nCc: Nathan Potter; sup...@opendap.org; pod...@podaac.jpl.nasa.gov\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 12, 2018 at 8:25:48 AM PDT\nTo: Aidin Jabbari <ajab...@uoguelph.ca>\nCc: Nathan Potter <n...@opendap.org>\nHi Aidin,\nSome specific answers:\nis the time the only variable that can be specified in this form?\nAny variable on the form may be marked for inclusion in the response by clicking the checkbox beside the variable name.\nIs ssha the water depth?\nThe variable ssha is \"sea surface height anomaly”.\nI suspect the variable named “bathymetry” would contain information about water depth.\nSome general comments:\n1) You may find this page:\nhttps://opendap.github.io/documentation/QuickStart.html\nhelpful in understanding how to use a DAP service like the one you have been referencing.\nThis section:\nhttps://opendap.github.io/documentation/QuickStart.html#_an_easy_way_using_the_browser_based_opendap_server_dataset_access_form\nexplains how to utilize the Browser based data request form.\n2) In the data request form each variable is associated with a text box that contains all of the variable's metadata. If you want to know more about a particular variable, read the stuff in there.\n3) The dataset contains a satellite track, not gridded data. The latitude and longitude arrays are not monotonic because they describe the satellite's path for the time period in the dataset.\n4) In the Global attributes there is this:\nNC_GLOBAL.contact: CNES av...@oceanobs.com, EUMETSAT o...@eumetsat.int, ISRO TBC\nWhich would probably be an excellent starting place if you want to really understand the data in the file, as I am sure one of those contacts will be able to guide you to the appropriate resource.\nLet me know if there’s more you need.\nSincerely,\nNathan\nOn Apr 12, 2018, at 7:49 AM, Aidin Jabbari <ajab...@uoguelph.ca> wrote:\nHi Nathan, is the time the only variable that can be specified in this form? In \"show help\" looks like other variables can be specified too.\nAidin\nFrom: Aidin Jabbari\nSent: Wednesday, April 11, 2018 8:18:56 PM\nTo: Nathan Potter\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nHi Nathan,\nHere is one of the data sets I am trying to get water depth data from (close to Peele Island in the western basin of Lake Erie).\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c111/SRL_OPRSSHA_2PTS111_0832_20170822_152441_20170822_170421.EUM.nc.html\nIs ssha the water depth?\nAidin\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 12, 2018 at 12:54:16 PM PDT\nTo: Aidin Jabbari <ajab...@uoguelph.ca>\nCc: Nathan Potter <n...@opendap.org>\nAidin,\nOk, so now I think you were looking at the right kind of data at PODAAC\nMatlab should be able to access these datasets directly, with no intermediate dinking.\nSection 6.1.1 of this document: https://opendap.github.io/documentation/UserGuideComprehensive.html\nShows how that might go.\nAnd Level 3 or Level 4 data might be easier to work with, but may also be too coarse in spatial resolution to help.\nBut if you can get the data into Matlab easily you should be able to filter it for data within the area of interest.\nSincerely,\nNathan\nOn Apr 12, 2018, at 12:46 PM, Aidin Jabbari <ajab...@uoguelph.ca> wrote:\nHi Nathan,\nHere is some detail:\n- What are you actually trying to get? (When you say water depth, I translate that to bathymetry )\nI am interested in the changes in the water depth or surface level at the western basin of the Lake Erie (41.94833 N, -82.5425W) during May-Sep 2017. The water depth (the distance from the bed to the water surface) can change due to advection of water from the center basin to the western basin, so it can be time dependent.\n- How do you want it? (comma separated values? netcdf? tiff? something else?.)\nI work with MATLAB, so comma separated would be better.\n- What software will you use to view/explore/manipulate/plot it?\nMATLAB\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Hi Aidin,\nI have a mother user attempting to use Matlab to access a different dataset at the PODAAC server and they have run into some connection issues.\nWhere you able to get access with Matlab or did you have issues? What version of Matlab are you using?\nThanks,\nNathan\nThis is what the other user reported:\n> In the event that this problem comes up again, I’ve dug a bit farther. Specifically, this URL\n>\n> FJ = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc’;\n>\n> works on my laptop running this version of Matlab:\n>\n>>> ver\n> -----------------------------------------------------------------------------------------------------\n> MATLAB Version: 9.3.0.713579 (R2017b)\n> MATLAB License Number: 442299\n> Operating System: Mac OS X Version: 10.13.4 Build: 17E199\n> Java Version: Java 1.8.0_121-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n> -----------------------------------------------------------------------------------------------------\n> MATLAB Version 9.3 (R2017b)\n> Image Processing Toolbox Version 10.1 (R2017b)\n> Mapping Toolbox Version 4.5.1 (R2017b)\n> Signal Processing Toolbox Version 7.5 (R2017b)\n>\n> while the same URL\n>\n> FJ = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc’;\n>\n> does not work on my desktop running this version of Matlab:\n>\n>>> ver\n> ----------------------------------------------------------------------------------------------------\n> MATLAB Version: 9.1.0.441655 (R2016b)\n> MATLAB License Number: 225793\n> Operating System: Mac OS X Version: 10.12.6 Build: 16G1212\n> Java Version: Java 1.7.0_75-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n> ----------------------------------------------------------------------------------------------------\n> MATLAB Version 9.1 (R2016b)\n> Simulink Version 8.8 (R2016b)\n> Curve Fitting Toolbox Version 3.5.4 (R2016b)\n> Database Toolbox Version 7.0 (R2016b)\n> Fuzzy Logic Toolbox Version 2.2.24 (R2016b)\n> Global Optimization Toolbox Version 3.4.1 (R2016b)\n> Image Processing Toolbox Version 9.5 (R2016b)\n> MATLAB Compiler Version 6.3 (R2016b)\n> Mapping Toolbox Version 4.4 (R2016b)\n> Neural Network Toolbox Version 9.1 (R2016b)\n> Optimization Toolbox Version 7.5 (R2016b)\n> Parallel Computing Toolbox Version 6.9 (R2016b)\n> Partial Differential Equation Toolbox Version 2.3 (R2016b)\n> Signal Processing Toolbox Version 7.3 (R2016b)\n> Statistics and Machine Learning Toolbox Version 11.0 (R2016b)\n> Symbolic Math Toolbox Version 7.1 (R2016b)\n> Wavelet Toolbox Version 4.17 (R2016b)\n>>>\n>\n> Finally, I vaguely remember something about some low level configuration issue related to Matlab so it may not be a version problem.\n- show quoted text -"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Aidin Jabbari <ajab...@uoguelph.ca>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 13, 2018 at 5:46:52 AM PDT\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\nI could open the data files in MATLAB. It is much easier! Here is a code I wrote on a sample data.\nHope it helps.\nCheers,\nAidin\nFrom: Nathan Potter <n...@opendap.org>\nSent: Friday, April 13, 2018 8:10:19 AM\nTo: Aidin Jabbari\nCc: Nathan Potter; OPeNDAP User Support\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\n- show quoted text -"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Aidin Jabbari <ajab...@uoguelph.ca>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 13, 2018 at 5:51:06 AM PDT\nTo: Nathan Potter <n...@opendap.org>\nI am using MATLAB 2018a.\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Aidin Jabbari <ajab...@uoguelph.ca>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 13, 2018 at 10:08:18 AM PDT\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\ncouple of questions:\n-The longitude is east longitude relative to Greenwich meridian. So it means 82.611 W in these URLs is 360-82.54=277.46?\n-Looks like every lat corresponds to a specific lon at a specific time. The lat and lon at each URL is different from others, that I think it is because of satellite's locations. So if I am interested at a specific location, I need to download all the URLs and search the data (in my case by MATLAB) for that location? This means that location may or may not be in some URLs?\nAidin\nFrom: Aidin Jabbari\nSent: Friday, April 13, 2018 8:51:06 AM\nTo: Nathan Potter\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Water Depth-Western Basin of Lake Erie",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nDate: April 13, 2018 at 10:43:06 AM PDT\nTo: Aidin Jabbari <ajab...@uoguelph.ca>\nCc: Nathan Potter <n...@opendap.org>, pod...@podaac.jpl.nasa.gov, av...@oceanobs.com, o...@eumetsat.int\nOn Apr 13, 2018, at 10:08 AM, Aidin Jabbari <ajab...@uoguelph.ca> wrote:\nHi Nathan,\ncouple of questions:\n-The longitude is east longitude relative to Greenwich meridian. So it means 82.611 W in these URLs is 360-82.54=277.46?\nThat seems correct, but honestly I’m just the software engineer. The folks at PODAAC will know how to get you in touch with someone with domain experience for that data. (I am cc’ing them)Also the dataset metadata:https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc.dasContains contact information: String contact \"CNES av...@oceanobs.com, EUMETSAT o...@eumetsat.int, ISRO TBC\";I am cc’ing them also as they are probably the most likely to know the ins and outs of this dataset collection.\n-Looks like every lat corresponds to a specific lon at a specific time. The lat and lon at each URL is different from others, that I think it is because of satellite's locations. So if I am interested at a specific location, I need to download all the URLs and search the data (in my case by MATLAB) for that location? This means that location may or may not be in some URLs?\nI am afraid this is the case. Such is the travails of Level 2 data. My understanding (which is often flawed) is that the satellite circles the earth looking down and collecting what it sees. These Level 2 datasets are essentially the track of the satellite over a time period. If you work with Level 3 or Level 4 processed data then you’ll get a gridded view of the earth. These L3 & L4 collections typically have many files over time and the same coordinate index in each of the collections files is the same location. The trade off is that the spatial resolution in Level 3 and Level 4 is less that what you are currently looking at - although the vertical resolution will probably be the same, or possibly better due to averaging.Sincerely,Nathan\nAidin\nFrom: Aidin Jabbari\nSent: Friday, April 13, 2018 8:51:06 AM\nTo: Nathan Potter\nSubject: Re: [support] Water Depth-Western Basin of Lake Erie\nI am using MATLAB 2018a.\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Accessing data from a Hyrax server via Matlab",
     "name": "Peter Cornillon",
     "body": "Hi,\nI’m having problem accessing data from a file, https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc.html, served via Hyrax at JPL from Matlab. I can click on some data, for example, time and it will return it to the browser with no problem when I click on the “Get ASCII” button. However, if I unclick “Time”, capture the stuff in the “Data URL:” window and then enter that in ncdisp in Matlab it fails:\nI’ve include other stuff in the screen shot if that helps.\nHowever, if I download the file — click on the file link to the far right of the line with the filename on it (20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc) on the page https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/contents.html — and then look at the file in Matlab, it works fine:\nAny ideas about what the issue might be?\nPeter\n—\nPeter Cornillon\n215 South Ferry Road Telephone: (401) 874-6283\nGraduate School of Oceanography\nUniversity of Rhode Island\nNarragansett, RI 02882 USA"
    },
    {
     "page": "Accessing data from a Hyrax server via Matlab",
     "name": "Nathan Potter",
     "body": "Hi peter,\nI tried a few of things:\n- I was able to get the DDS and the time value using curl.\ncurl https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412000000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc.dds\ncurl https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412000000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc.ascii?time\n- I tried to open the dataset with Panoply and I got an error, which interestingly I saw for the first time earlier today.\nThe error message was: ”There was an error opening the dataset: handshake alert: unrecognized_name\"\nNow with this mornings occurrence when I drop the https it works fine in Panoply.\nAnd that trick word with your URL too.\nSo you might try that in Matlab, your http URL would be:\nhttp://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412000000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc\n(gotta drop the port number too)\nAnd see if that goes. It worked for both datasets in Panoply.\n- I used the ncdump utility:\nncdump -h https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412000000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc\nAnd that worked fine using the HTTPS URL.\nSincerely,\nNathan\n> On Apr 12, 2018, at 2:20 PM, Peter Cornillon <pcorn...@me.com> wrote:\n>\n> Hi,\n>\n> I’m having problem accessing data from a file, https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc.html, served via Hyrax at JPL from Matlab. I can click on some data, for example, time and it will return it to the browser with no problem when I click on the “Get ASCII” button. However, if I unclick “Time”, capture the stuff in the “Data URL:” window and then enter that in ncdisp in Matlab it fails:\n>\n> <Screen Shot 2018-04-12 at 5.08.43 PM.png>\n>\n> I’ve include other stuff in the screen shot if that helps.\n>\n> However, if I download the file — click on the file link to the far right of the line with the filename on it (20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc) on the page https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/contents.html — and then look at the file in Matlab, it works fine:\n>\n> <Screen Shot 2018-04-12 at 5.13.36 PM.png>\n>\n> Any ideas about what the issue might be?\n>\n> Peter\n> —\n> Peter Cornillon\n> 215 South Ferry Road Telephone: (401) 874-6283\n> Graduate School of Oceanography\n> University of Rhode Island\n> Narragansett, RI 02882 USA\n>\n>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Accessing data from a Hyrax server via Matlab",
     "name": "Peter Cornillon",
     "body": "Thanks a million Nathan, the URL you suggested:\nhttp://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412000000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc\nworked.\nI have had this problem before with, as I recall, data produced by the same place but from another server so it seems to be something in the file.\nPeter\n—\nPeter Cornillon\n215 South Ferry Road Telephone: (401) 874-6283\nGraduate School of Oceanography\nUniversity of Rhode Island\nNarragansett, RI 02882 USA\n- show quoted text -"
    },
    {
     "page": "Accessing data from a Hyrax server via Matlab",
     "name": "Nathan Potter",
     "body": "Hi Peter,\nI am thinking it’s a problem with the way they have implemented/installed/configured their web service and HTTPS/SSL/TLS.\nHere’s the tip: If you use curl to try to get the DDS,\ncurl -s -i http://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412000000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc.dds\nYou won’t get a DDS, but you will get a 302 redirect from their web service. And the redirect is directly to the https URL. Why does this work in Matlab and Panoply? Beats me, but it smells like SSL/TLS trouble to me.\nI’m glad that got it sorted for you.\nCheers,\nNathan\n- show quoted text -"
    },
    {
     "page": "Accessing data from a Hyrax server via Matlab",
     "name": "Peter Cornillon",
     "body": "Hi Nathan,\nIn the event that this problem comes up again, I’ve dug a bit farther. Specifically, this URL\nFJ = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc’;\nworks on my laptop running this version of Matlab:\n>> ver\n-----------------------------------------------------------------------------------------------------\nMATLAB Version: 9.3.0.713579 (R2017b)\nMATLAB License Number: 442299\nOperating System: Mac OS X Version: 10.13.4 Build: 17E199\nJava Version: Java 1.8.0_121-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n-----------------------------------------------------------------------------------------------------\nMATLAB Version 9.3 (R2017b)\nImage Processing Toolbox Version 10.1 (R2017b)\nMapping Toolbox Version 4.5.1 (R2017b)\nSignal Processing Toolbox Version 7.5 (R2017b)\nwhile the same URL\nFJ = 'https://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L2P/VIIRS_NPP/OSPO/v2.41/2018/102/20180412010000-OSPO-L2P_GHRSST-SSTsubskin-VIIRS_NPP-ACSPO_V2.41-v02.0-fv01.0.nc’;\ndoes not work on my desktop running this version of Matlab:\n>> ver\n----------------------------------------------------------------------------------------------------\nMATLAB Version: 9.1.0.441655 (R2016b)\nMATLAB License Number: 225793\nOperating System: Mac OS X Version: 10.12.6 Build: 16G1212\nJava Version: Java 1.7.0_75-b13 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n----------------------------------------------------------------------------------------------------\nMATLAB Version 9.1 (R2016b)\nSimulink Version 8.8 (R2016b)\nCurve Fitting Toolbox Version 3.5.4 (R2016b)\nDatabase Toolbox Version 7.0 (R2016b)\nFuzzy Logic Toolbox Version 2.2.24 (R2016b)\nGlobal Optimization Toolbox Version 3.4.1 (R2016b)\nImage Processing Toolbox Version 9.5 (R2016b)\nMATLAB Compiler Version 6.3 (R2016b)\nMapping Toolbox Version 4.4 (R2016b)\nNeural Network Toolbox Version 9.1 (R2016b)\nOptimization Toolbox Version 7.5 (R2016b)\nParallel Computing Toolbox Version 6.9 (R2016b)\nPartial Differential Equation Toolbox Version 2.3 (R2016b)\nSignal Processing Toolbox Version 7.3 (R2016b)\nStatistics and Machine Learning Toolbox Version 11.0 (R2016b)\nSymbolic Math Toolbox Version 7.1 (R2016b)\nWavelet Toolbox Version 4.17 (R2016b)\n>>\nFinally, I vaguely remember something about some low level configuration issue related to Matlab so it may not be a version problem.\nPeter\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Why no love?",
     "name": "Nathan Potter",
     "body": "I got the error (below) when trying to access a Level 2 data set at PODAAC.\nWith the exception of \"time\" all of the variables in the dataset are Grids with a single Map of \"time”.\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc.dds\nCould this arragement may be the source of the issue?\nOtherwise I am concerned that the error message \"handshake alert: unrecognized_name\" may point to some deeper issue with protocol compatibility that we may need to examine.\nThanks,\nNathan\nThere was an error loading the data:\nError creating data source:file.grid with: https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nGrid data source failed making data set: https://podaac-opendap.jpl.nasa.gov/opendap/allData/saral/preview/L2/XOGDR-SSHA/c112/SRL_OPRSSHA_2PTS112_0002_20170828_153728_20170828_171708.EUM.nc\nhandshake alert: unrecognized_name\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Pass user name and password to OPENDAP (in R?)",
     "name": "Alexey Shiklomanov",
     "body": "Hi!\nI am trying to download data via OPENDAP, for instance from the NASA GES DISC. However, these data are password-protected. I can get to them via interactive login in the web browser, but I was wondering how I could go about doing this via command line (preferably, within R -- thus far, I've been trying to access with `ncdf4::nc_open(url)`).\nFor other OPENDAP clients, I've been able to craft URLs like: http://USERNAME:PASSWORD@website.edu/data/etc.... However, this doesn't seem to work for the NASA GES. Are there other approaches to command line HTTP(S?) authentication?\nThanks in advance!\nAlexey\n--\nAlexey Shiklomanov\nDepartment of Earth & Environment\nBoston University"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Fwd: DAP2Exception questions",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: DAP2Exception questions\nDate: April 11, 2018 at 10:59:51 AM PDT\nTo: Ann Smittu Joseph <ann_j...@cipm.info>\nCc: Nathan Potter <n...@opendap.org>, support-n...@unidata.ucar.edu, Unidata THREDDS Support <support...@unidata.ucar.edu>\nHi Ann,\nRegarding your questions:\n1a) The inconsistency that must be something in how the targeted server (an instance of the THREDDS Data Server) adds HTTP headers to its DAP responses or the way that ucar.nc2.dataset.NetcdfDataset.openFile() transmits/receives/interprets relevant HTTP header fields.\n1b) The error means that client library code, ucar.nc2.dataset.NetcdfDataset is expecting to find DAP protocol specific HTTP headers, either “XDAP\" or \"XDODS-Server\", in the DAP responses returned by the TDS located at http://convection.meas.ncsu.edu:8080/thredds/\nWhen I try getting the DDS and DAS responses for the dataset you provided I get these HTTP headers:\n[-bash: ] curl -i http://convection.meas.ncsu.edu:8080/thredds/dodsC/prism/daily/combo/2017/PRISM_combo_20170629.nc.das\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nLast-Modified: Sun, 02 Jul 2017 15:13:04 GMT\nXDODS-Server: opendap/3.7\nContent-Description: dods-das\nContent-Type: text/plain\nTransfer-Encoding: chunked\nDate: Wed, 11 Apr 2018 17:49:25 GMT\n[-bash: ] curl -i http://convection.meas.ncsu.edu:8080/thredds/dodsC/prism/daily/combo/2017/PRISM_combo_20170629.nc.dds\nHTTP/1.1 200 OK\nServer: Apache-Coyote/1.1\nLast-Modified: Sun, 02 Jul 2017 15:13:04 GMT\nXDODS-Server: opendap/3.7\nContent-Description: dods-dds\nContent-Type: text/plain\nTransfer-Encoding: chunked\nDate: Wed, 11 Apr 2018 17:50:54 GMT\n2) It seems like your code should work, as DAP2Exception is a child of java.lang.Exception and so should be caught by try{}catch{Exception e} there are, of course, things like null pointer exceptions that cannot be caught this way.\nI am cc’ing the folks at UNIDATA that author and support both the NetCDF-Java library and the THREDDS Data Server with the hope that they can help you find the issue.\nSincerely,\nNathan\nOn Apr 11, 2018, at 9:08 AM, Ann Smittu Joseph <ann_j...@cipm.info> wrote:\nHi,\nI work at NCSU and I am developing a pest forecast model using PRISM data and the biological data of pest Bactrocera invadens (Diptera: Tephritidae). My code is written in Java and I am accessing the daily PRISM data from http://convection.meas.ncsu.edu:8080/thredds/catalog/prism/daily/combo/catalog.html\nI am encountering multiple issues and it would be great if you could provide some insight or at least direct me in the right direction.\n1. When I try to access file http://convection.meas.ncsu.edu:8080/thredds/dodsC/prism/daily/combo/2017/PRISM_combo_20170629.nc using the command NetcdfFile nc = ucar.nc2.dataset.NetcdfDataset.openFile(path, cancel); I get the below exception.\na. This exception is inconsistent. Sometimes I do not get any exception and the same code runs without errors. Could you help me understand why the inconsistency?\nb. Also, could you explain to me the meaning of this error. Am I accessing the file incorrectly?\n2. I am unable to catch the DAP2Exception. I have placed the above command in a try{}catch{Exception e} but the error is being thrown as below and not handled in my catch{}. What am I doing wrong?\nERROR\nopendap.dap.DAP2Exception: Not a valid OPeNDAP server - Missing MIME Header fields! Either \"XDAP\" or \"XDODS-Server.\" must be present.\nat opendap.dap.ServerVersion.<init>(ServerVersion.java:150)\nat opendap.dap.DConnect2.openConnection(DConnect2.java:313)\nat opendap.dap.DConnect2.getDAS(DConnect2.java:530)\nat ucar.nc2.dods.DODSNetcdfFile.<init>(DODSNetcdfFile.java:251)\nat sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\nat java.lang.reflect.Constructor.newInstance(Unknown Source)\nat ucar.nc2.dataset.NetcdfDataset.openDodsByReflection(NetcdfDataset.java:1078)\nat ucar.nc2.dataset.NetcdfDataset.acquireDODS(NetcdfDataset.java:1028)\nat ucar.nc2.dataset.NetcdfDataset.openOrAcquireFile(NetcdfDataset.java:718)\nat ucar.nc2.dataset.NetcdfDataset.openFile(NetcdfDataset.java:572)\nat cipm.phenology.PrismTDS.readwrite(PrismTDS.java:166)\nat cipm.phenology.PhenologyModel.processrequest(PhenologyModel.java:279)\nat web.cipm.DegreeUserServlet.doPost(DegreeUserServlet.java:262)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:650)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:731)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218)\nat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)\nat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505)\nat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)\nat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\nat org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:958)\nat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)\nat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:452)\nat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1087)\nat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)\nat org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\nat java.lang.Thread.run(Unknown Source)\nopendap.dap.DAP2Exception: Not a valid OPeNDAP server - Missing MIME Header fields! Either \"XDAP\" or \"XDODS-Server.\" must be present.\nat opendap.dap.ServerVersion.<init>(ServerVersion.java:150)\nat opendap.dap.DConnect2.openConnection(DConnect2.java:313)\nat opendap.dap.DConnect2.getData(DConnect2.java:876)\nat opendap.dap.DConnect2.getData(DConnect2.java:1176)\nat ucar.nc2.dods.DODSNetcdfFile.readDataDDSfromServer(DODSNetcdfFile.java:1516)\nat ucar.nc2.dods.DODSNetcdfFile.readArrays(DODSNetcdfFile.java:1573)\nat ucar.nc2.dods.DODSNetcdfFile.<init>(DODSNetcdfFile.java:370)\nat sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\nat java.lang.reflect.Constructor.newInstance(Unknown Source)\nat ucar.nc2.dataset.NetcdfDataset.openDodsByReflection(NetcdfDataset.java:1078)\nat ucar.nc2.dataset.NetcdfDataset.acquireDODS(NetcdfDataset.java:1028)\nat ucar.nc2.dataset.NetcdfDataset.openOrAcquireFile(NetcdfDataset.java:718)\nat ucar.nc2.dataset.NetcdfDataset.openFile(NetcdfDataset.java:572)\nat cipm.phenology.PrismTDS.readwrite(PrismTDS.java:166)\nat cipm.phenology.PhenologyModel.processrequest(PhenologyModel.java:279)\nat web.cipm.DegreeUserServlet.doPost(DegreeUserServlet.java:262)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:650)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:731)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218)\nat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)\nat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505)\nat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)\nat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\nat org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:958)\nat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)\nat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:452)\nat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1087)\nat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)\nat org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\nat java.lang.Thread.run(Unknown Source)\nApr 11, 2018 11:43:43 AM ucar.nc2.dods.DODSNetcdfFile readArrays\nSEVERE: ERROR readDataDDSfromServer on ?t,latitude,longitude\nopendap.dap.DAP2Exception: opendap.dap.DAP2Exception: Not a valid OPeNDAP server - Missing MIME Header fields! Either \"XDAP\" or \"XDODS-Server.\" must be present.\nat opendap.dap.DConnect2.openConnection(DConnect2.java:334)\nat opendap.dap.DConnect2.getData(DConnect2.java:876)\nat opendap.dap.DConnect2.getData(DConnect2.java:1176)\nat ucar.nc2.dods.DODSNetcdfFile.readDataDDSfromServer(DODSNetcdfFile.java:1516)\nat ucar.nc2.dods.DODSNetcdfFile.readArrays(DODSNetcdfFile.java:1573)\nat ucar.nc2.dods.DODSNetcdfFile.<init>(DODSNetcdfFile.java:370)\nat sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\nat java.lang.reflect.Constructor.newInstance(Unknown Source)\nat ucar.nc2.dataset.NetcdfDataset.openDodsByReflection(NetcdfDataset.java:1078)\nat ucar.nc2.dataset.NetcdfDataset.acquireDODS(NetcdfDataset.java:1028)\nat ucar.nc2.dataset.NetcdfDataset.openOrAcquireFile(NetcdfDataset.java:718)\nat ucar.nc2.dataset.NetcdfDataset.openFile(NetcdfDataset.java:572)\nat cipm.phenology.PrismTDS.readwrite(PrismTDS.java:166)\nat cipm.phenology.PhenologyModel.processrequest(PhenologyModel.java:279)\nat web.cipm.DegreeUserServlet.doPost(DegreeUserServlet.java:262)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:650)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:731)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218)\nat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)\nat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505)\nat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)\nat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\nat org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:958)\nat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)\nat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:452)\nat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1087)\nat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)\nat org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\nat java.lang.Thread.run(Unknown Source)\nCaused by: opendap.dap.DAP2Exception: Not a valid OPeNDAP server - Missing MIME Header fields! Either \"XDAP\" or \"XDODS-Server.\" must be present.\nat opendap.dap.ServerVersion.<init>(ServerVersion.java:150)\nat opendap.dap.DConnect2.openConnection(DConnect2.java:313)\n... 37 more\nApr 11, 2018 11:43:43 AM ucar.nc2.dataset.NetcdfDataset openDodsByReflection\nSEVERE: Error openDodsByReflection:\njava.io.IOException: opendap.dap.DAP2Exception: Not a valid OPeNDAP server - Missing MIME Header fields! Either \"XDAP\" or \"XDODS-Server.\" must be present.\nat ucar.nc2.dods.DODSNetcdfFile.readArrays(DODSNetcdfFile.java:1578)\nat ucar.nc2.dods.DODSNetcdfFile.<init>(DODSNetcdfFile.java:370)\nat sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\nat java.lang.reflect.Constructor.newInstance(Unknown Source)\nat ucar.nc2.dataset.NetcdfDataset.openDodsByReflection(NetcdfDataset.java:1078)\nat ucar.nc2.dataset.NetcdfDataset.acquireDODS(NetcdfDataset.java:1028)\nat ucar.nc2.dataset.NetcdfDataset.openOrAcquireFile(NetcdfDataset.java:718)\nat ucar.nc2.dataset.NetcdfDataset.openFile(NetcdfDataset.java:572)\nat cipm.phenology.PrismTDS.readwrite(PrismTDS.java:166)\nat cipm.phenology.PhenologyModel.processrequest(PhenologyModel.java:279)\nat web.cipm.DegreeUserServlet.doPost(DegreeUserServlet.java:262)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:650)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:731)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\nat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)\nat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\nat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218)\nat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)\nat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505)\nat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169)\nat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\nat org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:958)\nat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)\nat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:452)\nat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1087)\nat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637)\nat org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\nat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\nat java.lang.Thread.run(Unknown Source)\nThank you for your time,\nAnn Joseph\nBusiness & Technology Application Analyst,\nNSF Center for Integrated Pest Management\n1730 Varsity Drive, STE 110\nNCSU Centennial Campus\nRaleigh NC 27696\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Fwd: DAP2Exception questions",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Ann Smittu Joseph <ann_j...@cipm.info>\nSubject: RE: DAP2Exception questions\nDate: April 11, 2018 at 11:16:14 AM PDT\nTo: Nathan Potter <n...@opendap.org>\nThank you for your quick reply and explanation. I do have some related questions but I will wait to see if UNIDATA provides some more input.\n-Ann\n-----Original Message-----\nFrom: Nathan Potter [mailt...@opendap.org]\nSent: Wednesday, April 11, 2018 2:00 PM\nTo: Ann Smittu Joseph <ann_j...@cipm.info>\n- show quoted text -\n- show quoted text -"
    }
   ]
  },
  {
  },
  {
   "name": [
    {
     "page": "Downloads trough Matlab-certificate troubles",
     "name": "Virginie Destuynder",
     "body": "Hello,\nI'm trying to download netcdf files from Princeton opendap server trough matlab, but I have trouble with certificates, for example:\n>> A=websave('nc2015.nc','https://data.princetonclimate.com:443/opendap/MSWEP_V2.1/tiles_3hourly_010deg/155/2015.nc?precipitation[0:1:2919][0:1:99][0:1:99],time[0:1:2919],lon[0:1:99],lat[0:1:99].nc','Username','Destuynder','Password','ftJAfCUgNkyCpc');\nError using websave (line 97)\nCould not establish a secure connection to \"data.princetonclimate.com\". The reason is\n\"error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed\". Check your\ncertificate file (C:\\Program Files\\MATLAB\\R2017a\\sys\\certificates\\ca\\rootcerts.pem) for\nexpired, missing or invalid certificates.\nIs there a way to update the certificate file in order to get access to the file from matlab?\nThank you very much,\nVirginie Destuynder"
    },
    {
     "page": "Downloads trough Matlab-certificate troubles",
     "name": "jgallagher",
     "body": "- show quoted text -\nBased on what I see, this should work. Let me check on this.\nJames\nVirginie Destuynder\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Downloads trough Matlab-certificate troubles",
     "name": "jgallagher",
     "body": "- show quoted text -\nI have not been able to find out anything about this. I’d try contacting princetonclimate.com and see if they can help you.\nJames\nJames\nVirginie Destuynder\n--\nJames Gallagher\njgall...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "jgallagher",
     "body": "On Feb 14, 2018, at 07:35, Aafaque (CTR), Aafaque <aafaque.a...@usgs.gov> wrote:\nHi James,\nThanks for comments. Attached are ncml files / test file I am using for ncml handler test running through \"besstandalone with ncml debugging on\" (besstandalone -d \"cerr,ncml\" -c bes.conf -i test_agg_tif.bescmd) (I have placed the files in directories as present in ncml handler)\nYou can download the \"tif\" files used by ncml handler from following location (https://opendap.atlassian.net/secure/attachment/13310/Geotiff_Rename_Aggregation.tar.gz)\nBrief update: While this is not a fix, here’s the error I see on OSX. I will look into this tomorrow.\nedamame:test jimg$ besstandalone -d \"cerr,ncml\" -c ../tests/bes.conf -i test_agg_tif2.bescmd\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Initializing NCML Module ncml\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Done Initializing NCML Module ncml\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Created NCMLParser.\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Beginning NcML parse of file=/Users/jimg/src/opendap/hyrax_git/bes/modules/ncml_module/data/ncml/gdal/h02v10_4Files.ncml\n…\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Success, new variable name is open at this scope.\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Renaming variable band_1 to SRB2\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] static void agg_util::AggregationUtil::transferArrayConstraints(libdap::Array *, const libdap::Array &, bool, bool, bool, const std::string &): GAggregationUtil::transferArrayConstraints: Expected the dimensions to have the same name but they did not.\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] NCMLModule InternalError: [static void agg_util::AggregationUtil::transferArrayConstraints(libdap::Array *, const libdap::Array &, bool, bool, bool, const std::string &)]: ASSERTION FAILED: condition=( fromArrIt->name == toArrIt->name ) GAggregationUtil::transferArrayConstraints: Expected the dimensions to have the same name but they did not.\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Caught BESError&, deferring...\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n<response reqID=\"some_unique_value\" xmlns=\"http://xml.opendap.org/ns/bes/1.0#\">\n<getDODS>\n<BESError>\n<Type>1</Type>\n<Message>NCMLModule InternalError: [static void agg_util::AggregationUtil::transferArrayConstraints(libdap::Array *, const libdap::Array &amp;, bool, bool, bool, const std::string &amp;)]: ASSERTION FAILED: condition=( fromArrIt-&gt;name == toArrIt-&gt;name ) GAggregationUtil::transferArrayConstraints: Expected the dimensions to have the same name but they did not.</Message>\n<Administrator>admin.email.address@your.domain.name</Administrator>\n<Location>\n<File>AggregationUtil.cc</File>\n<Line>776</Line>\n</Location>\n</BESError>\n</getDODS>\n</response>\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Cleaning NCML module ncml\n[MST Wed Feb 14 17:16:41 2018 id: 29424][ncml] Done Cleaning NCML module ncml\nedamame:test jimg$\nMore hopefully soon…\nJames\nAafaque\nSoftware Engineer\nInnovate!\nContractor to the USGS Earth Resources Observation and Science (EROS) Center\n47914 252nd Street\nSioux Falls, SD 57198\n(Phone) 605-594-2735\naafaque.a...@usgs.gov\nOn Tue, Feb 13, 2018 at 4:38 PM, James Gallagher <jgall...@opendap.org> wrote:\nOn Feb 13, 2018, at 08:06, Aafaque (CTR), Aafaque <aafaque.a...@usgs.gov> wrote:\nHi James,\nI was wondering if you have had some time to consider my comments and questions below. Let me know if I need to clarify anything.\nI’ll look at this week. Comments below…\nAafaque\nSoftware Engineer\nInnovate!\nContractor to the USGS Earth Resources Observation and Science (EROS) Center\n47914 252nd Street\nSioux Falls, SD 57198\n(Phone) 605-594-2735\naafaque.a...@usgs.gov\nOn Thu, Jan 25, 2018 at 4:26 PM, Aafaque (CTR), Aafaque <aafaque.a...@usgs.gov> wrote:\nHi James,\nI did some debugging using the steps mentioned by Slav, and also tried different ncml aggregation ways (JoinNew with rename of Variables) and found the place it actually breaks the code.\nSeems like it cause at processAggVarJoinExistingForArray function inside AggregationElement.cc. It give \"Segmentation fault (core dumped)\" (memory access issue) .\nThat issue is caused by dds.add_var function which is a wrapper class for the original libdap:Array (libdap:DDS), I found out your comment that (OPTIMIZE change to add_var_nocopy when it exists instead). I think LibDAP4 already has add_var_nocopy exist and I tried to change the add_var to nocopy in ncml module. see the following commit (https://github.com/aafaque33/bes/commit/133b5074755c0d6feea42dd126ef39734271e7bd)\nBut seems like it even didn't changed any behavior, So Question is: is that nocopy function in libdap in working condition or have some issues still?\nNo, that should be fine to use.\nAlso how can I catch the errors from libdap lib as seems like that Segmentation fault error is of not much help.\nYou cannot catch a segmentation fault. However, if you run the code with valgrind or address sanitizer, that will pinpoint the problem.\nThe dds.add_var() works fine for aggregation of up-to 3 files. So actual issue seems like in libdap module not ncml module.\nPlease let me know If you think of some other issue that may be causing this aggregation problem.\nCan you send me files to reproduce this problem - my apologies if you already have.\nJames\nRegards\nAafaque\nSoftware Engineer\nInnovate!\nContractor to the USGS Earth Resources Observation and Science (EROS) Center\n47914 252nd Street\nSioux Falls, SD 57198\n(Phone) 605-594-2735\naafaque.a...@usgs.gov\nOn Fri, Jan 19, 2018 at 11:25 AM, James Gallagher <jgall...@opendap.org> wrote:\nThanks!\nKeep me in the loop and don’t be shy with questions if you are blocked and think I can help.\nJames\nOn Jan 19, 2018, at 07:45, Aafaque (CTR), Aafaque <aafaque.a...@usgs.gov> wrote:\nThanks James and Slav, for the detailed reply.\nI will look through the steps in file and issue mentioned and get back to you if I have any questions.\nAafaque\nSoftware Engineer\nInnovate!\nContractor to the USGS Earth Resources Observation and Science (EROS) Center\n47914 252nd Street\nSioux Falls, SD 57198\n(Phone) 605-594-2735\naafaque.a...@usgs.gov\nOn Fri, Jan 19, 2018 at 8:39 AM, Slav Korolev <Vyacheslav...@raytheon.com> wrote:\nHi Aafaque,\nMy name is Slav and I work with James.\nHere is attached text file with steps you should do to reproduce an error when variable is changed for aggregated tifs.\nThank you.\nSlav Korolev.\nwork: 301-851-8339\ncell: 240-481-4003\nVyacheslav...@raytheon.com\nSlav.K...@nasa.gov\nFrom: James Gallagher <jgall...@opendap.org>\nSent: Thursday, January 18, 2018 5:53 PM\nTo: Aafaque (CTR), Aafaque; Slav Korolev\nCc: James Gallagher; Hendrix (CTR), Cody; Friesz (CTR), Aaron; Werpy Jason; Bradley Schroeder (CTR)\nSubject: [External] Re: GDAL Handler ( latest build and source code )\nOn Jan 16, 2018, at 09:45, Aafaque (CTR), Aafaque <aafaque.a...@usgs.gov> wrote:\nHi James,\nCurrently I am working on a fix for GDAL Handler (GeoTIFF multiple aggregation issue), but I am not sure which one is the Updated GDAL Handler on OPeNDAP github.\nThere is a standalone GDAL Handler repository https://github.com/OPENDAP/gdal_handler.git which seems like not the updated one to me.\nHi,\nFirst off, I’m very grateful for your help! Now, onto your questions...\nRight. Some time ago I rolled all the BES-dependent handler git repos (which were loaded into the BES code as git submodules) into the bes. Those old repos are old and I should remove them. Sorry for the confusion.\nSo my question is, is the updated changes for GDAL Handler currently is inside the Sub module inside bes repository https://github.com/OPENDAP/bes?\nYes!\nIf yes, is there a way to just build gdal handler not the whole BES module after I do some changes inside GDAL Handler? (Note: I have forked the repository and working on it not actual OPeNDAP repository).\nYes. you should be able to build just the gdal handler by running make in the modules/gdal_handler directory. The Makefiles support that - it’s what I do and I routinely work from the command line and emacs (and also Eclipse, which supports make well).\nI just don't want to build the whole bes module every time when I do some little changes in GDAL Handler only as it takes a lot of time to build bes module.\nRight, and you definitely don’t have to. In fact, not only are all of the handlers standalone modules that load at runtime, but the BES is broken into shared libs.\nAlso I have find the similar issue here https://opendap.atlassian.net/browse/HYRAX-362, which seems like you were working back in 2017, as I think I found out the pull request related to this issue and tried the code (by rebuilding the bes) but seems like it didn't fixed issue for me. So if you can give any input related to fix you tried it would be great.\nI checked and added the following comment to the ticker (which I should have done before): The issue noted above - that the gdal handler kept the _GDAL Handle_ open so it could be used by subsequent calls was fixed by this ticket.\nThat fixed has been merged back into the master branch.\nThe remaining issue with GeoTiFF aggregation seems to be that renaming variables does not work. I’m including Slav Korolev on this reply since he has been looking into this issue as well.\nSlav: Can you send Aafaque information on the error?\nThanks,\nJames\nPS. If you have more questions, please ask.\nThanks,\nAafaque\nSoftware Engineer\nInnovate!\nContractor to the USGS Earth Resources Observation and Science (EROS) Center\n47914 252nd Street\nSioux Falls, SD 57198\n(Phone) 605-594-2735\naafaque.a...@usgs.gov\n--\nJames Gallagher\njgall...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org\n<ncm_files.tar.gz>\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "jgallagher",
     "body": "Aafaque,\nMy apologies for the top post, but the thread is getting quite long.\nHere’s what I found, the issue does not appear to be a problem of aggregating four or more files - I can get the error below to trigger with just one file. The problem is the renaming of the band_1 variable after it has been built up during an aggregation. (I’m very glad you sent all the various tiff and ncml files - I’d have never figured this out w/o them.)\nHere’s what _will_ work: If you use NCML to rename the Grid variable ‘band_1’ to ‘SRB’ for each file you intend to aggregate, you can then aggregate on the Grid ‘SRB.’ So, given:\nedamame:tests jimg$ ls ../data/gdal/\nLC08_CU_002010_20130416_20170727_C01_V01_SRB2.ncml\nLC08_CU_002010_20130416_20170727_C01_V01_SRB2.tif\nLC08_CU_002010_20130418_20170727_C01_V01_SRB2.ncml\nLC08_CU_002010_20130418_20170727_C01_V01_SRB2.tif\nLC08_CU_002010_20130425_20170727_C01_V01_SRB2.ncml\nLC08_CU_002010_20130425_20170727_C01_V01_SRB2.tif\nLC08_CU_002010_20130504_20170727_C01_V01_SRB2.ncml\nLC08_CU_002010_20130504_20170727_C01_V01_SRB2.tif\nWhere the .ncml files rename ‘band_1’ to ‘SRB2’ in each of the matching .tif files, the following NCML will work:\n<?xml version='1.0' encoding='utf-8'?>\n<netcdf title=\"Time based join for california_LC08_SRB2\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<aggregation dimName=\"time\" type=\"joinNew\">\n<variableAgg name=\"SRB2\" />\n<netcdf coordValue=\"4854\" location=\"/data/gdal/LC08_CU_002010_20130416_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4856\" location=\"/data/gdal/LC08_CU_002010_20130418_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4863\" location=\"/data/gdal/LC08_CU_002010_20130425_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4872\" location=\"/data/gdal/LC08_CU_002010_20130504_20170727_C01_V01_SRB2.ncml\" />\n</aggregation>\n<variable name=\"time\" type=\"double\">\n<attribute name=\"units\" type=\"string\">days since 2000-01-01 00:00</attribute>\n</variable>\n</netcdf>\nAnd the resulting 3 dimensional Grid will be named SRB2.\nAnd, if you can live with the Grid being named ‘band_1,’ you can aggregate the .tif files like this:\n<?xml version='1.0' encoding='UTF-8'?>\n<netcdf title=\"Time based join for california_LC08_SRB2\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<aggregation dimName=\"time\" type=\"joinNew\">\n<variableAgg name=\"band_1\" />\n<netcdf coordValue=\"4854\" location=\"/data/gdal/LC08_CU_002010_20130416_20170727_C01_V01_SRB2.tif\" />\n<netcdf coordValue=\"4856\" location=\"/data/gdal/LC08_CU_002010_20130418_20170727_C01_V01_SRB2.tif\" />\n<netcdf coordValue=\"4863\" location=\"/data/gdal/LC08_CU_002010_20130425_20170727_C01_V01_SRB2.tif\" />\n<netcdf coordValue=\"4872\" location=\"/data/gdal/LC08_CU_002010_20130504_20170727_C01_V01_SRB2.tif\" />\n</aggregation>\n<variable name=\"time\" type=\"double\">\n<attribute name=\"units\" type=\"string\">days since 2000-01-01 00:00</attribute>\n</variable>\n<!-- variable name=\"SRB2\" orgName=\"band_1\" / -->\n</netcdf>\n(note that I commented out the 'variable name=\"SRB2\" orgName=“band_1”’ part).\nBut, if you add <variable name=\"SRB2\" orgName=“band_1\"/> back in, the aggregation will fail. It’s a bug, at least I think it is a bug, but I cannot promise a fix soon. I tried, as a work around, using NCML to change the name of the resulting 3d Grid, like this:\n<?xml version='1.0' encoding='UTF-8'?>\n<netcdf location=\"/data/ncml/gdal/h02v10_tiff_files.ncml\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<variable name=\"SRB2\" orgName=\"band_1\" />\n</netcdf>\nBut that does not work, again, that maybe a bug.\nSo for now: You can aggregate as many TIFF files as you want, but if you want to rename the variable that results from the aggregation, you must do so by renaming all of its components first, then combine those together.\nI’m sorry this took so long to sort out and that the outcome is less than satisfactory. What you’re trying seems logically correct, but fails because the NCML implementation is applying the variable rename operation (<variable name=\"SRB2\" orgName=\"band_1\" />) to the DDS of the elements of the aggregation, and not the DDS that results from the previous operations (i.e., the DDS of the aggregation).\nHere’s the actual error message from the ASSERT in the code:\n[MST Thu Feb 15 10:56:17 2018 id: 34209][ncml] NCMLModule InternalError: [static void agg_util::AggregationUtil::transferArrayConstraints(libdap::Array *, const libdap::Array &, bool, bool, bool, const std::string &)]: ASSERTION FAILED: condition=( fromArrIt->name == toArrIt->name ) GAggregationUtil::transferArrayConstraints: Expected the dimensions to have the same name but they did not: 'easting', 'northing’.\nThis is not in the code you’re running - that might be an issue as well - because assertions are compiled out for production releases. I’ll change that so we trap these conditions and produce a more informative error message.\nThanks,\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "Schroeder (CTR), Bradley",
     "body": "Hi James,\nI may be missing something in what you're saying. But I believe what you are suggesting is the exact thing that we tried originally and get an error as soon as we try to aggregate more than 3 .ncml files. We created one .ncml file for each .tif file. The .ncml file just renamed band_1 to SRB2. We can aggregate 3 of those together, but as soon as we add in a 4th file, it fails. I might be missing something, so I'll paste in here the contents of our time aggregation .ncml file, and the contents for the first referenced rename .ncml file.\n<?xml version='1.0' encoding='UTF-8'?>\n<netcdf title=\"Time based join for california_LC08_SRB2\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<aggregation dimName=\"time\" type=\"joinNew\">\n<variableAgg name=\"SRB2\" />\n<netcdf coordValue=\"4854\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130416_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4856\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130418_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4863\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130425_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4872\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130504_20170727_C01_V01_SRB2.ncml\" />\n</aggregation>\n<variable name=\"time\" type=\"double\">\n<attribute name=\"units\" type=\"string\">days since 2000-01-01 00:00</attribute>\n</variable>\n</netcdf>\nHere is the contents of /LC08_CU_002010_20130416_20170727_C01_V01_SRB2.ncml\n<?xml version='1.0' encoding='UTF-8'?>\n<netcdf location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130416_20170727_C01_V01_SRB2.tif\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<variable name=\"SRB2\" orgName=\"band_1\" />\n</netcdf>\nIs that what you are suggesting? Because this does not work for us. When we try to retrieve data through the OPeNDAP UI it returns a 200 code, but doesn't return any data. As soon as I comment out any one of the 4 .ncml files (i.e. LC08_CU_002010_20130418_20170727_C01_V01_SRB2.ncml ) from the time aggregation .ncml it works as expected and returns data.\nYour second suggestion of leaving it named band_1 does work as far as aggregation, we are using that as a work around right now. But that is preventing us from our ultimate goal. This product has 11 bands all in separate files and all named band_1 in the GeoTIFF. Our ultimate goal is to rename each band_1 to a meaningful name like SRB1, and then do a union on all of those variables and display the product as 1 product instead of 11 products. Here is an example of a union ncml we are trying. This is from a different test, so the filenames and directories don't match the previous examples.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<netcdf xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<aggregation type=\"union\">\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB1/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB2/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB3/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB4/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB5/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB6/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB7/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_LINEAGEQA/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_RADSATQA/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_PIXELQA/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRAEROSOLQA/h005v002.ncml\"/>\n</aggregation>\n<attribute name=\"title\" type=\"string\" value=\"Simple union aggregation test\"/>\n</netcdf>\nBrad"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "jgallagher",
     "body": "On Feb 15, 2018, at 17:24, Schroeder (CTR), Bradley <bradley.sc...@usgs.gov> wrote:\nHi James,\nI may be missing something in what you're saying. But I believe what you are suggesting is the exact thing that we tried originally and get an error as soon as we try to aggregate more than 3 .ncml files. We created one .ncml file for each .tif file. The .ncml file just renamed band_1 to SRB2. We can aggregate 3 of those together, but as soon as we add in a 4th file, it fails. I might be missing something, so I'll paste in here the contents of our time aggregation .ncml file, and the contents for the first referenced rename .ncml file.\n<?xml version='1.0' encoding='UTF-8'?>\n<netcdf title=\"Time based join for california_LC08_SRB2\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<aggregation dimName=\"time\" type=\"joinNew\">\n<variableAgg name=\"SRB2\" />\n<netcdf coordValue=\"4854\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130416_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4856\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130418_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4863\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130425_20170727_C01_V01_SRB2.ncml\" />\n<netcdf coordValue=\"4872\" location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130504_20170727_C01_V01_SRB2.ncml\" />\n</aggregation>\n<variable name=\"time\" type=\"double\">\n<attribute name=\"units\" type=\"string\">days since 2000-01-01 00:00</attribute>\n</variable>\n</netcdf>\nHere is the contents of /LC08_CU_002010_20130416_20170727_C01_V01_SRB2.ncml\n<?xml version='1.0' encoding='UTF-8'?>\n<netcdf location=\"LANDSAT_ARD/california/h02v10/LC08_CU_002010_20130416_20170727_C01_V01_SRB2.tif\" xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<variable name=\"SRB2\" orgName=\"band_1\" />\n</netcdf>\nIs that what you are suggesting? Because this does not work for us. When we try to retrieve data through the OPeNDAP UI it returns a 200 code, but doesn't return any data. As soon as I comment out any one of the 4 .ncml files (i.e. LC08_CU_002010_20130418_20170727_C01_V01_SRB2.ncml ) from the time aggregation .ncml it works as expected and returns data.\nYes. Hmmm. It worked for me, although I have to admit, I was running on OSX and not Linux…\nYour second suggestion of leaving it named band_1 does work as far as aggregation, we are using that as a work around right now. But that is preventing us from our ultimate goal. This product has 11 bands all in separate files and all named band_1 in the GeoTIFF. Our ultimate goal is to rename each band_1 to a meaningful name like SRB1, and then do a union on all of those variables and display the product as 1 product instead of 11 products. Here is an example of a union ncml we are trying. This is from a different test, so the filenames and directories don't match the previous examples.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<netcdf xmlns=\"http://www.unidata.ucar.edu/namespaces/netcdf/ncml-2.2\">\n<aggregation type=\"union\">\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB1/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB2/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB3/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB4/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB5/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB6/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRB7/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_LINEAGEQA/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_RADSATQA/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_PIXELQA/h005v002.ncml\"/>\n<netcdf location=\"LANDSAT_ARD_AGGREGATES/LANDSAT_ARD_SRAEROSOLQA/h005v002.ncml\"/>\n</aggregation>\n<attribute name=\"title\" type=\"string\" value=\"Simple union aggregation test\"/>\n</netcdf>\nOK, I will look into this some more.\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "Aafaque (CTR), Aafaque",
     "body": "Hi James,\nThis might be useful to investigate more. I tried bes-debug with \"ncml:memory\" for the procedure that is working for you. (Aggregating resulting ncmls after changing names in each ncml first)\nAttached is log file generated using following command besstandalone -d \"cerr,ncml:memory\" -c bes.conf -i test_agg_tif.bescmd\nalso logs for besstandalone -d \"cerr,ncml\" -c bes.conf -i test_agg_tif.bescmd\nLet me know if you need any more information."
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "jgallagher",
     "body": "On Feb 20, 2018, at 08:11, Aafaque (CTR), Aafaque <aafaque.a...@usgs.gov> wrote:\nHi James,\nThis might be useful to investigate more. I tried bes-debug with \"ncml:memory\" for the procedure that is working for you. (Aggregating resulting ncmls after changing names in each ncml first)\nAttached is log file generated using following command besstandalone -d \"cerr,ncml:memory\" -c bes.conf -i test_agg_tif.bescmd\nalso logs for besstandalone -d \"cerr,ncml\" -c bes.conf -i test_agg_tif.bescmd\nLet me know if you need any more information.\nThanks.\nSlav Korolev will be looking at this issue.\nJames\n- show quoted text -\n<ncm-memory logs.txt><ncml logs.txt>\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "Friesz (CTR), Aaron",
     "body": "Hey James and Slav,\nI may have missed some emails on this... Has there been any movement on this activity? What's the status?\nAaron"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "Friesz (CTR), Aaron",
     "body": "Hi James and Slav,\nI wanted to follow up on this NCML Handler/Libdap4 issue. Has there been any movement on this activity? What's the status?\nAaron"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "jgallagher",
     "body": "On Apr 9, 2018, at 07:54, Friesz (CTR), Aaron <aaron.fr...@usgs.gov> wrote:\nHi James and Slav,\nI wanted to follow up on this NCML Handler/Libdap4 issue. Has there been any movement on this activity? What's the status?\nAaron, et al.,\nThis is a complicated bug. We are continuing to work on it, but the renaming issue is looking like it’s a problem deep in the NcML processing software. What this means is that I still can’t give you an estimate on when we will fix it.\nMy understanding is there are two problems you’ve found, both having to do with aggregating TIFF files:\n1. If you rename variables in a tiff file using NCML, you can aggregate three of the resulting things, but not more than that.\n2. You can aggregate 3, 4, …, tiff files but then renaming the resulting variable does not work.\nSlav, CC’d on this response is working on #1 and I will work on #2\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "Schroeder (CTR), Bradley",
     "body": "Hi James,\nYes, that is the issue.\nOne question we had. In a previous email it sounded like you are able to get #1 to work on your OSX machine. Is it possible that you have some updated code that we don't have running hyrax 1.14.0?\nThanks,\nBrad"
    },
    {
     "page": "Re: NCML Handler / Libdap4 ( Join New aggregation / Changing variable name Issue )",
     "name": "jgallagher",
     "body": "On Apr 9, 2018, at 15:02, Schroeder (CTR), Bradley <bradley.sc...@usgs.gov> wrote:\nHi James,\nYes, that is the issue.\nOne question we had. In a previous email it sounded like you are able to get #1 to work on your OSX machine. Is it possible that you have some updated code that we don't have running hyrax 1.14.0?\nPossible, but more likely I was confused. These are really two very different problems and, I suspect, have nothing to do with TIFF. When you posed the problem initially, I was sure I knew exactly where the issue was and that it was part of an error in the TIFF processing module, but I was wrong.\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi , I hope you’re having a great day! I came across Open Source Project For A Network again a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a technology service specifically built for small to medium-sized nonprofits. We help organizations save time and money by automating your fundraising interactions, in one place, with no technical work required on your end.\nWith your Flipcause subscription, you'll receive every feature and service we have to offer regardless of budget or size.\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you're open to a quick conversation, take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you're not interested, let me know and you'll never hear from me again. :)\nThank you for your time and I hope to hear from you soon!\nJoan Ramirez\nOutreach Representative\nFlipcause Inc.\n283 4th Street, Suite 101 Oakland, CA 94607\n(800) 523-1950\nReply with \"stop\" to unsubscribe\nHere are all of the features and services that come with your Flipcause subscription (we know, it's a lot)…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nMobile app with event and volunteer check-in\nPeer to peer fundraising\nTeam fundraising\nFull payment processing and refund system\nCredit card swiper/card reader\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated Success Team (with unlimited concierge services included at no additional cost)\nFully built website and web hosting services\nSocial media integration\nUnlimited webmaster services (at no additional cost)\nSSL certification and installation\nFully customizable fundraising pages to market your campaigns\nContact database to keep track of your supporters in one place\nEasy to transfer fundraising activity into Quickbooks"
    },
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi , I hope you’re having a great day! I came across Open Source Project For A Network again a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a technology service specifically built for small to medium-sized nonprofits. We help organizations save time and money by automating your fundraising interactions, in one place, with no technical work required on your end.\nWith your Flipcause subscription, you'll receive every feature and service we have to offer regardless of budget or size.\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you're open to a quick conversation, take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you're not interested, let me know and you'll never hear from me again. :)\nThank you for your time and I hope to hear from you soon!\nJoan Ramirez\nOutreach Representative\nFlipcause Inc.\n283 4th Street, Suite 101 Oakland, CA 94607\n(800) 523-1950\nUNSUBSCRIBE: If you would like to stop receiving further messages, reply \"Stop\" to this email.\nHere are all of the features and services that come with your Flipcause subscription (we know, it's a lot)…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nMobile app with event and volunteer check-in\nPeer to peer fundraising\nTeam fundraising\nFull payment processing and refund system\nCredit card swiper/card reader\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated Success Team (with unlimited concierge services included at no additional cost)\nFully built website and web hosting services\nSocial media integration\nUnlimited webmaster services (at no additional cost)\nSSL certification and installation\nFully customizable fundraising pages to market your campaigns\nContact database to keep track of your supporters in one place\nEasy to transfer fundraising activity into Quickbooks"
    }
   ]
  },
  {
  },
  {
   "name": [
    {
     "page": "Join us for the Docker Enterprise Container virtual events, April 25th and 26th",
     "name": "Docker",
     "body": "Hello ,\nJoin Docker and thousands of your peers for the Docker Enterprise Container Platform Virtual Event as we showcase how enterprises are accelerating their digital and multi-cloud initiatives with Docker Enterprise Edition (EE).\nRegister Now for your local virtual event:\nWednesday, April 25th at 11AM PST\nThursday, April 26th at 9AM BST\nMaking a container platform choice does not have to be difficult. Learn from Docker experts and your peers who have done it before how Docker EE uniquely eliminates risk, by addressing key container platform questions, like:\nHow can I avoid technology or infrastructure lock-in?\nHow can I get applications to market faster and lower IT costs?\nHow do I get greater application protection and improve governance without sacrificing performance?\nDon’t miss this chance to chat live with Docker experts, watch new demos and leave with a blueprint for your containerization strategy that can support your ever-changing business environment.\nRegister now to save your seat, wherever it may be.\nThe Docker Team\nRegister Now\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Opendap Netcdf data",
     "name": "Mohammadjavad Ali Zadeh",
     "body": "Dear Administrator\nI am trying to get Cmip5 wind data following your construction.BTW, I can not find the 'Get as NETCDF' module to download the data in .nc file. The figure of the order is attached with this email. May I ask you to help me with.\nI am looking forward to hearing from you\nBest regards\nM.J.Alizadeh​"
    },
    {
     "page": "Opendap Netcdf data",
     "name": "jgallagher",
     "body": "On Apr 3, 2018, at 20:49, Mohammadjavad Ali Zadeh <m.ali...@uq.edu.au> wrote:\nDear Administrator\nI am trying to get Cmip5 wind data following your construction.BTW, I can not find the 'Get as NETCDF' module to download the data in .nc file. The figure of the order is attached with this email. May I ask you to help me with.\nPlease send us the URL of the server you are accessing.\nThanks,\nJames\nI am looking forward to hearing from you\nBest regards\nM.J.Alizadeh​\n<Capture.PNG>\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Please kindly be advised",
     "name": "ALKER & OMMEN GMBH",
     "body": "Dear Sir,\nPlease find attached new Farhad inquiry, and submit your quotation with\nbest price and delivery time, please help us to treat this as urgent,\nso as to avoid Farhad ordering from other suppliers,\nplease feel free to ask any question if you do not understand anything on the attached RFQ.\nThank you as we expect your urgent quotation.\nBest Regards\nMs. Ayah Assadi\nAssistant Procurement Manager"
    }
   ]
  },
  {
   "name": [
    {
     "page": "opendap on matlab with XP win64",
     "name": "Kamazima Lwiza",
     "body": "Hello,\nPlease send (point to) me the latest version of opendap that works with matlab on XP win 64 pc.\nThank you\nKamazima Lwiza\nSchool of Marine and Atmospheric Sciences\nStony Brook University\nStony Brook, NY 11794-5000, USA\nTel: 631-632-7309; Fax: 631-632-8820\nkamazim...@stonybrook.edu\nhttp://www.somas.stonybrook.edu\n******************************************************************************"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Satellite data download bug",
     "name": "Stefan.S...@dlr.de",
     "body": "Dear ladies and gentlemen,\ntrying to download your data from the server I have experienced some difficulties:\nWhereas the initial two links worked:\nhttps://acdisc.gsfc.nasa.gov/data/Aura_OMI_Level3/OMAEROe.003/doc/OMAEROe_OSIPS_README_V003.doc\nhttps://docserver.gesdisc.eosdis.nasa.gov/repository/Mission/OMI/3.3_ScienceDataProductDocumentation/3.3.2_ProductRequirements_Designs/README.OMI_DUG.pdf\nthe datalinks like, e.g., this one:\nhttp://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMAEROe.003/2004/OMI-Aura_L3-OMAEROe_2004m1001_v003-2011m1109t080411.he5.ncml?AbsorbingAerosolOpticalThicknessMW[0:4][0:719][0:1439],AerosolModelMW[0:719][0:1439],AerosolOpticalThicknessMW[0:4][0:719][0:1439],AerosolOpticalThicknessPassedThresholdMean[0:719][0:1439],AerosolOpticalThicknessPassedThresholdStd[0:719][0:1439],SingleScatteringAlbedoMW[0:4][0:719][0:1439],SingleScatteringAlbedoPassedThresholdMean[0:719][0:1439],SingleScatteringAlbedoPassedThresholdStd[0:719][0:1439],SolarZenithAngle[0:719][0:1439],TerrainReflectivity[0:719][0:1439],UVAerosolIndex[0:719][0:1439],VISAerosolIndex[0:719][0:1439],ViewingZenithAngle[0:719][0:1439],nWavelDiagnostic,lat[0:719],lon[0:1439]\ndid not.\nThe specific error message associated with my request was:\nFailed to locate resource: /HDF-EOS5/ncml/Aura_OMI_Level3/OMAEROe.003/2004/OMI-Aura_L3-OMAEROe_2004m1001_v003-2011m1109t080411.he5.nc\nDo you have an idea to solve the problem? Thank you so much in advance.\nBest regards\nStefan Scharring\n——————————————————————————\nDeutsches Zentrum für Luft- und Raumfahrt e.V. (DLR)\nGerman Aerospace Center\nInstitute of Technical Physics | Pfaffenwaldring 38-40 | 70569 Stuttgart | Germany\nDr. Stefan Scharring\nTelephone 0711 6862 517 | Telefax 0711 6862 788 | stefan.scharring@dlr.de\nwww.DLR.de"
    },
    {
     "page": "Satellite data download bug",
     "name": "Nathan Potter",
     "body": "Hi Stefan,\nWould you please share with me a narrative regarding how you came to experience the error?\nWhen I looked at your subset URL it appears you have copied and pasted it from the HTML Data Request Form here:\nhttp://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMAEROe.003/2004/OMI-Aura_L3-OMAEROe_2004m1001_v003-2011m1109t080411.he5.ncml.html\nWhich I think is a mistake, in that you cannot use the URL as it appears in the text field without some modification.\nFor example, if you push the “Get ASCII” button on the form page the URL is altered by appending a suffix of “.ascii” to the request URL path, the part before the question mark.\nHere’s another example that returns just the DDS metadata response. I took your request URL and appended “.dds” to the URL path:\nhttp://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMAEROe.003/2004/OMI-Aura_L3-OMAEROe_2004m1001_v003-2011m1109t080411.he5.ncml.dds?AbsorbingAerosolOpticalThicknessMW[0:4][0:719][0:1439],AerosolModelMW[0:719][0:1439],AerosolOpticalThicknessMW[0:4][0:719][0:1439],AerosolOpticalThicknessPassedThresholdMean[0:719][0:1439],AerosolOpticalThicknessPassedThresholdStd[0:719][0:1439],SingleScatteringAlbedoMW[0:4][0:719][0:1439],SingleScatteringAlbedoPassedThresholdMean[0:719][0:1439],SingleScatteringAlbedoPassedThresholdStd[0:719][0:1439],SolarZenithAngle[0:719][0:1439],TerrainReflectivity[0:719][0:1439],UVAerosolIndex[0:719][0:1439],VISAerosolIndex[0:719][0:1439],ViewingZenithAngle[0:719][0:1439],nWavelDiagnostic,lat[0:719],lon[0:1439]\nNOTE: Your emailer is likely to botch this link by dropping the closing “]” from the link - you will need to copy and paste it into a browser’s address bar or into a curl command to make it work:\ncurl -g -k -n -c ursCookies -b ursCookies -L --url http://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/ncml/Aura_OMI_Level3/OMAEROe.003/2004/OMI-Aura_L3-OMAEROe_2004m1001_v003-2011m1109t080411.he5.ncml.dds?AbsorbingAerosolOpticalThicknessMW[0:4][0:719][0:1439],AerosolModelMW[0:719][0:1439],AerosolOpticalThicknessMW[0:4][0:719][0:1439],AerosolOpticalThicknessPassedThresholdMean[0:719][0:1439],AerosolOpticalThicknessPassedThresholdStd[0:719][0:1439],SingleScatteringAlbedoMW[0:4][0:719][0:1439],SingleScatteringAlbedoPassedThresholdMean[0:719][0:1439],SingleScatteringAlbedoPassedThresholdStd[0:719][0:1439],SolarZenithAngle[0:719][0:1439],TerrainReflectivity[0:719][0:1439],UVAerosolIndex[0:719][0:1439],VISAerosolIndex[0:719][0:1439],ViewingZenithAngle[0:719][0:1439],nWavelDiagnostic,lat[0:719],lon[0:1439]\nIf by chance you are planning on using curl to retrieve data you should read the instructions for making it work with NASA’s Earthdata Login (aka URS) service here:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_authentication_for_dap_clients\nYou may also request data in a number of response encodings by utilizing other suffix expressions on the URL path:\nDAP2: “.dods”\nDAP4: “.dap\"\nNetCDF-3: “.nc”\nNetCDF-4: “.nc4”\nJSON: “.json”\nPlease let me know if this is enough to get you going, and feel free to ask for additional support.\nSincerely,\nNathan\n- show quoted text -\n> Telephone 0711 6862 517 | Telefax 0711 6862 788 | stefan.s...@dlr.de\n> www.DLR.de\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Matt Ueckermann",
     "body": "HI Folks,\nI’ve been badgering the excellent people at NSIDC (see below) about OpenDAP access using PyDAP. My problem is described here: https://github.com/pydap/pydap/issues/163 with additional details below.\nEssentially it seems like there’s been a string parsing or escaping issue after 1.13.1. As part of the metadata, a greater-than sign is no longer decoded – and since the html encoding contains a semi-colon, the ‘das’ file is not correctly interpreted.\nHyrax 1.14.0:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nHyrax 1.13.5:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nPrevious Hyrax version (1.13.1):\n<gco:CharacterString>NSIDC DAAC > National Snow and Ice Data Center DAAC</gco:CharacterString>\nThe same thing seems to happen in decoding the keys use for the dataset. Copied from below:\nFor example, I used to access longitude using the key:\no ‘Soil_Moisture_Retrieval_Data_1km_longitude_1km’\n· but now it is\no ‘%2FSoil_Moisture_Retrieval_Data_1km%2Flongitude_1km’\nHere the forward slash “/” is not escaped and the %2F remains.\nPlease let me know if this is a bug, or if it’s new / expected behavior. If the latter, I need to work with the pydap folks for a work-around.\nThanks very much,\nMatt\nFrom: National Snow & Ice Data Center [mailto:ns...@nsidc.org]\nSent: Monday, March 19, 2018 7:31 AM\nTo: Matt Ueckermann\nSubject: [National Snow & Ice Data Center] Re: openDap access\n## Please do not write below this line ##\nMatt Ueckermann, Mar 19, 05:30 MDT:\nHi Amy,\nI will contact the OpenDAP folks as well.\nIn the mean time, my eds = … line of code is somewhat complicated, but I’ve copied it below.\nBest,\nMatt\n=====================================================================================================================\nimport requests\nclass SessionWithHeaderRedirection(requests.Session):\n\"\"\"\nModified from: https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+Python\n\"\"\"\nAUTH_HOST = ''\ndef __init__(self, username=None, password=None):\nsuper(SessionWithHeaderRedirection, self).__init__()\nself.auth = (username, password)\n# Overrides from the library to keep headers when redirected to or from\n# the NASA auth host.\ndef rebuild_auth(self, prepared_request, response):\nheaders = prepared_request.headers\nurl = prepared_request.url\nif 'Authorization' in headers:\noriginal_parsed = requests.utils.urlparse(response.request.url)\nredirect_parsed = requests.utils.urlparse(url)\nif (original_parsed.hostname != redirect_parsed.hostname) and \\\nredirect_parsed.hostname != self.AUTH_HOST and \\\noriginal_parsed.hostname != self.AUTH_HOST:\ndel headers['Authorization']\nreturn\nclass EarthDataSession(SessionWithHeaderRedirection):\nAUTH_HOST = 'urs.earthdata.nasa.gov'\neds = EarthDataSession (username=<My EarthDataLogin Username>, password=<My EarthDataLogin Password>)\n# Test url: used to generate cookies for session\neds.get(\"https://n5eil01u.ecs.nsidc.org:443/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5.dds\")\n=====================================================================================================================\n...\nAmy, Mar 16, 10:57 MDT:\nHi Matt,\nWe were told that this bug was fixed in 1.14, so I apologize for this misunderstanding.\nOur System Administrator working on OPeNDAP suggested reaching out to their main contact email: sup...@opendap.org. He said they are usually very responsive. We are also happy to ping them on Monday if you don't hear anything today.\nOur SA was also wondering about the following line in your python code:\neds = #authenticated earth data login session\nDo you mind letting us know what this variable is so that we can attempt to replicate?\nThanks once again for your patience on this. Pydap is not something we support directly but we're doing what we can to understand the problem and make sure we can catch these issues in the future.\nBest regards,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Mar 15, 18:20 MDT:\nI Amy,\nI've just tried it. Unfortunately, I see the same thing as I did with Hyrax 1.13.5... so this did not solve my problem.\nHyrax 1.14.0:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nHyrax 1.13.5:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nPrevious Hyrax version (1.13.1):\n<gco:CharacterString>NSIDC DAAC > National Snow and Ice Data Center DAAC</gco:CharacterString>\nShould I connect with the OpenDAP folks directly? Do you have contact information for me?\nThanks,\nMatt\n________________________________________\nAmy, Mar 15, 12:10 MDT:\nHi Matt,\nHyrax 1.14.0 is now installed. Thank you for your patience. Our testing process does not involve pydap directly (we are mainly focused on verification of subsetting and reformatting compared to the original file) but I do see some changes to the das files compared to 1.13.5 so I hope this solves your problem. Please let me know if you have any problems with access.\nBest regards,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Mar 1, 15:17 MST:\nThanks very much for the update. This timeline works for me.\nAmy, Mar 1, 15:15 MST:\nHi Matt,\nThanks for providing more details on how this is impacting you and your team. We decided to bypass our external development path and get this upgrade installed in our test mode on Monday. We will test the system next week and, assuming everything looks good, we'll plan to upgrade to 1.14 on our public server the following week.\nI hope this timeline works for you, and I'll let you know if anything changes.\nThanks for your patience,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Feb 28, 15:41 MST:\nHI Amy,\nThanks for the update. It’s not an immediate impediment, but our engineers are prioritizing other tasks until this is fixed.\nI’m a developer of a library that facilitates access to SMAP data and we have some internal folks who are doing validation work that’s been put on hold until this is fixed. I’m trying to gauge whether or not to deploy the workaround to them, but I’d have to revert some changes once the upgrade finishes, so I’d rather just wait.\nThanks,\nMatt\nAmy, Feb 28, 09:25 MST:\nHi Matt,\nUnfortunately I do not have an ETA yet. We are coordinating this upgrade with our external development team and we have prioritized it relatively high in their backlog. I know you mentioned that you have some workarounds in place but please let me know if this metadata bug is a major impediment for you so that I can add more information on user impact in the upgrade ticket. I can also work with our in house developers on pydap workarounds, or other access methods that can suite your needs so let me know how we can help in the meantime. Again, I apologize for the inconvenience and I will update you once I have a better estimate.\nThanks,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Feb 28, 08:07 MST:\nHi Amy,\nIs there an ETA on the upgrade to Hyrax v1.14?\nThanks,\nMatt\nMatt Ueckermann, Feb 13, 17:17 MST:\nHi Amy,\nThe different keys are not a complete roadblock for me -- thank you. I just thought it might be helpfu0 informationl, but is seems that if the OpenDAP folks know about the problem, and have fixed it, it probably wasn't useful.\nPlease let me know when the new version is installed.\nThanks,\nMatt\n________________________________________\nAmy, Feb 13, 17:01 MST:\nHi Matt,\nThanks for the additional information. We heard from the folks at OPeNDAP and they are aware of the issue, but they also let us know that the problem is fixed in the newest version of Hyrax, v1.14. We will work to get this installed as soon as possible, and I will keep you updated on our progress.\nAs far as the dataset keys are concerned, is this something that is a complete roadblock for you, or are you able to work around this issue as well? Again, please let me know how I can help and I apologize again for the inconvenience.\nThanks,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Feb 13, 12:29 MST:\nHi Amy,\nThanks for working on this.\nIncidentally, I have been working on a work-around from the pydap side. I can now manage to load the dataset, but it seems like the dataset keys have also changed.\nFor example:\n· For the file mentioned below, I used to access longitude using the key:\no ‘Soil_Moisture_Retrieval_Data_1km_longitude_1km’\n· but now it is\no ‘%2FSoil_Moisture_Retrieval_Data_1km%2Flongitude_1km’\nPerhaps that additional information will help to debug the problem.\nThanks for keeping me up to date on your progress,\nMatt\nAmy, Feb 13, 12:09 MST:\nHi Matt,\nThanks for providing that file and confirming the change since last week. We rolled back the version in our test mode and we did notice a change in the .das file:\nPrevious Hyrax version (1.13.1):\n<gco:CharacterString>NSIDC DAAC > National Snow and Ice Data Center DAAC</gco:CharacterString>\nHyrax 1.13.5:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nFor the time being, we would like to retain the newer version in our public OPeNDAP due to the overall improvements for our system. However, we have notified the OPeNDAP team of this problem and we will make sure to work with them to understand the issue and work towards a resolution. In the meantime, I want to make sure we can help you with a workaround so that you can continue to access SMAP data. I know you reported the issue on Github, so perhaps someone working on pydap can help as far as that library is concerned. On our end, we have several other methods of accessing SMAP data, including our HTTPS data pool (https://n5eil01u.ecs.nsidc.org/SMAP/) and our subsetting, reformatting, and reprojection service API (https://nsidc.org/support/how/how-do-i-programmatically-request-data-services).\nPlease let me know how we can help further, and I'll make sure to keep you updated on our progress.\nThank you,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Feb 12, 15:33 MST:\nHi Amy,\nThanks for looking into my issue.\nI used to be able to access this file:\nhttps://n5eil01u.ecs.nsidc.org:443/opendap/SMAP/SPL2SMAP_S.001/2017.02.08/SMAP_L2_SM_SP_1AIWDV_20170208T011127_20170208T004300_079E30N_R15180_001.h5\n… but, now I’m getting the same error as described in my email below.\nThanks,\nMatt\nAmy, Feb 12, 15:18 MST:\nHi Matt,\nThanks for contacting NSIDC regarding this OPeNDAP issue. We did install Hyrax 1.13.5 last Thursday, so we're wondering if your issue is related to the upgrade. No other changes were made on our end as far as metadata goes.\nIf you have a SMAP file that you successfully accessed with pydap prior to last Thursday, can you try to access it again? Do you get the same error? This will help us troubleshoot whether or not the Hyrax version is implicated. If so, we will roll back to the previous version and I'll ask if you can try accessing via pydap once again.\nThanks for working with us, and I apologize for the inconvenience.\nBest regards,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: ns...@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Feb 12, 09:10 MST:\nGreetings,\n* Has NSIDC recently changed the meta-data served through the OpenDAP protocol?\n* If yes, is there any way to remove errant semi-colons that are in the metadata?\nI've been accessing SMAP datasources using the OpenDAP protocol with pydap (through Python).\nRecently (noticed this Friday), I have been unable to so because of an error parsing the metadata in the .das file (which is part of OpenDAP). I've posted an issue on the pydap github page here with the technical details: https://github.com/pydap/pydap/issues/163\nThe problematic section of the metadata is as follows:\n<gmd:organisationName>\n<gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n</gmd:organisationName>\nEssentially, the organization name has an errant semi-colon which looks a lot like escaped html, but the .das format uses semi-colons to separate different fields. As such, pydap does not successfully parse the metadata and fails ungracefully.\nI'm not sure if the problem is with pydap, or with the metadata from NSIDC -- i.e. whether the errant semi-colon follows or breaks the .das fileformat spec.\nAny help much appreciated,\nMatt\n------------------\nSubmitted from: https://nsidc.org/\nYou can add a comment by replying to this email.\nThis email is a service from National Snow & Ice Data Center.\n[Z39R9X-76G2]"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "jgallagher",
     "body": "On Mar 19, 2018, at 05:50, Matt Ueckermann <m...@creare.com> wrote:\nHI Folks,\nI’ve been badgering the excellent people at NSIDC (see below) about OpenDAP access using PyDAP. My problem is described here: https://github.com/pydap/pydap/issues/163 with additional details below.\nEssentially it seems like there’s been a string parsing or escaping issue after 1.13.1. As part of the metadata, a greater-than sign is no longer decoded – and since the html encoding contains a semi-colon, the ‘das’ file is not correctly interpreted.\nHyrax 1.14.0:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nHyrax 1.13.5:\n(<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nMatt,\nThe response above, with the &gt; is actually correct XML. It’s a pain because the XML and DAS syntaxes are fighting here, since the ; in a value has to be escaped either with a backslash or by enclosing it in double quotes. Luckily, the latter _is_ the case, so a simple modification to the DAS parser should fix this problem. I hope the PyDAP developers can help you with that.\nThat is, for a DAS attribute value that is a String, the characters between double quotes should not be evaluated except that embedded double quotes should be escaped (and that’s the case with these servers’ values).\nThis next question is a bit harder...\nPrevious Hyrax version (1.13.1):\n<gco:CharacterString>NSIDC DAAC > National Snow and Ice Data Center DAAC</gco:CharacterString>\nThe same thing seems to happen in decoding the keys use for the dataset. Copied from below:\nFor example, I used to access longitude using the key:\no ‘Soil_Moisture_Retrieval_Data_1km_longitude_1km’\n· but now it is\no ‘%2FSoil_Moisture_Retrieval_Data_1km%2Flongitude_1km’\nHere the forward slash “/” is not escaped and the %2F remains.\nPlease let me know if this is a bug, or if it’s new / expected behavior. If the latter, I need to work with the pydap folks for a work-around.\nThis is a new behavior. Working with The HDF Group, we’ve been trying to sort out how to represent HDF5 Groups in DAP2 and decided on using slash characters. Of course, that’s not a great syntax for the web, so the slash is escaped. The old technique was to represent slash as an underscore, but that meant that the names were hard to recover in some cases.\nThus, this is the new expected behavior.\nSorry for the hassles this causes. If you would like me to talk to the PyDAP developers, feel free to ask.\nJames\nThanks very much,\nMatt\nFrom: National Snow & Ice Data Center [mailto...@nsidc.org]\n- show quoted text -\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Matt Ueckermann",
     "body": "Hi James,\nThanks for the response. This is very helpful. Seems like the new behavior is the desired behavior, but unfortunately breaks PyDAP.\nBased on your response below, I can fix PyDAP locally, but this really should get into the main repository. If you could ping the PyDAP developers for me, that would be very helpful. I made an issue a month ago but haven’t gotten any response.\nThanks,\nMatt\nFrom: James Gallagher [mailto:jgall...@opendap.org]\nSent: Monday, March 19, 2018 1:47 PM\nTo: Matt Ueckermann\nCc: James Gallagher; sup...@opendap.org\nSubject: Re: [support] PyDAP Access broken after HyRAX upgrade\n- show quoted text -\nThanks very much,\nMatt\nFrom: National Snow & Ice Data Center [mailto:nsidc@nsidc.org]\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\nOur System Administrator working on OPeNDAP suggested reaching out to their main contact email: support@opendap.org. He said they are usually very responsive. We are also happy to ping them on Monday if you don't hear anything today.\nOur SA was also wondering about the following line in your python code:\neds = #authenticated earth data login session\nDo you mind letting us know what this variable is so that we can attempt to replicate?\nThanks once again for your patience on this. Pydap is not something we support directly but we're doing what we can to understand the problem and make sure we can catch these issues in the future.\nBest regards,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: nsidc@nsidc.org\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\nEmail: nsidc@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Mar 1, 15:17 MST:\nThanks very much for the update. This timeline works for me.\nAmy, Mar 1, 15:15 MST:\nHi Matt,\nThanks for providing more details on how this is impacting you and your team. We decided to bypass our external development path and get this upgrade installed in our test mode on Monday. We will test the system next week and, assuming everything looks good, we'll plan to upgrade to 1.14 on our public server the following week.\nI hope this timeline works for you, and I'll let you know if anything changes.\nThanks for your patience,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: nsidc@nsidc.org\nURL: http://nsidc.org\nTwitter: @NSIDC\nNational Snow and Ice Data Center * Distributed Active Archive Center\nMatt Ueckermann, Feb 28, 15:41 MST:\nHI Amy,\nThanks for the update. It’s not an immediate impediment, but our engineers are prioritizing other tasks until this is fixed.\nI’m a developer of a library that facilitates access to SMAP data and we have some internal folks who are doing validation work that’s been put on hold until this is fixed. I’m trying to gauge whether or not to deploy the workaround to them, but I’d have to revert some changes once the upgrade finishes, so I’d rather just wait.\nThanks,\nMatt\nAmy, Feb 28, 09:25 MST:\nHi Matt,\nUnfortunately I do not have an ETA yet. We are coordinating this upgrade with our external development team and we have prioritized it relatively high in their backlog. I know you mentioned that you have some workarounds in place but please let me know if this metadata bug is a major impediment for you so that I can add more information on user impact in the upgrade ticket. I can also work with our in house developers on pydap workarounds, or other access methods that can suite your needs so let me know how we can help in the meantime. Again, I apologize for the inconvenience and I will update you once I have a better estimate.\nThanks,\nAmy\nNSIDC User Services\nCIRES, 449 UCB\nUniversity of Colorado\nBoulder, CO 80309-0449, USA\nPhone: +1 303-492-6199\nFax: +1 303-492-2468\nEmail: nsidc@nsidc.org\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\nEmail: nsidc@nsidc.org\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\nEmail: nsidc@nsidc.org\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\n- show quoted text -\nEmail: nsidc@nsidc.org\n- show quoted text -\n- show quoted text -\n- show quoted text -\nThis email is a service from National Snow & Ice Data Center.\n[Z39R9X-76G2]\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Nathan Potter",
     "body": "Matt,\nIf you “fork” the PyDAP project on GitHub you can make a local fix, push it to your GitHub account and then issue a pull request for your changes. That might make it happen much faster.\nCheers,\nNathan\n- show quoted text -\n> From: National Snow & Ice Data Center [mailto:ns...@nsidc.org]\n> Sent: Monday, March 19, 2018 7:31 AM\n> To: Matt Ueckermann\n> Subject: [National Snow & Ice Data Center] Re: openDap access\n>\n> ## Please do not write below this line ##\n>\n- show quoted text -\n> Amy, Mar 16, 10:57 MDT:\n> Hi Matt,\n>\n> We were told that this bug was fixed in 1.14, so I apologize for this misunderstanding.\n>\n> Our System Administrator working on OPeNDAP suggested reaching out to their main contact email: sup...@opendap.org. He said they are usually very responsive. We are also happy to ping them on Monday if you don't hear anything today.\n>\n> Our SA was also wondering about the following line in your python code:\n> eds = #authenticated earth data login session\n>\n> Do you mind letting us know what this variable is so that we can attempt to replicate?\n>\n> Thanks once again for your patience on this. Pydap is not something we support directly but we're doing what we can to understand the problem and make sure we can catch these issues in the future.\n>\n> Best regards,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Mar 15, 18:20 MDT:\n> I Amy,\n> I've just tried it. Unfortunately, I see the same thing as I did with Hyrax 1.13.5... so this did not solve my problem.\n> Hyrax 1.14.0:\n> (<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n> Hyrax 1.13.5:\n> (<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n> Previous Hyrax version (1.13.1):\n> <gco:CharacterString>NSIDC DAAC > National Snow and Ice Data Center DAAC</gco:CharacterString>\n> Should I connect with the OpenDAP folks directly? Do you have contact information for me?\n> Thanks,\n> Matt\n> ________________________________________\n>\n> Amy, Mar 15, 12:10 MDT:\n> Hi Matt,\n>\n> Hyrax 1.14.0 is now installed. Thank you for your patience. Our testing process does not involve pydap directly (we are mainly focused on verification of subsetting and reformatting compared to the original file) but I do see some changes to the das files compared to 1.13.5 so I hope this solves your problem. Please let me know if you have any problems with access.\n>\n> Best regards,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Mar 1, 15:17 MST:\n> Thanks very much for the update. This timeline works for me.\n>\n> Amy, Mar 1, 15:15 MST:\n> Hi Matt,\n>\n> Thanks for providing more details on how this is impacting you and your team. We decided to bypass our external development path and get this upgrade installed in our test mode on Monday. We will test the system next week and, assuming everything looks good, we'll plan to upgrade to 1.14 on our public server the following week.\n>\n> I hope this timeline works for you, and I'll let you know if anything changes.\n>\n> Thanks for your patience,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Feb 28, 15:41 MST:\n> HI Amy,\n> Thanks for the update. It’s not an immediate impediment, but our engineers are prioritizing other tasks until this is fixed.\n> I’m a developer of a library that facilitates access to SMAP data and we have some internal folks who are doing validation work that’s been put on hold until this is fixed. I’m trying to gauge whether or not to deploy the workaround to them, but I’d have to revert some changes once the upgrade finishes, so I’d rather just wait.\n> Thanks,\n> Matt\n>\n> Amy, Feb 28, 09:25 MST:\n> Hi Matt,\n>\n> Unfortunately I do not have an ETA yet. We are coordinating this upgrade with our external development team and we have prioritized it relatively high in their backlog. I know you mentioned that you have some workarounds in place but please let me know if this metadata bug is a major impediment for you so that I can add more information on user impact in the upgrade ticket. I can also work with our in house developers on pydap workarounds, or other access methods that can suite your needs so let me know how we can help in the meantime. Again, I apologize for the inconvenience and I will update you once I have a better estimate.\n>\n> Thanks,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Feb 28, 08:07 MST:\n> Hi Amy,\n> Is there an ETA on the upgrade to Hyrax v1.14?\n> Thanks,\n> Matt\n>\n> Matt Ueckermann, Feb 13, 17:17 MST:\n> Hi Amy,\n> The different keys are not a complete roadblock for me -- thank you. I just thought it might be helpfu0 informationl, but is seems that if the OpenDAP folks know about the problem, and have fixed it, it probably wasn't useful.\n> Please let me know when the new version is installed.\n> Thanks,\n> Matt\n> ________________________________________\n>\n> Amy, Feb 13, 17:01 MST:\n> Hi Matt,\n>\n> Thanks for the additional information. We heard from the folks at OPeNDAP and they are aware of the issue, but they also let us know that the problem is fixed in the newest version of Hyrax, v1.14. We will work to get this installed as soon as possible, and I will keep you updated on our progress.\n>\n> As far as the dataset keys are concerned, is this something that is a complete roadblock for you, or are you able to work around this issue as well? Again, please let me know how I can help and I apologize again for the inconvenience.\n>\n> Thanks,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Feb 13, 12:29 MST:\n> Hi Amy,\n> Thanks for working on this.\n> Incidentally, I have been working on a work-around from the pydap side. I can now manage to load the dataset, but it seems like the dataset keys have also changed.\n> For example:\n> · For the file mentioned below, I used to access longitude using the key:\n> o ‘Soil_Moisture_Retrieval_Data_1km_longitude_1km’\n> · but now it is\n> o ‘%2FSoil_Moisture_Retrieval_Data_1km%2Flongitude_1km’\n> Perhaps that additional information will help to debug the problem.\n> Thanks for keeping me up to date on your progress,\n> Matt\n>\n> Amy, Feb 13, 12:09 MST:\n> Hi Matt,\n>\n> Thanks for providing that file and confirming the change since last week. We rolled back the version in our test mode and we did notice a change in the .das file:\n>\n> Previous Hyrax version (1.13.1):\n> <gco:CharacterString>NSIDC DAAC > National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n> Hyrax 1.13.5:\n> (<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n> For the time being, we would like to retain the newer version in our public OPeNDAP due to the overall improvements for our system. However, we have notified the OPeNDAP team of this problem and we will make sure to work with them to understand the issue and work towards a resolution. In the meantime, I want to make sure we can help you with a workaround so that you can continue to access SMAP data. I know you reported the issue on Github, so perhaps someone working on pydap can help as far as that library is concerned. On our end, we have several other methods of accessing SMAP data, including our HTTPS data pool (https://n5eil01u.ecs.nsidc.org/SMAP/) and our subsetting, reformatting, and reprojection service API (https://nsidc.org/support/how/how-do-i-programmatically-request-data-services).\n>\n> Please let me know how we can help further, and I'll make sure to keep you updated on our progress.\n>\n> Thank you,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Feb 12, 15:33 MST:\n> Hi Amy,\n> Thanks for looking into my issue.\n> I used to be able to access this file:\n> https://n5eil01u.ecs.nsidc.org:443/opendap/SMAP/SPL2SMAP_S.001/2017.02.08/SMAP_L2_SM_SP_1AIWDV_20170208T011127_20170208T004300_079E30N_R15180_001.h5\n> … but, now I’m getting the same error as described in my email below.\n> Thanks,\n> Matt\n>\n> Amy, Feb 12, 15:18 MST:\n> Hi Matt,\n>\n> Thanks for contacting NSIDC regarding this OPeNDAP issue. We did install Hyrax 1.13.5 last Thursday, so we're wondering if your issue is related to the upgrade. No other changes were made on our end as far as metadata goes.\n>\n> If you have a SMAP file that you successfully accessed with pydap prior to last Thursday, can you try to access it again? Do you get the same error? This will help us troubleshoot whether or not the Hyrax version is implicated. If so, we will roll back to the previous version and I'll ask if you can try accessing via pydap once again.\n>\n> Thanks for working with us, and I apologize for the inconvenience.\n>\n> Best regards,\n> Amy\n>\n> NSIDC User Services\n>\n> CIRES, 449 UCB\n>\n> University of Colorado\n>\n> Boulder, CO 80309-0449, USA\n>\n> Phone: +1 303-492-6199\n> Fax: +1 303-492-2468\n> Email: ns...@nsidc.org\n> URL: http://nsidc.org\n> Twitter: @NSIDC\n>\n> National Snow and Ice Data Center * Distributed Active Archive Center\n>\n>\n> Matt Ueckermann, Feb 12, 09:10 MST:\n> Greetings,\n> * Has NSIDC recently changed the meta-data served through the OpenDAP protocol?\n> * If yes, is there any way to remove errant semi-colons that are in the metadata?\n> I've been accessing SMAP datasources using the OpenDAP protocol with pydap (through Python).\n> Recently (noticed this Friday), I have been unable to so because of an error parsing the metadata in the .das file (which is part of OpenDAP). I've posted an issue on the pydap github page here with the technical details: https://github.com/pydap/pydap/issues/163\n> The problematic section of the metadata is as follows:\n> <gmd:organisationName>\n> <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n> </gmd:organisationName>\n> Essentially, the organization name has an errant semi-colon which looks a lot like escaped html, but the .das format uses semi-colons to separate different fields. As such, pydap does not successfully parse the metadata and fails ungracefully.\n> I'm not sure if the problem is with pydap, or with the metadata from NSIDC -- i.e. whether the errant semi-colon follows or breaks the .das fileformat spec.\n> Any help much appreciated,\n> Matt\n> ------------------\n> Submitted from: https://nsidc.org/\n> You can add a comment by replying to this email.\n> This email is a service from National Snow & Ice Data Center.\n> [Z39R9X-76G2]\n>\n> --\n> James Gallagher\n> jgall...@opendap.org\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Matt Ueckermann",
     "body": "Hi Nathan,\nI should have said that I *think* I can fix it... I first have to figure out how to fix it.\nIs there spec for the .das file that I can refer to?\nThanks,\nMatt\n-----Original Message-----\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Monday, March 19, 2018 1:59 PM\nTo: Matt Ueckermann\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "jgallagher",
     "body": "On Mar 19, 2018, at 12:26, Matt Ueckermann <m...@creare.com> wrote:\nHi Nathan,\nI should have said that I *think* I can fix it... I first have to figure out how to fix it.\nIs there spec for the .das file that I can refer to?\nThat’s on www.opendap.org under support.\nTo parse the DAS, for String attributes, just gobble up chars untl you find the matching double quote. Since internal d.quotes are escaped, that should be pretty simple.\nJames\nThanks,\nMatt\n-----Original Message-----\nFrom: Nathan Potter [mailt...@opendap.org]\nSent: Monday, March 19, 2018 1:59 PM\nTo: Matt Ueckermann\nCc: Nathan Potter; James Gallagher; sup...@opendap.org\nSubject: Re: [support] PyDAP Access broken after HyRAX upgrade\nMatt,\nIf you “fork” the PyDAP project on GitHub you can make a local fix, push it to your GitHub account and then issue a pull request for your changes. That might make it happen much faster.\nCheers,\nNathan\nOn Mar 19, 2018, at 10:52 AM, Matt Ueckermann <m...@creare.com> wrote:\nHi James,\nThanks for the response. This is very helpful. Seems like the new behavior is the desired behavior, but unfortunately breaks PyDAP.\nBased on your response below, I can fix PyDAP locally, but this really should get into the main repository. If you could ping the PyDAP developers for me, that would be very helpful. I made an issue a month ago but haven’t gotten any response.\nThanks,\nMatt\nFrom: James Gallagher [mailto:j...@opendap.org]\n- show quoted text -\nFrom: National Snow & Ice Data Center [mailto...@nsidc.org]\n- show quoted text -\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Matt Ueckermann",
     "body": "As far as I can tell, d.quotes are NOT escaped… at least by the time pydap gets a hold of them in python.\nFrom here:\nhttps://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5.das\nA small excerpt:\nString iso_19139_dataset_xml \"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\" ?>\n<gmd:DS_Series xmlns:gmd=\"http://www.isotc211.org/2005/gmd\" xmlns=\"http://www.isotc211.org/2005/gmi\" xmlns:eos=\"http://earthdata.nasa.gov/schema/eos\" xmlns:gco=\"http://www.isotc211.org/2005/gco\" xmlns:gmi=\"http://www.isotc211.org/2005/gmi\" xmlns:gml=\"http://www.opengis.net/gml/3.2\" xmlns:gmx=\"http://www.isotc211.org/2005/gmx\" xmlns:gsr=\"http://www.isotc211.org/2005/gsr\" xmlns:gss=\"http://www.isotc211.org/2005/gss\" xmlns:gts=\"http://www.isotc211.org/2005/gts\" xmlns:srv=\"http://www.isotc211.org/2005/srv\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.isotc211.org/2005/gmi http://cdn.earthdata.nasa.gov/iso/schema/1.0/ISO19115-2_EOS.xsd\">\n…\nAfter this line https://github.com/pydap/pydap/blob/release/3.2.1/src/pydap/handlers/dap.py#L61, I have unescaped d.quotes in the python string.\nI’m now using the following regex, and that *seems* to do the trick: \".*?[^\\\\]\"[,;] but it could fail in some instances as well.\nStill testing…\nMatt\nFrom: James Gallagher [mailto:jgall...@opendap.org]\nSent: Monday, March 19, 2018 3:51 PM\nTo: Matt Ueckermann\nCc: James Gallagher; Nathan Potter; sup...@opendap.org\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Nathan Potter",
     "body": "Hi Matt,\nIn our test server we have an NSIDC dataset, and the double quotes are coming out escapade:\nhttp://test.opendap.org/opendap/NSIDC/SMAP_L3_SM_P_20150406_R14010_001.h5.das\nNot that this helps you but clearly something odd is happening…\nNathan\n> On Mar 19, 2018, at 1:13 PM, Matt Ueckermann <m...@creare.com> wrote:\n>\n> As far as I can tell, d.quotes are NOT escaped… at least by the time pydap gets a hold of them in python.\n>\n> From here:\n> https://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5.das\n>\n> A small excerpt:\n> String iso_19139_dataset_xml \"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\" ?>\n> <gmd:DS_Series xmlns:gmd=\"http://www.isotc211.org/2005/gmd\" xmlns=\"http://www.isotc211.org/2005/gmi\" xmlns:eos=\"http://earthdata.nasa.gov/schema/eos\" xmlns:gco=\"http://www.isotc211.org/2005/gco\" xmlns:gmi=\"http://www.isotc211.org/2005/gmi\" xmlns:gml=\"http://www.opengis.net/gml/3.2\" xmlns:gmx=\"http://www.isotc211.org/2005/gmx\" xmlns:gsr=\"http://www.isotc211.org/2005/gsr\" xmlns:gss=\"http://www.isotc211.org/2005/gss\" xmlns:gts=\"http://www.isotc211.org/2005/gts\" xmlns:srv=\"http://www.isotc211.org/2005/srv\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.isotc211.org/2005/gmihttp://cdn.earthdata.nasa.gov/iso/schema/1.0/ISO19115-2_EOS.xsd\">\n> …\n>\n> After this line https://github.com/pydap/pydap/blob/release/3.2.1/src/pydap/handlers/dap.py#L61, I have unescaped d.quotes in the python string.\n>\n> I’m now using the following regex, and that *seems* to do the trick: \".*?[^\\\\]\"[,;] but it could fail in some instances as well.\n>\n> Still testing…\n>\n> Matt\n>\n>\n> From: James Gallagher [mailto:jgall...@opendap.org]\n> Sent: Monday, March 19, 2018 3:51 PM\n> To: Matt Ueckermann\n> Cc: James Gallagher; Nathan Potter; sup...@opendap.org\n> Subject: Re: [support] PyDAP Access broken after HyRAX upgrade\n>\n>\n>\n>\n> On Mar 19, 2018, at 12:26, Matt Ueckermann <m...@creare.com> wrote:\n>\n> Hi Nathan,\n>\n> I should have said that I *think* I can fix it... I first have to figure out how to fix it.\n>\n> Is there spec for the .das file that I can refer to?\n>\n> That’s on www.opendap.org under support.\n>\n> To parse the DAS, for String attributes, just gobble up chars untl you find the matching double quote. Since internal d.quotes are escaped, that should be pretty simple.\n>\n> James\n>\n>\n>\n> Thanks,\n> Matt\n>\n>\n> -----Original Message-----\n> From: Nathan Potter [mailto:n...@opendap.org]\n> Sent: Monday, March 19, 2018 1:59 PM\n> To: Matt Ueckermann\n> Cc: Nathan Potter; James Gallagher; sup...@opendap.org\n> Subject: Re: [support] PyDAP Access broken after HyRAX upgrade\n>\n> Matt,\n>\n> If you “fork” the PyDAP project on GitHub you can make a local fix, push it to your GitHub account and then issue a pull request for your changes. That might make it happen much faster.\n>\n>\n> Cheers,\n>\n> Nathan\n>\n>\n>\n> On Mar 19, 2018, at 10:52 AM, Matt Ueckermann <m...@creare.com> wrote:\n>\n> Hi James,\n>\n> Thanks for the response. This is very helpful. Seems like the new behavior is the desired behavior, but unfortunately breaks PyDAP.\n>\n> Based on your response below, I can fix PyDAP locally, but this really should get into the main repository. If you could ping the PyDAP developers for me, that would be very helpful. I made an issue a month ago but haven’t gotten any response.\n>\n> Thanks,\n> Matt\n>\n> From: James Gallagher [mailto:jgall...@opendap.org]\n- show quoted text -\n> From: National Snow & Ice Data Center [mailto:ns...@nsidc.org]\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Matt Ueckermann",
     "body": "Well... that would be helpful. The test server is very helpful. I'll point this out to the NSIDC folks and see what they think.\n-----Original Message-----\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Monday, March 19, 2018 4:17 PM\nTo: Matt Ueckermann\nCc: Nathan Potter; James Gallagher; sup...@opendap.org\nSubject: Re: [support] PyDAP Access broken after HyRAX upgrade\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Nathan Potter",
     "body": "Hi Matt,\nAccording to Kent the difference is likely attributable to a configuration discrepancy. The server at test.opendap.org is using EnableCF=true while NSIDC’s server is not.\nKent said he would reach out to them so maybe they’ll change it.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Nathan Potter",
     "body": "Hi Matt,\nI got in touch with the NSIDC admin and he changed the configuration so now all of those pesky double quotes are escaped:\nhttps://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5.das\nHopefully this makes the next steps easier!\nLet me know if there are more issues.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Nathan Potter",
     "body": "Hi Kent,\n(I am including Mark and Matt because they are stakeholders in this issue.)\nYour explanation helped a lot.\nI went to test.opendap.org and configured it with:\nH5.EnableCF=true\nH5.EnableDropLongString=false\nhttp://test.opendap.org/opendap/NSIDC/SMAP_L3_SM_P_20150406_R14010_001.h5.das\nThis exposed BOTH the top level attributes:\niso_19139_dataset_xml\niso_19139_series_xml\nAnd while now all of the double quotes are correctly escaped the greater-than-sign character is encoded differently in the two attributes:\niso_19139_dataset_xml:\n<gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\niso_19139_series_xml:\n<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nAnd I think that Ken is correct, the iso_19139_dataset_xml encoding is borking the PyDAP DAS parser.\nSo if NSIDC sees value in publishing the iso_19139_dataset_xml attribute then something needs to change, either the PyDAP DAS parser or the source file content. I think one can make a case for both changes:\n- The DAS parser should be lenient w.r.t String valued attribute content and pretty much just slurp up everything between the un-escaped double quotes.\n- The source data file should NOT be double encoding the embedded XML document content (&gt; is correct, &amp;gt; is not correct)\nDoes this make sense to everyone? Matt, Mark, & James may want to read the thread below to catch up, but I think this summary has all of the salient points.\nSorry for all the confusion, but I think this is a better statement of the issue than previously.\nSincerely,\nNathan\n> On Mar 20, 2018, at 7:01 AM, Kent Yang <mya...@hdfgroup.org> wrote:\n>\n> Hi Nathan,\n>\n> Don't blame yourself. It is easy to get confused. This is the issue from this SMAP file. See if my explanation makes sense to you. If not, we may set up a phone conversation to get this clear. My phone is 217-531-6107. I am at my office 9 am to 12pm and 1 pm to 5 pm Central time.\n>\n> There are two attributes in this file:\n> iso_19139_dataset_xml\n> iso_19139_series_xml\n>\n> Inside iso_19139_series_xml, you have\n> <gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n> Inside iso_19139_dataset_xml, you have\n> <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n> Notice the &amp above: I think that causes an issue with pydap.\n>\n> When EnableCF=false, the handler tries to match everything in this SMAP to DAP2 regardless of any Conventions users preferred. So both attributes are mapped to DAP2.\n> When EnableCF=true, the handler will customize the mapping to follow the CF and please popular client(netCDFjava etc.). So for this version, iso_19139_dataset_xml is ignored.\n>\n> What you see that the attribute is changed to a different name is actually the attribute iso_19139_series_xml. The attribute name is not changed. This is just another attribute. The attribute that causes pydap choken is simply ignored.\n>\n> This can be verified from the attached ncdump header.\n>\n> As for the issue pydap faces, I think the user should contact NSIDC to report the issue inside the HDF5 file itself rather than asking us to hack the DAP2 or the handler to make his problem go away.\n>\n> Kent\n> -----Original Message-----\n> From: Nathan Potter [mailto:n...@opendap.org]\n> Sent: Monday, March 19, 2018 8:59 PM\n> To: Kent Yang\n> Cc: n...@opendap.org\n> Subject: Re: [support] PyDAP Access broken after HyRAX upgrade\n>\n>\n>\n>> On Mar 19, 2018, at 6:19 PM, Kent Yang <mya...@hdfgroup.org> wrote:\n>>\n>> Nathan.\n>>\n>> I fully understand what you said. However, I don't see\n>> iso_19139_dataset_xml attribute in this https://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5.das .\n>>\n>> That's why I don't think he made H5.EnableDropLongString=false in his configuration. So we cannot be sure whether the issue is resolved.\n>>\n>> Am I checking the correct URL?\n>\n> You are, and to be honest I am pretty damn confused. Basically when the server was EnableCF=false we saw this messed up attribute value for “iso_19139_dataset_xml” which was an XML document with incorrectly escaped double quotes.\n>\n> Now, with EnableCF=true, the XML document appears to be associated with an attribute of a different name, and it’s double quotes are correctly escaped.\n>\n> You can download the dataset from the NSIDC site or from test.opendap.org:\n>\n> http://test.opendap.org/opendap/NSIDC/SMAP_L3_SM_P_20150406_R14010_001.h5\n>\n> Sorry, I just don’t understand.\n>\n> Nathan\n>\n>\n>>\n>> Kent\n>>\n>>\n>> On Mar 19, 2018, at 5:53 PM, Nathan Potter <n...@opendap.org> wrote:\n>>\n>>> Kent,\n>>>\n>>> I think we are talking about two different issues.\n>>>\n>>> When he changed the config to: H5.EnableCF=true then his output, here:\n>>>\n>>> https://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SM\n>>> AP_L3_SM_P_20150406_R14010_001.h5.das\n>>>\n>>> Looks correct in that the double quote characters in the “iso_19139_dataset_xml” attribute are now correctly escaped.\n>>>\n>>> Which is what I was trying to understand. And the attribute is still in the dataset DAS so I am thinking the he already has H5.EnableDropLongString=false in his config.\n>>>\n>>> Does that make sense?\n>>>\n>>>\n>>> Nathan\n>>>\n>>>\n>>>> On Mar 19, 2018, at 2:34 PM, Kent Yang <mya...@hdfgroup.org> wrote:\n>>>>\n>>>> Nathan,\n>>>>\n>>>> Actually this is not quite right.\n>>>> The NSIDC should change\n>>>> H5.EnableDropLongString=true\n>>>> to\n>>>> H5.EnableDropLongString=false\n>>>>\n>>>> The current das output ignores attribute iso_19139_dataset_xml. The disappearance of double quote is because the attribute is turned off.\n>>>> As in the email I forwarded to you, if there are any issues in the double quotes in the CF option, we can check that. In my last investigation, the issue comes from the HDF5 itself, not in the Hyrax layer.\n>>>> I may be wrong but I need to be fed more information. Do you guys have a ticket related to this?\n>>>>\n>>>> Kent\n>>>>\n>>>>\n>>>>\n>>>>\n>>>> -----Original Message-----\n>>>> From: Nathan Potter [mailto:n...@opendap.org]\n>>>> Sent: Monday, March 19, 2018 4:22 PM\n>>>> To: Matt Ueckermann\n>>>> Cc: n...@opendap.org; James Gallagher; sup...@opendap.org; Kent Yang\n>>>> Subject: Re: [support] PyDAP Access broken after HyRAX upgrade\n>>>>\n>>>>\n>>>> Hi Matt,\n>>>>\n- show quoted text -\n>>>>>>> 6/S MA P_L3_SM_P_20150406_R14010_001.h5.das\n>>>>>>>\n>>>>>>> A small excerpt:\n>>>>>>> String iso_19139_dataset_xml \"<?xml version=\"1.0\" encoding=\"UTF-8\"\n>>>>>>> standalone=\"no\" ?> <gmd:DS_Series\n>>>>>>> xmlns:gmd=\"http://www.isotc211.org/2005/gmd\"\n>>>>>>> xmlns=\"http://www.isotc211.org/2005/gmi\"\n>>>>>>> xmlns:eos=\"http://earthdata.nasa.gov/schema/eos\"\n>>>>>>> xmlns:gco=\"http://www.isotc211.org/2005/gco\"\n>>>>>>> xmlns:gmi=\"http://www.isotc211.org/2005/gmi\"\n>>>>>>> xmlns:gml=\"http://www.opengis.net/gml/3.2\"\n>>>>>>> xmlns:gmx=\"http://www.isotc211.org/2005/gmx\"\n>>>>>>> xmlns:gsr=\"http://www.isotc211.org/2005/gsr\"\n>>>>>>> xmlns:gss=\"http://www.isotc211.org/2005/gss\"\n>>>>>>> xmlns:gts=\"http://www.isotc211.org/2005/gts\"\n>>>>>>> xmlns:srv=\"http://www.isotc211.org/2005/srv\"\n>>>>>>> xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n>>>>>>> xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\n>>>>>>> xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n>>>>>>> xsi:schemaLocation=\"http://www.isotc211.org/2005/gmihttp://cdn.ea\n>>>>>>> rth da ta.nasa.gov/iso/schema/1.0/ISO19115-2_EOS.xsd\">\n- show quoted text -\n>>>>>>> === == ===============================================\n- show quoted text -\n>>>>>>> === == ===============================================\n- show quoted text -\n> <attr-ndr>\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Kent Yang",
     "body": "Nathan,\nI agree with your statement.\nOne add-on for this thread. Because of this issue, I improved the handler to support the long string attribute like this iso_19139_dataset.xml in the CF option. This is because the current netCDF java can accept this case rather than throwing an error. So In the next Hyrax release, NSIDC folks won't see the drop of long string in their SMAP service. This may easily make the issue related to the SMAP file itself out of the surface.\nKent\n-----Original Message-----\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Tuesday, March 20, 2018 10:06 AM\nTo: Kent Yang; Matt Ueckermann; Mark Schwab\nCc: n...@opendap.org; User Support; James Gallagher\nSubject: Re: [support] PyDAP Access broken after HyRAX upgrade\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Nathan Potter",
     "body": "Matt,\nSo I heard from Mark @NSIDC and now there server is correctly escaping the double quote, but the change in content to replace the offending &amp;gt; with &gt; is only something he can suggest to the team that encodes the metadata.\nhttps://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5.das\nI think the upshot is that James' suggestion to make the PyDAP DAS parser smarter is the right thing to do, and now that double quotes are correctly escaped within the string valued attributes it should be possible.\nWe can help communicating with the PyDAP team if needed.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Matt Ueckermann",
     "body": "HI Nathan and Mark,\nThanks for all your help. PyDAP's DAS parser has no problem now that the double quotes are escaped, so I'm all set.\nThanks again for getting this sorted out, much appreciated,\nMatt\n-----Original Message-----\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Tuesday, March 20, 2018 11:48 AM\nTo: Matt Ueckermann\n- show quoted text -"
    },
    {
     "page": "PyDAP Access broken after HyRAX upgrade",
     "name": "Mark Schwab",
     "body": "I'm happy we got it sorted out. (Thanks mostly to Nathan and the\nOpenDAPers.)\nCheers,\nMark\n__o\n_-\\<,\n(_)/(_)___________________________________________________\nMark J. Schwab\nSr System Administator\nRaytheon Systems\nNational Snow and Ice Data Center\nUniversity of Colorado\nCampus Box 449 / 1540 30th St\nBoulder, C0 80309\nPh: (303) 735-3061 Fax: (303) 492-2468\n----------------------------------------------------------\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Ask",
     "name": "FREDDY HERNANDEZ VACA",
     "body": "Dear\nI have error message\nwith matlab\norg.apache.commons.httpclient.auth.InvalidCredentialsException: HTTP/1.1 401 Unauthorized\nat opendap.dap.DConnect2.openConnection(DConnect2.java:267)\nat opendap.dap.DConnect2.getData(DConnect2.java:826)\nat opendap.dap.DConnect2.getData(DConnect2.java:1116)\nat ucar.nc2.dods.DODSNetcdfFile.readDataDDSfromServer(DODSNetcdfFile.java:1485)\nat ucar.nc2.dods.DODSNetcdfFile.readArrays(DODSNetcdfFile.java:1542)\nat ucar.nc2.dods.DODSNetcdfFile.<init>(DODSNetcdfFile.java:370)\nat ucar.nc2.dods.DODSNetcdfFile.<init>(DODSNetcdfFile.java:202)\nError using nc_varget_java (line 31)\nCould not open 'https://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_RT/TRMM_3B42RT_Daily.7/2018/03/3B42RT_Daily.20180311.7.nc4' with java backend.\nError in nc_vargetr (line 40)\n[data,info] = nc_varget_java(ncfile,varname,varargin{:});\nError in nc_varget (line 35)\n[data,info] = nc_vargetr(ncfile,varname,varargin{:});\nIs it normal?\nRegards\n--\nFreddy Hernández Vaca\nDivisión El Niño y Clima Oceánico\nDirección Oceanografía Naval\nInstituto Oceanográfico de la Armada\nGuayaquil-Ecuador\n+5930995615644"
    },
    {
     "page": "Ask",
     "name": "Nathan Potter",
     "body": "Hi Freddy,\nThis is in fact a “normal” error.\nYou are attempting to access a dataset that requires you to be authenticated in order to get data access (thus the 401 response)\nAll NASA datasets require data access requests to be authenticated using their Earthdata Login system.\nYou can read how to enable Matlab to do this here:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_authentication_for_dap_clients\nSections \"6.3.2. Earthdata Login Users\" and \"6.3.9 Matlab, Ferret, Other applications that use NetCDF C” should help you get it sorted out.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Data set access",
     "name": "Ali Ibrahim",
     "body": "Dear,\nI can't access to the data set? can you help me please.\nThanks\n--\nAli K. Ibrahim\nEE Graduated Student\nFlorida Atlantic University (FAU)\n626 NW 13thh st Apt# 11\nBoca Raton , Florida, 33486\nMobile : +15017659435"
    },
    {
     "page": "Data set access",
     "name": "Nathan Potter",
     "body": "Hi Ali,\nI need you to send me the URL of the dataset you are trying to access before I can help.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Calculation",
     "name": "Amit",
     "body": "Dear Sir\nIs it possible to get the dataset in standard .csv or json format? We were unable to extract the data in its existing format.\nPlease also share us any documentation on how to calculate the resultant ocean current and direction from the raw data.\nWith kind regards\nAmitava talukdar\nViswalab,"
    },
    {
     "page": "Calculation",
     "name": "Nathan Potter",
     "body": "Hi Amitava,\nIt may be possible to retrieve the data in an alternate encoding, but that feature is highly dependent on the particular server with which you are working. If you send me the URL of the server (or preferably the dataset that you are trying to access) then I may be able to provide additional insight.\nWith regards to your request computational documentation I’ll see what I can help you find, or at least try to direct you to the right person, but again I will need the URL to the dataset in question.\nSincerely,\nNahan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Calculation",
     "name": "Amit",
     "body": "Dear Nathan\nThe dataset we are trying to access is\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/\nThe documentation contains explanation of four parameters u, v, um and vm and we are trying to compute\nThe resultant sea current at specific lat/long on specific utc time and its resultant direction.\nWe are quite new to this and it’s an ocean of information where we are sort of drowning. Any help will be much appreciated.\nWith kind regards\namitava\n- show quoted text -"
    },
    {
     "page": "Calculation",
     "name": "Nathan Potter",
     "body": "Hi Amitava,\nThat server will return an ASCII response (which is not exactly CSV - but it’s close) and a JSON response. These are not very efficient transfer encodings and the individual variables u, v, um, and vm each have 41 million values, not counting their associated map vectors. I mention this because it suggests that unless your area of interest is the entire globe you will want to subset the data to just your area of interest. Subsetting is accomplished using the array indices of the variables and NOT by using the associated values. If you need help with that I can assist.\nFor now lets look at the encodings and computational issues.\nYou can see the overall structure of the data set by examining the DDS response, note the suffix I have added to the dataset URL:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel1993.nc.gz.dds\nTo keep the results manageable I will request just the latitude array.\nHere is a request for the latitude array as an ASCII response. Please, note the change of suffix and the addition of a query string that names the requested variable, latitude:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel1993.nc.gz.ascii?latitude\nTo get the same data as JSON simply change the suffix to “.json\":\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel1993.nc.gz.json?latitude\nWith regards to the computational question: There are metadata embedded in the dataset that provide a number of insights into the variables and the dataset in general. You can view the complete metadata response by changing the URL suffix to “.das”:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel1993.nc.gz.das\nThere you can see the units of each variable and also find contact information for the people that generated the dataset. I suggest that you contact them with your computational questions (We at OPeNDAP only provide the data access service software and we do not produce the data served.)\nHowever your basic question: “How to calculate the resultant ocean current and direction from the raw data?” Would seem to me to be a matter of combining the values of u and v to produce a current vector with direction and magnitude at each physical location. If you need more guidance you should definitely write the authors referenced in the dataset metadata.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Calculation",
     "name": "Nathan Potter",
     "body": "Hi Jay,\nFirst please understand that OPeNDAP is in no way involved with the production or analysis of the data. We provide the server software. Your questions about calculations should be directed to the publisher/authors listed in the metadata.\nYou might consider reading the documentation provided on the site. In the catalog directory for the data you are working with:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/\nThere is a “docs” directory link:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/docs/\nWhich contains a PDF file that you should probably read as it discusses, in detail, the file naming conventions, the data organization, and the calculations used in the production of the data.\nI will provide other answers inline below.\n> On Mar 9, 2018, at 9:55 AM, JAYARAMAN IYER <jayar...@yahoo.com> wrote:\n>\n> Hi Nathan,\n>\n> Thanks for providing the details to Amitava, who had forwarded the same to me.\n>\n> I am looking for Ocean Surface currents as at specific locations as on a specific time\n> (these will correspond to location of a few ships for which we will be analyzing using this data.)\n>\n> I am not much aware of how this data is organized. Will be helpful to us if you could further clarify on these or point us the direction.\n>\n> 1) The dataset url used was https://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel9284.nc.gz.json\n>\n>\n> I understand that the data corresponds to \"Mar.07,2018\".\n>\n> How will the url be for getting data as on any other date of 2017 or 2018? Can we get the url based on the date?\nRead the documentation I referenced above!\nYou have to figure that out from the catalog.\n- Note that the units of time (in the metadata) are \"day since 1992-10-05 00:00:00”\n- Note that each dataset in the catalog https://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg appears to have single time point (I have not checked them all)\n- Note that the dataset you referenced at https://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel9284.nc.gz has a single time value of 9284, which is the number of days since October 5th 1992 and that value appears in the file name\nI believe that this is enough information for you to sort it out from there.\n>\n> 2) Are there consolidated datasets available organized with full year data?\nWell, there appear to be some 1 degree data with annual files here:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/hyrax/allData/oscar/L4/oscar_1_deg/contents.html\nAlong with documentation and software for IDL and MatLab to assist in working with the data.\n>\n> 3) Is it possible to get the data for a specific location?\n>\n> I tried filtering by specifying the same in url like:\n> https://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel9284.nc.gz.json?Latitude=21&Longitude=21\n>\n> That did not return results but gave error: No such identifier in dataset:\nThe response was correct, you made several mistakes.\n- The variable names are case sensitive. It is not Latitude, but latitude. It is not Longitude, but longitude. The capitalization and spelling must match what you see in the DDS response precisely: https://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel9284.nc.gz.dds\n- You CANNOT subset using values. You MUST subset using the array index of the thing you want.\n- The subset syntax you used is not supported.\nThis URL:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/oscar/preview/L4/oscar_third_deg/oscar_vel9284.nc.gz.ascii?latitude[21],longitude[21]\nReturns the 22nd value of the latitude array and the 22nd value of the longitude array (array indexes begin at zero)\nIf you have an area of interest you can determine the indices by examining the map vectors (there are 4 in this dataset but time and depth have only a single index so they are trivial) and then determining the index bounds of the area you wish to retrieve. Alternately, since in this dataset the latitude and longitude vectors appear to be regularly space, you could compute the index from a value by making a simple math problem. Please notice that the metadata says clearly:\n\"Longitude extends from 20 E to 420 E to avoid a break in major ocean basins. Data repeats in overlap region.\"\nSo be careful with that.\nIn the end you will have a constraint that resembles this:\n?u[0][0][220:1:260][881:1:961]\nWhere 220 is the index of the maximum latitude value and 260 is the index of the minimum latitude value (Note that latitude is decreasing)\nand 881 is the index of the minimum longitude value and 961 is the index of the maximum longitude value.\nObviously these indices won’t work for you unless I have some how read your mind, but hopefully this illustrates the idea.\nYou can ask for all 4 variables like this (using the same constraint):\ndatasetURL.json?u[0][0][220:1:260][881:1:961],v[0][0][220:1:260][881:1:961],um[0][0][220:1:260][881:1:961],vm[0][0][220:1:260][881:1:961]\nOr one at a time:\ndatasetURL.json?u[0][0][220:1:260][881:1:961]\ndatasetURL.json?v[0][0][220:1:260][881:1:961]\ndatasetURL.json?um[0][0][220:1:260][881:1:961]\ndatasetURL.json?vm[0][0][220:1:260][881:1:961]\n>\n> 4) Using the four values of a point u, v, um, vm:\n>\n> We compute the resultant vector magnitude (in m/s) as sqrt(u ^2 + v ^2) and direction (in degrees) as atan(vm / um)\n>\n> Are these calculations correct? (I used this based on general understanding of vector calculations)\nI would be guessing. Read the documentation I referenced above...\n>\n> 5) Are there any premium subscriptions available with you which can get us filtered data?\n>\n> Will be very helpful if you could help on the above.\nWhat do you mean “filtered data”?\nThis data is published by NASA’s Jet Propulsion Laboratory and as far as I know there are no “premium subscriptions”\nCheers,\nNathan\n>\n>\n> Thanks & regards,\n> Jay\n>\n>\n> ----- Forwarded message -----\n> From: amit <am...@viswalab.com>\n> To: \"jayar...@yahoo.com\" <jayar...@yahoo.com>\n> Sent: Friday, 9 March, 2018, 8:19:22 PM IST\n> Subject: Fwd: Re: [support] Calculation\n>\n>\n>\n>\n>\n> Sent from my Samsung Galaxy smartphone.\n>\n> -------- Original message --------\n> From: Nathan Potter <n...@opendap.org>\n> Date: 09/03/2018 22:20 (GMT+08:00)\n> To: Amit <am...@viswalab.com>\n- show quoted text -"
    },
    {
     "page": "Calculation",
     "name": "Nathan Potter",
     "body": "Hi Jay,\nHere’s a more detailed answer I provided to another user recently. You’ll need to adjust the details to fit your target data circumstance, but I think it’s a relevant template for determining the indices of your subset:\n> The server software only subsets by array index, not by value.\n>\n> You have to do a little work to associate the value range that you desire with the appropriate indices.\n>\n> First, look at the sizes of the things in question. This URL will return a formatted description of the structure of the dataset:\n>\n> https://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.dds\n>\n> You can easily view the values of the map variables in ascii by selecting just the variable name and specifying the type of responses (ascii) by appending that to the URL like so:\n>\n> https://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?lat\n> https://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?lon\n> https://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?time\n>\n> From this you should be able to find the indices for each map variable that correspond to the desired value range.\n>\n> Since the values of lat and lon are regular you can calculate the indices from them.\n>\n> First express the value in terms of the index:\n>\n> lat_value = -1*(index - 900)/10;\n>\n> The -1 multiplier is because the values are decreasing, not increasing. The 900 is because that is the location of the lat value of 0, more or less.\n>\n> Solving for index:\n>\n> index = -1*lat_value*10 + 900;\n>\n> For the lower bound and upper bounds you want to push the index out by 1 index in the appropriate direction to make sure you get all the values that what you want:\n>\n> lat_index_min = -1*lat_value_min*10 + 900 + 1;\n> lat_index_max = -1*lat_value_min*10 + 900 - 1;\n>\n> For your stated area of interest in latitude:\n>\n> lat_index_min = -1*25*10 + 900 + 1;\n> lat_index_max = -1*40*10 + 900 - 1;\n>\n> lat_index_min = 651;\n> lat_index_max = 499;\n>\n> And the numbers are “reversed\" because the data is decreasing in value by index.\n>\n> So your constraint for latitude should be roughly:\n>\n> lat[499:1:651]\n>\n> You can test each set of indices by requesting just the map values in the index expression added to the map variable name:\n>\n> https://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?lat[499:1:651]\n>\n> You can do something quite similar for longitude, which I will leave as an exercise for you to complete.\n>\nHopefully this wil provide some additional clarity.\nSincerely,\nNathan\n> On Mar 9, 2018, at 4:22 PM, JAYARAMAN IYER <jayar...@yahoo.com> wrote:\n>\n> Hi Nathan,\n> Thanks for immediate response.\n>\n> regards,\n> Jay\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Precipitaion data downloading",
     "name": "Ali Fallah",
     "body": "Dear support’s team,\nI would like to download MSWEP V2.1 precipitation data with 0.1-degree resolution over Iran (Time: month (total data) Lat: 25-40 degree lon: 42-63 degree).\nI was wondering if you help me how it would be possible to download data in 0.1o resolution in NetCDF4? I’ve entered variables like this:\nTime: 0:1:455\nLat: 25:1:40\nLon: 42:1:63\nHowever, it seems, data downloads in 1-degree resolution!\nI have another problem! When I would like to download such a data for all of the earth and enter time, lat and lon by default, there is a Hyrax Error – Internal Error (500)!\nhttps://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.nc4?precipitation[0:1:455][0:1:1799][0:1:3599]\nI’m looking forward to your reply.\nRegards,\nAli Fallah\n​"
    },
    {
     "page": "Precipitaion data downloading",
     "name": "Nathan Potter",
     "body": "Hi Ali,\nThe server software only subsets by array index, not by value.\nYou have to do a little work to associate the value range that you desire with the appropriate indices.\nFirst, look at the sizes of the things in question. This URL will return a formatted description of the structure of the dataset:\nhttps://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.dds\nYou can easily view the values of the map variables in ascii by selecting just the variable name and specifying the type of responses (ascii) by appending that to the URL like so:\nhttps://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?lat\nhttps://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?lon\nhttps://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?time\nFrom this you should be able to find the indices for each map variable that correspond to the desired value range.\nSince the values of lat and lon are regular you can calculate the indices from them.\nFirst express the value in terms of the index:\nlat_value = -1*(index - 900)/10;\nThe -1 multiplier is because the values are decreasing, not increasing. The 900 is because that is the location of the lat value of 0, more or less.\nSolving for index:\nindex = -1*lat_value*10 + 900;\nFor the lower bound and upper bounds you want to push the index out by 1 index in the appropriate direction to make sure you get all the values that what you want:\nlat_index_min = -1*lat_value_min*10 + 900 + 1;\nlat_index_max = -1*lat_value_min*10 + 900 - 1;\nFor your stated area of interest in latitude:\nlat_index_min = -1*25*10 + 900 + 1;\nlat_index_max = -1*40*10 + 900 - 1;\nlat_index_min = 651;\nlat_index_max = 499;\nAnd the numbers are “reversed\" because the data is decreasing in value by index.\nSo your constraint for latitude should be roughly:\nlat[499:1:651]\nYou can test each set of indices by requesting just the map values in the index expression added to the map variable name:\nhttps://data.princetonclimate.com/opendap/MSWEP_V2.1/global_monthly_010deg.nc.ascii?lat[499:1:651]\nYou can do something quite similar for longitude, which I will leave as an exercise for you to complete.\nLet me know if you have questions.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "MSWEP Data Access",
     "name": "prince...@gmail.com",
     "body": "Dear Nathan Potter,\nThank you for registering. The MSWEP data are available for download via https://data.princetonclimate.com/opendap. Please use the following login information:\nUser ID: NathanPotter963\nPassword: 2YU27F0YKRDRc44\nPlease keep in mind that the accuracy of the dataset is significantly better from 2000 onwards. Furthermore, when comparing gridded precipitation datasets with daily gauge observations, it is recommended to compute daily accumulations for all gridded datasets for the 24-hour period ending at the specific reporting time of the gauge. This is essential to avoid temporal mismatches between time series based on the gridded datasets and the observations. The MSWEP V2 technical documentation (available via www.gloh2o.org) provides reporting times for the GHCN-D and GSOD gauges.\nPlease cite the following paper when using MSWEP in any publication:\nBeck, H.E., A.I.J.M. van Dijk, V. Levizzani, J. Schellekens, D.G. Miralles, B. Martens, A. de Roo: MSWEP: 3-hourly 0.25 global gridded precipitation (1979-2015) by merging gauge, satellite, and reanalysis data, Hydrology and Earth System Sciences, 21(1), 589-615, 2017.\nThe MSWEP V2 paper is currently in preparation. The methodology for V2 is described in the technical documentation (available via www.gloh2o.org). Note that MSWEP V1 and V2 and many other precipitation datasets have been evaluated in the following paper:\nBeck, H. E., Vergopolan, N., Pan, M., Levizzani, V., van Dijk, A. I. J. M., Weedon, G. P., Brocca, L., Pappenberger, F., Huffman, G. J., and Wood, E. F.: Global-scale evaluation of 22 precipitation datasets using gauge observations and hydrological modeling, Hydrol. Earth Syst. Sci., 21, 6201-6217, 2017.\nPlease let me know if you have questions.\nRegards,\nHylke Beck\nwww.gloh2o.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "ASCII file from GCM",
     "name": "Muhammad Saleem Malik",
     "body": "Dear Sir/ Madam\nI am working with CMIP5 data for my PhD project. I am trying to subset required GCM data using OpenDAP acess form. however, i need data in NetCDF formate but the form gives only options for GET ASCII or Get Binary.\nHow can i get data in NetCDF format for further processing?\nThanking you for inanticipation and best regards\nSaleem\nKeep Faith and Smiling\nMalik Muhammad Saleem\nPhD Research Scholar\nFaculty of Applied Computer Science\nInstitute of Geography\nDepartment of Physical Geography and Quantitative Methods\nUniversity of Augsburg 86159\nGermany.\nPhone (Cell) : +49-15217256385\nPhone (Off) : +49-821 598 2712\nFax (Off) : +49- 821 598-2264\nPhone (Home): +49- 8231 9786967"
    },
    {
     "page": "ASCII file from GCM",
     "name": "Nathan Potter",
     "body": "Hi Saleem,\nI thin the answer to your question is yes. but, to be sure, and to help you formulate the request I need the URL of the OPeNDAP Data Access for that you are working with.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "ASCII file from GCM",
     "name": "Nathan Potter",
     "body": "Hi Saleem,\nI think that if all you want to do is to download the netcdf file you can simply alter the URL like this:\nYour original DAP Access URL:\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc\nHTTP File Service URL:\nhttps://esgf.nci.org.au/thredds/fileServer/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc\nBut the file is large (more than 1GB) and you may wish to subset it. In order to do that you can use the DAP URL with a query string. Here is an example (that returns the structural/syntactic metadata and not the numeric values) of the dataset with a subset expression that asks for the 100th time slice of the Grid variable “zg”:\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc.dds?zg[99:1:99][0:1:16][0:1:143][0:1:191]\nTo get the data response back as a netcdf file you can use the NetCDF command line tools command “nccopy” like this (Please observe the minor but crucial change in the URL path where the “.dds” suffix has been removed from the previous URL):\nnccopy \"https://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc?zg[99:1:99][0:1:16][0:1:143][0:1:191]” localFileName.nc\nThe NetCDF command used here is part of the NetCDF library which you can get from: https://www.unidata.ucar.edu/software/netcdf/\nUsing the query string can greatly reduce the amount of data transferred by allowing you to subset the dataset to retrieve just the times and area of interest. The subsetting is accomplished using the array index values and NOT the actual data values. You can use the ascii data response to examine the Map variables to determine which array indices are appropriate for your area of interest:\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc.ascii?time\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc.ascii?lat\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc.ascii?lon\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc.ascii?plev\nNote that array index subsetting begins at “0” for the first element, see the query string in my example above.\nHopefully that will get you going!\nCheers,\nNathan\nOn Mar 5, 2018, at 4:47 AM, Muhammad Saleem Malik <salee...@gmail.com> wrote:\nDear Nathan\nThanks alot for your very prompt response. Here is link of data URL for your further investigations\nhttps://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc\nPlease let me know if this was what you excactly needed.\nWith best regards\nSaleem\nKeep Faith and Smiling\nMalik Muhammad Saleem\nPhD Research Scholar\nFaculty of Applied Computer Science\nInstitute of Geography\nDepartment of Physical Geography and Quantitative Methods\nUniversity of Augsburg 86159\nGermany.\nPhone (Cell) : +49-15217256385\nPhone (Off) : +49-821 598 2712\nFax (Off) : +49- 821 598-2264\nPhone (Home): +49- 8231 9786967\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "ASCII file from GCM",
     "name": "Nathan Potter",
     "body": "Hi Saleem,\nI should also add that your e-mail software will probably butcher the links I sent by truncating the active link part (but not the displayed text) at a “]” character. You will need to copy the entire URL - including the last “]” and paste it into a terminal or browser as needed. Don’t rely on “clicking” the links - they are unlikely to work correctly.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "ASCII file from GCM",
     "name": "Nathan Potter",
     "body": "Hi Saleem,\nYou have an extraneous period at the end of the URL path, just before the question mark. Remove the period and it should work.\nCheers,\nNathan\n> On Mar 6, 2018, at 1:25 AM, Muhammad Saleem Malik <salee...@gmail.com> wrote:\n>\n> Dear Nathan\n>\n> Trust you will be doing well. I tried to follow your instructions and outcome is below with error message also\n>\n> pomeemuh@alcc83:[...]/pomeemuh: nccopy \"https://esgf.nci.org.au/thredds/dodsC/cmip5/output1/CSIRO-BOM/ACCESS1-0/historical/mon/atmos/Amon/r1i1p1/v20120727/zg/zg_Amon_ACCESS1-0_historical_r1i1p1_190001-194912.nc.?zg[99:1:99][0:1:16][0:1:143][0:1:191]\" Oit.nc\n> NetCDF: I/O failure\n> Location: file nccopy.c; line 967\n>\n> I am using central Linux computing system of my university to apply nccopy command on one of sample data with subseting query. However, still not successful.\n>\n> I hope you can help a bit more here. I am sorry for being so much asking, but i need help here\n>\n> With best regards\n>\n> Saleem\n>\n>\n>\n> Keep Faith and Smiling\n>\n>\n>\n> Malik Muhammad Saleem\n>\n> PhD Research Scholar\n>\n> Faculty of Applied Computer Science\n> Institute of Geography\n> Department of Physical Geography and Quantitative Methods\n> University of Augsburg 86159\n> Germany.\n>\n> Phone (Cell) : +49-15217256385\n> Phone (Off) : +49-821 598 2712\n> Fax (Off) : +49- 821 598-2264\n> Phone (Home): +49- 8231 9786967\n>\n>\n>\n>\n>\n>\n>\n> On Tue, Mar 6, 2018 at 8:13 AM, Muhammad Saleem Malik <salee...@gmail.com> wrote:\n> Dear Natahn\n>\n> Thank you very much for your kind quick response. You was exactly right as i needed two things:, i) NetcdF file formate for subsequent CDO manipulations and ii) subsetiing of required temporal and spatial domains from CMIP5 data sets. I knew first part solution by using HTTP URL as you also rightly mentioned. The second requirement (subsetting) i will try to follow as per your suggestion as its my dire need.\n>\n> While basically i am water resources engineer and not really familir with different data management perspective and so is why a bit struggling. I am working on Windows 7 system and MAC. I hope netCDF package suggested by you will work for both of these systems. Just last querry, the command ncopy will work with CDO or R Studio or only on terminal?\n>\n> Once again many thanks for your nice guidance and wishing you best of the day.\n>\n> with best regards\n>\n> Saleem\n>\n>\n> Keep Faith and Smiling\n>\n>\n>\n> Malik Muhammad Saleem\n>\n> PhD Research Scholar\n>\n> Faculty of Applied Computer Science\n> Institute of Geography\n> Department of Physical Geography and Quantitative Methods\n> University of Augsburg 86159\n> Germany.\n>\n> Phone (Cell) : +49-15217256385\n> Phone (Off) : +49-821 598 2712\n> Fax (Off) : +49- 821 598-2264\n> Phone (Home): +49- 8231 9786967\n>\n>\n>\n>\n>\n>\n>\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Increase Your Web Visibility: opendap.org",
     "name": "Joel Walson",
     "body": "Hello opendap.org Team,\nHope you are doing well.\nWe went through your website and got to know few key points, which needs your attention on priority. This might seem irrelevant now; but believe me with recent updates from Google, we think you are losing out the most.\nYour website traffic trend analysis clearly states you are not competing well with others, because of some strategic flaws.\nHere are some points where your website needs immediate attention:\nLow online presence for major competitive keywords or, phrases.\nThe website is not responsive for major browsers and devices.\nTechnical errors that restrict your website from search engine Indexing.\nLack of theme based quality back links.\nSocial media updates are don’t have adequate followers.\nWhat we can do for you?\nOur certified Google Analytics professional understand all recent Algorithmic updates. We will prepare an advanced digital marketing campaign to generate maximum traffic and boost your search engine ranking.\nThis e-mail provides you with a glimpse of information. If you have any queries about our services then kindly contact us back for a free website audit report.\nInterested! Please get in touch with us on following co-ordinates.\nBest Regards,\nJoel Walson|Marketing Consultant\n--------------------------------------------------"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Hyrax Error",
     "name": "Max Campbell",
     "body": "Dear Sir/Madam,\nNot sure why this URL isn’t working https://opendap.jpl.nasa.gov/opendap/, thought I would mention it in case there is problem with the server.\nRegards,\nMax"
    },
    {
     "page": "Hyrax Error",
     "name": "jgallagher",
     "body": "On Feb 28, 2018, at 18:35, Max Campbell <m.d.ca...@uq.net.au> wrote:\nDear Sir/Madam,\nNot sure why this URL isn’t working https://opendap.jpl.nasa.gov/opendap/, thought I would mention it in case there is problem with the server.\nThanks. I think the JPL folks are looking at it.\nRegards,\nMax\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Downloading DEM from NOAA",
     "name": "Matt Skakun",
     "body": "Hello,\nI am wondering if you can assist me in downloading a DEM file for Central Oregon Coast.\nThank you,\nMatt Skakun\nSan Diego State University - Geological Sciences"
    }
   ]
  },
  {
   "name": [
    {
     "page": "PO.DAAC OPeNDAP down?",
     "name": "Jason Roberts",
     "body": "Hi PO.DAAC support,\nIt appears that the OPeNDAP server is down. When I go to https://opendap.jpl.nasa.gov/opendap/ I get the Hyrax Error “OUCH! Something Bad Happened On This Server.” message.\nThanks,\nJason"
    }
   ]
  },
  {
   "name": [
    {
     "page": "OPeNDAPâ„¢, Impressed by your Github Repositories",
     "name": "Abinaya",
     "body": "Hey OPeNDAPâ„¢,\nI’ve been scouting for Colorado, Montana, Oregon and Rhode Island best engineers for a while now, and no one has impressed me as much as you!\nYour experience and skills, particularly C++, make you a terrific fit.\nI’d love to tell you a bit more about the company, and why I think you would love it!\nCan we have a chat?\nThanks & Regards,\nAbinaya"
    }
   ]
  },
  {
   "name": [
    {
     "page": "libdap 3.19.1 test failures",
     "name": "Orion Poplawski"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: [OPENDAP/bes] Add fileout_covjson module to bes (#105)",
     "name": "jgallagher",
     "body": "On Feb 20, 2018, at 1:23 PM, Lewis John McGibbney <notifi...@github.com> wrote:\n@rrimer07, @hemphilc, @RiverHendriksen and I are working on implementing a CovJSON module for serving CovJSON responses through a Hyrax deployment.\nWe will be submitting a PR for a feature branch in the short term, it will link to this issue.\nFeature branch and PR: a good idea.\nRegarding the question about SUBDIRS, I think you are not building the code correctly. Because the BES is a large project that has evolved from a collection of 17 interrelated projects, it has some quirks to its build.\nFor the issue you are asking about, run ‘autoreconf -fiv’ _only_ in the top level (bes) directory. Don’t run it anywhere else (e.g., do not run it in bes/modules). I believe that will fix this problem. If you are having a hard time building the BES, let me know ASAP. Many people do because the BES build _looks_ like a regular build for github + autotool, but it has a few quirks and if those steps are skipped or done out of order - both easy to do because otherwise it’s a ‘standard’ auto tools build - they build will fail and you’ll be left with the usual (inscrutable) auto tools error messages.\nJames\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: [OPENDAP/bes] Add fileout_covjson module to bes (#105)",
     "name": "jgallagher",
     "body": "On Feb 20, 2018, at 2:23 PM, Lewis John McGibbney <notifi...@github.com> wrote:\nthanks @jgallagher59701 , see below\nautoreconf (GNU Autoconf) 2.69\nCopyright (C) 2012 Free Software Foundation, Inc.\nLicense GPLv3+/Autoconf: GNU GPL version 3 or later\n<http://gnu.org/licenses/gpl.html>, <http://gnu.org/licenses/exceptions.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nWritten by David J. MacKenzie and Akim Demaille.\nThen to build\nautoreconf -fiv\n...\nmodules/Makefile.am:9: warning: SUBDIRS multiply defined in condition TRUE ...\nmodules/Makefile.am:7: ... 'SUBDIRS' previously defined here\nautoreconf: automake failed with exit status: 1\nAh, yes. My mistake. I’m not sure how our CI builds worked at all.\nThanks for bringing this to our attention. Nathan fixed it.\nYou’ll see a bunch of warnings about subdir-objects - ignore those.\nThanks and sorry for the crossed wires on my part.\nJames\n—\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Double declaration in BES/Modules/Makefile.am",
     "name": "innerne...@gmail.com",
     "body": "Hello,\nAs a follow up to a previous post about implementation of a new BES module, my team has found an error when running autoreconf in the modules sub directory. It looks as if SUBDIRS is being declared twice which is causing the Makefile.am to fail. Does your team know why this variable is declared twice? I attached a screenshot of the file and double declaration.\nThanks,\nRiver Hendriksen"
    },
    {
     "page": "Double declaration in BES/Modules/Makefile.am",
     "name": "innerne...@gmail.com",
     "body": "UPDATE: Changing the second line to += instead of = allowed for compilation of autoreconf, however I do not know if this is supposed to be the correct method.\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "das file change between 1.13.1 and 1.13.5 (and maybe 1.14)",
     "name": "Mark Schwab",
     "body": "Hi,\nWe had a user report getting errors with the formatting\nof the .das metadata output from Opendap starting on\nFriday. So, I rolled back opendap on our TS1 server,\nand verified that the format change is directly related\nto the update I made Thursday night.\nThe DAS file outputs from the same file from the two versions:\nNew version of opendap DAS file produced by 1.13.5 looks like:\n<gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center\nDAAC</gco:CharacterString>\nold version of opendap (1.13.1) :\n<gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center\nDAAC</gco:CharacterString>\n--\nNotice the extra 'amp;' in the 1.13.5 output.\n--\nI talked with Nathan Potter about this a little since I had him\nlooking at something else with me, and he tested it against\nthe current master in your repo and it went away again.\nOther background...\nThe user is using PYDAP, a python opendap client and it breaks\nhis ability use this to get and manipulate the data, the paring\nin the program can't handle that.\nI think the user could download, remove that, and then use\nhis pydap script locally but, that seems like a nightmare.\nAnyway, if this was an intentional change, then perhaps a\nbug notice to the PYDAP developers is better, but if it's\nnot and they can fix it then that solves the issue too.\nThanks\nMark\n__o\n_-\\<,\n(_)/(_)___________________________________________________\nMark J. Schwab\nSr System Administator\nRaytheon Systems\nNational Snow and Ice Data Center\nUniversity of Colorado\nCampus Box 449 / 1540 30th St\nBoulder, C0 80309\nPh: (303) 735-3061 Fax: (303) 492-2468\n----------------------------------------------------------"
    },
    {
     "page": "das file change between 1.13.1 and 1.13.5 (and maybe 1.14)",
     "name": "Nathan Potter",
     "body": "Hi Mark,\nThanks for sending this in. I grabbed this HDF5 file:\nhttps://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5\nAnd put it up on our test server which is running Hyrax-1.14.0 here:\nhttp://test.opendap.org/opendap/NSIDC/contents.html\nAnd it looks like the behavior has reverted to Hyrax-1.13.1 w.r.t. this metadata representation issue.\nAlso, I used the “ncdump” command line utility on the file and the metadata that it printed out matched what you are getting from Hyrax-1.13.5 (interesting…)\nI am including Kent Yang (he is the keeper of the Hyrax HDF5 handler code) in the hopes that he can shed some light on this issue.\nCheers,\nNathan\nBegin forwarded message:\nFrom: Mark Schwab <sch...@nsidc.org>\nSubject: [support] das file change between 1.13.1 and 1.13.5 (and maybe 1.14)\nDate: February 13, 2018 at 2:34:32 PM PST\nTo: sup...@opendap.org\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "das file change between 1.13.1 and 1.13.5 (and maybe 1.14)",
     "name": "Kent Yang",
     "body": "Hi Nathan,Mark,\nI haven�t changed the handling of DAS in the HDF5 handler for years. After checking the original HDF5 file, I think I find the reasons. That can also explain Nathan's ncdump output.\n1)\nIn the original HDF5 file, there are two <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString> strings.\nOne is <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nAnother one is <gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\nI�ve attached the ncdump output. I also checked h5dump and HDF5View. It both gave me the same results.\nInside attribute iso_19139_dataset_xml, you have <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n2) The HDF5 handler with the current configuration will ignore iso_19139_dataset_xml due to its length. So <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString> doesn't show up in the DAS output.\nMore details:\nIn the default HDF5 handler configuration:\nCheck file h5.conf,\nThere is a key H5.EnableDropLongString=true\nThe description of this key states:\n# We find netCDF java has a string size limit(currently 32767). If an HDF5 string size\n# is greater than this limit, visualization tools(Panoply etc.) that depend on\n# the netCDF Java may not open the HDF5 file. So this key is set to be true to\n# skip the HDF5 string of which size is greater than 32767. Users should set this\n# key to false if that long string information is necessary or visualization clients\n# are not used.\nThe size of iso_19139_dataset_xml is 182437, which is greater than 32767. So this long string is dropped.\nTo show this string, change H5.EnableDropLongString=false and restart your bes. You should see both <gco:CharacterString>..&amp; gt and &gt... at your DAS.\nBTW, I can use Panoply to check this SMAP product with this long string on although for other products(HIRDLS), Panoply still fails. Depending on the priority, I can change this\nH5.EnableDropLongString=true\nto\nH5.EnableDropLongString=false\nin the next release.\nBest,\nKent\nFrom: Nathan Potter [mailt...@opendap.org]\nSent: Wednesday, February 14, 2018 11:55 AM\nTo: Mark Schwab; Kent Yang\nCc: n...@opendap.org; User Support\nSubject: Fwd: [support] das file change between 1.13.1 and 1.13.5 (and maybe 1.14)\ncp\nHi Mark,\nThanks for sending this in. I grabbed this HDF5 file:\nhttps://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5\nAnd put it up on our test server which is running Hyrax-1.14.0 here:\nhttp://test.opendap.org/opendap/NSIDC/contents.html\nAnd it looks like the behavior has reverted to Hyrax-1.13.1 w.r.t. this metadata representation issue.\nAlso, I used the �ncdump� command line utility on the file and the metadata that it printed out matched what you are getting from Hyrax-1.13.5 (interesting�)\nI am including Kent Yang (he is the keeper of the Hyrax HDF5 handler code) in the hopes that he can shed some light on this issue.\nCheers,\nNathan\nBegin forwarded message:\nFrom: Mark Schwab <schwabm@nsidc.org>\nSubject: [support] das file change between 1.13.1 and 1.13.5 (and maybe 1.14)\nDate: February 13, 2018 at 2:34:32 PM PST\nTo: support@opendap.org\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "das file change between 1.13.1 and 1.13.5 (and maybe 1.14)",
     "name": "Mark Schwab",
     "body": "Hi,\nThis seems like an easy thing to try . Thanks Kent.\n-Mark\nOn Thu, 15 Feb 2018, Kent Yang wrote:\n>\n> Hi Nathan,Mark,\n>\n>\n>\n> I haven???t changed the handling of DAS in the HDF5 handler for years. After checking the original HDF5 file, I think I find the reasons. That can also explain Nathan's ncdump output.\n>\n>\n>\n> 1)\n>\n> In the original HDF5 file, there are two <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString> strings.\n>\n> One is <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n>\n>\n> Another one is <gco:CharacterString>NSIDC DAAC &gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n> I???ve attached the ncdump output. I also checked h5dump and HDF5View. It both gave me the same results.\n>\n>\n>\n> Inside attribute iso_19139_dataset_xml, you have <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center DAAC</gco:CharacterString>\n>\n>\n>\n> 2) The HDF5 handler with the current configuration will ignore iso_19139_dataset_xml due to its length. So <gco:CharacterString>NSIDC DAAC &amp;gt; National Snow and Ice Data Center\n> DAAC</gco:CharacterString> doesn't show up in the DAS output.\n>\n> More details:\n>\n> In the default HDF5 handler configuration:\n> Check file h5.conf,\n> There is a key H5.EnableDropLongString=true\n> The description of this key states:\n>\n> # We find netCDF java has a string size limit(currently 32767). If an HDF5 string size\n> # is greater than this limit, visualization tools(Panoply etc.) that depend on\n> # the netCDF Java may not open the HDF5 file. So this key is set to be true to\n> # skip the HDF5 string of which size is greater than 32767. Users should set this\n> # key to false if that long string information is necessary or visualization clients\n> # are not used.\n>\n>\n> The size of iso_19139_dataset_xml is 182437, which is greater than 32767. So this long string is dropped.\n>\n> To show this string, change H5.EnableDropLongString=false and restart your bes. You should see both <gco:CharacterString>..&amp; gt and &gt... at your DAS.\n>\n> BTW, I can use Panoply to check this SMAP product with this long string on although for other products(HIRDLS), Panoply still fails. Depending on the priority, I can change this\n>\n> H5.EnableDropLongString=true\n>\n> to\n> H5.EnableDropLongString=false\n>\n> in the next release.\n>\n> Best,\n>\n> Kent\n>\n>\n>\n> From: Nathan Potter [mailto:n...@opendap.org]\n> Sent: Wednesday, February 14, 2018 11:55 AM\n> To: Mark Schwab; Kent Yang\n> Cc: n...@opendap.org; User Support\n> Subject: Fwd: [support] das file change between 1.13.1 and 1.13.5 (and maybe 1.14)\n>\n>\n>\n> cp\n>\n> Hi Mark,\n>\n>\n>\n> Thanks for sending this in. I grabbed this HDF5 file:\n>\n>\n>\n> https://n5eil01u.ecs.nsidc.org/opendap/SMAP/SPL3SMP.004/2015.04.06/SMAP_L3_SM_P_20150406_R14010_001.h5\n>\n>\n>\n> And put it up on our test server which is running Hyrax-1.14.0 here:\n>\n>\n>\n> http://test.opendap.org/opendap/NSIDC/contents.html\n>\n>\n>\n> And it looks like the behavior has reverted to Hyrax-1.13.1 w.r.t. this metadata representation issue.\n>\n>\n>\n> Also, I used the ???ncdump??? command line utility on the file and the metadata that it printed out matched what you are getting from Hyrax-1.13.5 (interesting???)\n>\n>\n>\n>\n>\n> I am including Kent Yang (he is the keeper of the Hyrax HDF5 handler code) in the hopes that he can shed some light on this issue.\n>\n>\n>\n>\n>\n> Cheers,\n>\n>\n>\n> Nathan\n>\n>\n>\n>\n>\n>\n>\n> Begin forwarded message:\n>\n>\n>\n> From: Mark Schwab <sch...@nsidc.org>\n>\n> Subject: [support] das file change between 1.13.1 and 1.13.5 (and maybe 1.14)\n>\n> Date: February 13, 2018 at 2:34:32 PM PST\n>\n> To: sup...@opendap.org\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Docker Love: 20% off DockerCon this week only",
     "name": "DockerCon",
     "body": "Save $275 when you use code DOCKERLOVE by February 19th\nFrom beginner to black belt, DockerCon brings together the brightest minds to talk about all things containers and container management including networking, storage, security, kubernetes, creating effective images, machine learning, modernizing applications and more.\nDockerCon San Francisco 2018 will take place June 12 - 15 and feature over 120 sessions, free workshops, more learning and networking experiences and the amazing energy and vibe that results from 6,000 Docker users in one place sharing what we love.\nRegister & Save\nFeatured DockerCon Speakers\nTim Hockin,\nGoogle\nKubernetes Extensibility\nKinnary Jangla,\nPinterest\nAccelerating Development Velocity of Production ML Systems with Docker\nPeter Ngai,\nGE Digital\nPackaging Software for the Edge with Docker and Windows Server\nSanjoy Mukherjee,\nJCPenney\nHow JCPenney Handles Black Friday and 100K Deployments Per Year\nAbby Fuller,\nAWS\nCreating Effective Docker Images\nLiz Rice,\nAqua\nDon't Have a Meltdown! Practical Steps for Defending Your Apps\nRegister by Monday, Feb 19th with the code 'DOCKERLOVE' and get $275 off your DockerCon ticket and experience all that DockerCon has to offer!\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "BES Create Module Error",
     "name": "hend...@oregonstate.edu",
     "body": "Hello,\nI'm working with a team to create a new BES response handler module, from what we have read the best method for doing this is using BES ./bescreatemodule located in the hyrax/bes/bin directory. However when compiling it errors out stating that they are specific files in the template directory which do not exist. We have looked at the current build on github and it does not contain those templates either. Is this method for creating a module defunct, and if so what is the best method for creating a new module in BES?\nThank you,\nRiver"
    },
    {
     "page": "BES Create Module Error",
     "name": "jgallagher",
     "body": "On Feb 12, 2018, at 21:23, hend...@oregonstate.edu wrote:\nHello,\nI'm working with a team to create a new BES response handler module, from what we have read the best method for doing this is using BES ./bescreatemodule located in the hyrax/bes/bin directory. However when compiling it errors out stating that they are specific files in the template directory which do not exist. We have looked at the current build on github and it does not contain those templates either. Is this method for creating a module defunct, and if so what is the best method for creating a new module in BES?\nAt this point, yes, that code has seen any attention for a while and I’d be suspect of it. We should update our docs or the templates, or both. But, in the meantime, I’d suggest that you look for a module that does more or less what you want and copy that code. I’m guessing that you might be working on a module that returns JSON or some dialect thereof, so the fileout_json or the w10n_handler would be good places to start. I suspect that the former is the place to start.\nWe can help you understand how that code works - it’s not quite as hard as it looks, but like all framework systems, you really do need knowledge of the framework’s organization. Look first at the docs, then contact us if you still need help.\nJames\nThank you,\nRiver\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: OPeNDAP Hyrax Installation",
     "name": "jgallagher",
     "body": "On Jan 24, 2018, at 00:46, Benedict Castro <benedict...@gmail.com> wrote:\nHi James,\nI'm Benedict Castro, a researcher from the Philippines, and I am in need of assistance in performing the OPeNDAP Hyrax installation. I got your email from github since I was searching the web for easier ways to install the server. Upon looking at the OPENDAP github profile, I've seen there are repositories there on installation procedures. I'm not that proficient yet in this kind of work, so I wanted to know if how can I get started? Do I need to download all repositories? And what should I look first from those? Any kind of help will be appreciated. I hope you can help me on this endeavor. Thank you.\nP.S. I'm working using CentOS 7.\nThere are two ways forward: You can build the Hyrax source code or install binaries. Have you tried installing the binaries we provide? Here’s the link if you have not: https://www.opendap.org/software/hyrax/1.14.0\nThat page also have a link to the server’s installation and configuration documentation. Installation is very straightforward on a C7 machine: get the RPMs and one WAR file, use yum to install the RPMs and copy the WAR file to the correct place. Start the server. Configuration can be very simple too, but depends on how many optional features you want to tweak.\nLet me know if you need more information.\nJames\nBest regards,\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: OPeNDAP Hyrax Installation",
     "name": "jgallagher",
     "body": "On Jan 24, 2018, at 18:35, Benedict Castro <benedict...@gmail.com> wrote:\nHi,\nGood day!\nAs a summary, I have already installed java (1.8.0_161) and tomcat 8 on a CentOS 7 machine. I also tried installing the binaries but I had troubles in using BES. At first, I can start the BES, but upon reboot, I can no longer initialize it. I used sudo service besd start and the error that I've encountered is shown below:\n/usr/bin/besctl start\nStarting the BES\nThe besctl command cannot write to /var/run/bes.\nCheck that the directory exists and that the user running besctl has write permission for it.\nSo I tried to create that directory and fortunately I can start BES again. But after rebooting, the created /var/run/bes directory no longer exists and the error above reoccurs. I've tried searching solutions to the problem but no luck as of now. I've seen one, though, but it would mean reverting back to CentOS 6. Have you encountered this kind of problem before?\nYes. What’s happening here is that, in the second case, you’re running the BES as a regular user. When you use ‘sudo service besd start’ you’re starting the service as root and root can create directories, etc., everywhere.\nHave you set the BES daemon to start up on boot? Since ‘root’ handles that, the BES will start on boot. As a plus, you won’t have to remember to start it on every boot. Here’s a page that describes what to do for Centos 6, and 7 and Ubuntu. Unfortunately, the exact command differs between OSs.\nhttps://geekflare.com/how-to-auto-start-services-on-boot-in-linux/\nJames\nThank you for the time and assistance.\nBest regards,\nBenedict\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: OPeNDAP Hyrax Installation",
     "name": "jgallagher",
     "body": "On Jan 30, 2018, at 7:47 PM, Benedict Castro <benedict...@gmail.com> wrote:\nHi,\nI just installed the latest version of the CentOS 7 and I haven't installed other binaries except for hyrax-specific dependencies. I tried editing the besd file in the init.d directory to create or do sudo mkdir /var/run/bes. Is that okay or would it cause future problems? Thank you.\nI don’t think that would be a problem. I’ll double check in the morning.\nJames\nBest regards,\nBeni\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\nOn Wed, Jan 31, 2018 at 12:21 AM, James Gallagher <jgall...@opendap.org> wrote:\nOn Jan 29, 2018, at 18:12, Benedict Castro <benedict...@gmail.com> wrote:\nThat directory always disappears or seems to be removed after every boot. In any case, how can I know who owns the directory? I tried logging as root but the problem persists.\nWe’ll try to reproduce the problem here. Is there anything unusual you think we should know about the CentOS 7 machine you’re using?\nJames\nBest regards,\nBeni\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\nOn Tue, Jan 30, 2018 at 8:38 AM, James Gallagher <jgall...@opendap.org> wrote:\nOn Jan 28, 2018, at 23:55, Benedict Castro <benedict...@gmail.com> wrote:\nHi James,\nI can't seem to make BES start up on boot. Every time I reboot, I still get the same error before (see below).\n<image.png>\nThen I tried creating the directory and started bes again. Fortunately, it initialized.\n<image.png>\nHowever, I can't set it to start on boot. Here's what I did in the terminal:\n<image.png>\nAfter rebooting, the BES hasn't been enabled and I go back to my initial problem (at the topmost of this email). I don't know the problem so I hope you can help.\nWho owns the var/run/bes directory.\nJames\nThank you.\nBest regards,\nBeni\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: OPeNDAP Hyrax Installation",
     "name": "jgallagher",
     "body": "Begin forwarded message:\nFrom: James Gallagher <jgall...@opendap.org>\nSubject: Re: OPeNDAP Hyrax Installation\nDate: January 30, 2018 at 22:21:04 MST\nTo: Benedict Castro <benedict...@gmail.com>\nCc: James Gallagher <jgall...@opendap.org>, \"sup...@opendap.org support\" <sup...@opendap.org>\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: OPeNDAP Hyrax Installation",
     "name": "jgallagher",
     "body": "Uday,\nWhen you are responding to users, please include sup...@opendap.org on the CC list - even if they don’t initially send their message to support. That way the fix(es) will show up in the archived support messages.\nThanks,\nJames\nOn Feb 6, 2018, at 00:32, Uday Kari <uk...@opendap.org> wrote:\nHi Benedict,\n1. Is that okay? Yes, should be OK, as temporary workaround, since we just need that directory to exist somehow (for now). We are modifying the RPM to ensure that this will no longer be a concern. We recommend using the -p option for mkdir, as advised below.\n2. didn't find the file (and directory)...just FYI: The /var/lib/cloud/... directories should be installed in the Centos 7 AMI mentioned in message below and available at (just Google \"Centos 7 AMI\"):\nhttps://aws.amazon.com/marketplace/pp/B00O7WM7QW\nIf still not there, you will need to perhaps look into cloud-init:\nhttps://stackoverflow.com/questions/23411408/how-do-i-set-up-cloud-init-on-custom-amis-in-aws-centos\n3. text box with gray background. I just copied and pasted from our jira\nhttps://opendap.atlassian.net/browse/HYRAX-600?focusedWorklogId=21534&page=com.atlassian.jira.plugin.system.issuetabpanels%3Aworklog-tabpanel#worklog-21534\nto get codes below where I used {code} tag to generate the html needed like so:\n{code}\nThis is some code\n{code}\nThanks,\nUday Kari\nOn 2/5/18 5:28 PM, Benedict Castro wrote:\nHi Uday,\nI didn't find the file (and directory) /var/lib/cloud/scripts/per-boot/per-boot.sh so what I did was I tried editing the /etc/init.d/besd file. There, I added the sudo mkdir /var/run/bes. Is that okay? Or would it cause problems in the future? So far, I haven't encountered any. Thanks!\nOn a side note, how do you write codes or scripts in a sort of text box with gray background as you did in your message? I'm just curious. Thanks!\nBest regards,\nBenedict Castro\nBenedict Z. Castro\nPhysical Oceanography Laboratory\nThe Marine Science Institute\nUniversity of the Philippines\nDiliman, Quezon City\nOn Tue, Feb 6, 2018 at 4:09 AM, Uday Kari <uk...@opendap.org> wrote:\nHi Benedict,\nThanks for reporting this issue. We are looking into this, so Hyrax is stable between reboots. We will provide a fix as soon as we have it done. For now, we have found that creating the /var/run/bes directory just before bes start will allow normal start. Specifically, with Centos 7 AMI, I was able to start BES on boot by editing (or add if not existing) the following file\n/var/lib/cloud/scripts/per-boot/per-boot.sh\nwith following shell script\n#!/bin/sh\nsudo mkdir -p /var/run/bes\nsudo besctl start\nThanks,\nUday Kari\n- show quoted text -\n--\nUday Kari\n808-269-2740\nuk...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi , I hope you’re having a great day! I came across Open Source Project For A Network again a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a new tech service specifically built for small and volunteer-run nonprofits. We help you save time and money by automating your fundraising interactions, in one place, with no technical work required on your end.\nAll of our customers receive every feature and service we have to offer, which includes a dedicated Success Manager who will be an invaluable resource for your time-strapped nonprofit. Our tagline is, \"Get more with less\".\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you are open to a quick conversation, please take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you are not interested, please let me know and you will never hear from me again. :)\nThank you for your time today and I hope to hear from you soon!\nJoan\nHere are the features that come with your Flipcause subscription…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nPeer to peer fundraising\nTeam fundraising\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated Success Manager with free fundraising and technical assistance\nFree website and social media integration\nUnlimited, fully customizable fundraising pages to market your campaigns\nA donor dashboard to keep track of your supporters in one place\nEasy to transfer fundraising activity into your accounting system\nSortable donor data for targeted mass email communication\n--\nJoan Ramirez | Flipcause\nCommunity Outreach Representative\n311 Oak Street Ste 110\nOakland, CA 94607\noffice: (800) 523-1950 ext. 3\nwww.flipcause.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "compile error in libdap",
     "name": "Scott Phillips",
     "body": "Hello support,\nI’m attempting to build libdap to support an installation of the GrADS software. I’m getting errors when compiling dds.y related to something not defined in the scope (see screen shot below). Pertinent details are as follows. This is an older version of libdap as requested by the GrADS developers, so hoping that is not the issue. Any help here is greatly appreciated.\nThanks,\nScott\nCentOS 7.4\ngcc version 4.8.5 (gcc-g++ installed)\nlibdap version 3.7.8\nCPPFLAGS=-I$SUPPLIBS/include\nPKG_CONFIG_PATH=$SUPPLIBS/lib/pkgconfig\n./configure –prefix=$SUPPLIBS\nmake all\nScott D. Phillips, Ph.D.\nEngineer tel 603-640-2459\nCreare fax 603-643-4657\n16 Great Hollow Road email sxp@creare.com\nP.O. Box 71 web www.creare.com\nHanover, NH 03755\nNotice: This email (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. §§ 2510-2521, is confidential and may be legally privileged. If you are not the intended recipient, you are hereby notified that any retention, dissemination, distribution, or copying of this communication is strictly prohibited. Please reply to the sender that you have received the message in error, and then delete it. Thank you."
    },
    {
     "page": "compile error in libdap",
     "name": "Nathan Potter",
     "body": "Hi Scott,\nWhat version of bison do you have installed?\nThanks,\nNathan\n> On Jan 11, 2018, at 11:36 AM, Scott Phillips <s...@creare.com> wrote:\n>\n> Hello support,\n>\n> I’m attempting to build libdap to support an installation of the GrADS software. I’m getting errors when compiling dds.y related to something not defined in the scope (see screen shot below). Pertinent details are as follows. This is an older version of libdap as requested by the GrADS developers, so hoping that is not the issue. Any help here is greatly appreciated.\n>\n> Thanks,\n> Scott\n>\n> CentOS 7.4\n> gcc version 4.8.5 (gcc-g++ installed)\n> libdap version 3.7.8\n> CPPFLAGS=-I$SUPPLIBS/include\n> PKG_CONFIG_PATH=$SUPPLIBS/lib/pkgconfig\n> ./configure –prefix=$SUPPLIBS\n> make all\n>\n>\n> <image001.png>\n> Scott D. Phillips, Ph.D.\n> Engineer tel 603-640-2459\n> Creare fax 603-643-4657\n> 16 Great Hollow Road email s...@creare.com\n> P.O. Box 71 web www.creare.com\n> Hanover, NH 03755\n>\n> Notice: This email (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. §§ 2510-2521, is confidential and may be legally privileged. If you are not the intended recipient, you are hereby notified that any retention, dissemination, distribution, or copying of this communication is strictly prohibited. Please reply to the sender that you have received the message in error, and then delete it. Thank you.\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "compile error in libdap",
     "name": "Scott Phillips",
     "body": "Nathan,\nI have bison version 3.0.4-1.e17 installed.\nThanks,\nScott\n- show quoted text -"
    },
    {
     "page": "compile error in libdap",
     "name": "jgallagher",
     "body": "On Jan 11, 2018, at 12:36, Scott Phillips <s...@creare.com> wrote:\nHello support,\nI’m attempting to build libdap to support an installation of the GrADS software. I’m getting errors when compiling dds.y related to something not defined in the scope (see screen shot below). Pertinent details are as follows. This is an older version of libdap as requested by the GrADS developers, so hoping that is not the issue. Any help here is greatly appreciated.\nWhat version of libdap?\nJames\nThanks,\nScott\nCentOS 7.4\ngcc version 4.8.5 (gcc-g++ installed)\nlibdap version 3.7.8\nCPPFLAGS=-I$SUPPLIBS/include\nPKG_CONFIG_PATH=$SUPPLIBS/lib/pkgconfig\n./configure –prefix=$SUPPLIBS\nmake all\n<image001.png>\nScott D. Phillips, Ph.D.\nEngineer tel 603-640-2459\nCreare fax 603-643-4657\n16 Great Hollow Road email sxp@creare.com\nP.O. Box 71 web www.creare.com\nHanover, NH 03755\nNotice: This email (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. §§ 2510-2521, is confidential and may be legally privileged. If you are not the intended recipient, you are hereby notified that any retention, dissemination, distribution, or copying of this communication is strictly prohibited. Please reply to the sender that you have received the message in error, and then delete it. Thank you.\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "compile error in libdap",
     "name": "Scott Phillips",
     "body": "Hi James,\nThis was version 3.7.8, but a modified version that the GrADS maintainers provided that address some issues with “modern” compilers and linux OS’s. Note that I have since managed to get a precompiled version (for CentOS 7.3) to install on CentOS 7.4, so this is no longer an issue for me in this case. Thanks for your reply, however.\n-Scott\nFrom: James Gallagher [mailto:jgall...@opendap.org]\nSent: Wednesday, January 31, 2018 11:57 AM\nTo: Scott Phillips\nCc: James Gallagher; sup...@opendap.org\nSubject: Re: [support] compile error in libdap\nOn Jan 11, 2018, at 12:36, Scott Phillips <s...@creare.com> wrote:\nHello support,\nI’m attempting to build libdap to support an installation of the GrADS software. I’m getting errors when compiling dds.y related to something not defined in the scope (see screen shot below). Pertinent details are as follows. This is an older version of libdap as requested by the GrADS developers, so hoping that is not the issue. Any help here is greatly appreciated.\nWhat version of libdap?\nJames\nThanks,\nScott\nCentOS 7.4\ngcc version 4.8.5 (gcc-g++ installed)\nlibdap version 3.7.8\nCPPFLAGS=-I$SUPPLIBS/include\nPKG_CONFIG_PATH=$SUPPLIBS/lib/pkgconfig\n./configure –prefix=$SUPPLIBS\nmake all\n<image001.png>\nScott D. Phillips, Ph.D.\nEngineer tel 603-640-2459\nCreare fax 603-643-4657\n16 Great Hollow Road email s...@creare.com\nP.O. Box 71 web www.creare.com\nHanover, NH 03755\nNotice: This email (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. §§ 2510-2521, is confidential and may be legally privileged. If you are not the intended recipient, you are hereby notified that any retention, dissemination, distribution, or copying of this communication is strictly prohibited. Please reply to the sender that you have received the message in error, and then delete it. Thank you.\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "compile error in libdap",
     "name": "jgallagher",
     "body": "On Feb 1, 2018, at 13:30, Scott Phillips <s...@creare.com> wrote:\nHi James,\nThis was version 3.7.8, but a modified version that the GrADS maintainers provided that address some issues with “modern” compilers and linux OS’s. Note that I have since managed to get a precompiled version (for CentOS 7.3) to install on CentOS 7.4, so this is no longer an issue for me in this case. Thanks for your reply, however.\nThe precompiled version is for 3.7.8 or a newer copy of libdap?\nAnd thanks for the report and heads up.\nJames\n-Scott\nFrom: James Gallagher [mailto:j...@opendap.org]\n- show quoted text -\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "compile error in libdap",
     "name": "Scott Phillips",
     "body": "James,\nAs far as I know it is 3.7.8, but it is not explicitly specified so I’m basing this on the dependent packages that they specified for custom builds of the software. I think only the GrADS maintainers would be able to provide an accurate answer.\n-Scott\nFrom: James Gallagher [mailto:jgall...@opendap.org]\nSent: Thursday, February 01, 2018 4:38 PM\nTo: Scott Phillips\nCc: James Gallagher; sup...@opendap.org\nSubject: Re: [support] compile error in libdap\nOn Feb 1, 2018, at 13:30, Scott Phillips <s...@creare.com> wrote:\nHi James,\nThis was version 3.7.8, but a modified version that the GrADS maintainers provided that address some issues with “modern” compilers and linux OS’s. Note that I have since managed to get a precompiled version (for CentOS 7.3) to install on CentOS 7.4, so this is no longer an issue for me in this case. Thanks for your reply, however.\nThe precompiled version is for 3.7.8 or a newer copy of libdap?\nAnd thanks for the report and heads up.\nJames\n-Scott\nFrom: James Gallagher [mailto:j...@opendap.org]\nSent: Wednesday, January 31, 2018 11:57 AM\nTo: Scott Phillips\nCc: James Gallagher; sup...@opendap.org\nSubject: Re: [support] compile error in libdap\nOn Jan 11, 2018, at 12:36, Scott Phillips <sxp@creare.com> wrote:\nHello support,\nI’m attempting to build libdap to support an installation of the GrADS software. I’m getting errors when compiling dds.y related to something not defined in the scope (see screen shot below). Pertinent details are as follows. This is an older version of libdap as requested by the GrADS developers, so hoping that is not the issue. Any help here is greatly appreciated.\nWhat version of libdap?\nJames\nThanks,\nScott\nCentOS 7.4\ngcc version 4.8.5 (gcc-g++ installed)\nlibdap version 3.7.8\nCPPFLAGS=-I$SUPPLIBS/include\nPKG_CONFIG_PATH=$SUPPLIBS/lib/pkgconfig\n./configure –prefix=$SUPPLIBS\nmake all\n<image001.png>\nScott D. Phillips, Ph.D.\nEngineer tel 603-640-2459\nCreare fax 603-643-4657\n16 Great Hollow Road email sxp@creare.com\nP.O. Box 71 web www.creare.com\nHanover, NH 03755\nNotice: This email (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. §§ 2510-2521, is confidential and may be legally privileged. If you are not the intended recipient, you are hereby notified that any retention, dissemination, distribution, or copying of this communication is strictly prohibited. Please reply to the sender that you have received the message in error, and then delete it. Thank you.\n--\nJames Gallagher\njgallagher@opendap.org\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "compile error in libdap",
     "name": "jgallagher",
     "body": "On Feb 2, 2018, at 12:06, Scott Phillips <s...@creare.com> wrote:\nJames,\nAs far as I know it is 3.7.8, but it is not explicitly specified so I’m basing this on the dependent packages that they specified for custom builds of the software. I think only the GrADS maintainers would be able to provide an accurate answer.\nOK. Thanks.\n-Scott\nFrom: James Gallagher [mailto:j...@opendap.org]\nSent: Thursday, February 01, 2018 4:38 PM\n- show quoted text -\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "TRMM Data NetCDF",
     "name": "syoung20",
     "body": "Greetings,\nI'm trying to download TRMM data in a NetCDF4 for 08/2005 from the link below. I would like the height of the convective column for hurricane Katrina in the Gulf of Mexico so I believe I would download the convStormHH data for long and lat between between -100 long and -80 long and between 20N and 30N latitude. I tried not altering the variables but the file has no HH variable when downloaded, it comes up 0 even though the convective storm and long and lat are selected in the proper order during download. Am I requesting the data correctly?\nThank you,\nSherry Young\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.html"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nCan you send me the value of the “Data URL” field from the form just prior to you pressing one of the “Action” buttons?\nWith that I can make a better assessment of where this is going wrong for you.\nI tried the following:\nI selecting just the “stormHH”, “nlat”, and “nlon” variables in the form (no tinkering with the array subsets) and and then pressed the “Get ASCII”, “Get as NetCDF 4”, and “Gets NetCDF 3” buttons.\nIn each case I got the data I expected.\nHere are the request URLs that the form generated (when I pressed the associated “Action” button):\nASCII: https://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.ascii?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15]\nNetCDF-3: https://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15]\nNetCDF-4” https://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc4?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15]\nNote that you mail application will likely ignore the closing “]” character in these URLs and in order to utilize them you will need to copy them and paste them into a browser’s address bar.\nCheers,\nNathan\nhttps://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF?stormHH[0:1:29][0:1:71][0:1:15]\n> On Feb 5, 2018, at 5:02 AM, syoung20 <syou...@masonlive.gmu.edu> wrote:\n>\n> Greetings,\n>\n> I'm trying to download TRMM data in a NetCDF4 for 08/2005 from the link below. I would like the height of the convective column for hurricane Katrina in the Gulf of Mexico so I believe I would download theconvStormHH data for long and lat between between -100 long and -80 long and between 20N and 30N latitude. I tried not altering the variables but the file has no HH variable when downloaded, it comes up 0 even though the convective storm and long and lat are selected in the proper order during download. Am I requesting the data correctly?\n>\n> Thank you,\n> Sherry Young\n>\n> https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.html\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nI realized that I omitted the ncat2 coordinate variable from my examples.\nHere they are again with stormHH and all three of the relevant coordinate variables:\nASCII: https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.ascii?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nNetCDF-3: https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nNetCDF-4: https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc4?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: syoung20 <syou...@masonlive.gmu.edu>\nSubject: Re: [support] TRMM Data NetCDF\nDate: February 5, 2018 at 6:17:12 AM PST\nTo: Nathan Potter <n...@opendap.org>\nHI Nathan,\nI tried the links you sent below and it downloads fine, but once opened in arcgis the values are 0. The data set appears to be empty for the stormHH value. I did get a datum conflict warning as well.\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.html\nSherry\nFrom: Nathan Potter <n...@opendap.org>\nSent: Monday, February 5, 2018 10:08:44 AM\nTo: syoung20\nCc: Nathan Potter; sup...@opendap.org\nSubject: Re: [support] TRMM Data NetCDF\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: syoung20 <syou...@masonlive.gmu.edu>\nSubject: Re: [support] TRMM Data NetCDF\nDate: February 5, 2018 at 6:22:36 AM PST\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\nI was able to open this again, but the entire data set shows as 0 I get a single color raster.\nSherry\nFrom: Nathan Potter <n...@opendap.org>\nSent: Monday, February 5, 2018 10:16:58 AM\nTo: syoung20\nCc: Nathan Potter; sup...@opendap.org\nSubject: Re: [support] TRMM Data NetCDF\nHi Sherry,\nI realized that I omitted the ncat2 coordinate variable from my examples.\nHere they are again with stormHH and all three of the relevant coordinate variables:\nASCII: https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.ascii?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nNetCDF-3: https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nNetCDF-4: https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc4?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nSincerely,\nNathan\n> On Feb 5, 2018, at 6:08 AM, Nathan Potter <n...@opendap.org> wrote:\n>\n>\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: syoung20 <syou...@masonlive.gmu.edu>\nSubject: Re: [support] TRMM Data NetCDF\nDate: February 5, 2018 at 7:00:59 AM PST\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\nIt looks like the storm height mean has data that will work but the spatial reference is not attached. I tried to assign it a spatial reference in GIS but that doesn't work either. Would this be an easier fix?https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.html\nThank you,\nSherry Young\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] TRMM Data NetCDF\nDate: February 5, 2018 at 7:33:12 AM PST\nTo: syoung20 <syou...@masonlive.gmu.edu>\nCc: Nathan Potter <n...@opendap.org>\nHi Sherry,\nPlease inspect the ASCII response and you will see that the stormHH array does contain values:\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.ascii?stormHH[0:1:29][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0:1:29]\nIf you have the NetCDF tools on your system you can use “ncdump” from the command line to inspect the downloaded files:\nncdump 3A25.20050801.7.HDF.nc4\nMy guess is that you are only looking at the first layer of stormHH, stormHH[0][0:1:71][0:1:15], which appears to be all zeros. When I used the (free and awesome) Panoply application ( https://www.giss.nasa.gov/tools/panoply/ ) to open the data I was able to get nice images by fitting the “Scale\" (contour interval range) to the data and then selecting various ncat layers.\nPanoply can open the downloaded NetCDF file, or it can work directly with the server:\nSimply start Panoply, dismiss the pop-up that wants you to select a local file, and then under the “File” menu select “Open Remote Dataset…” and enter the URL:\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF\nPanoply will prompt you for your Earthdata Login credentials and then show you all of the variables in the file. Simply double click on “stormHH” and Panoply should make a picture double quick. I fiddled with the settings under the “Scale\" tab and pretty quickly got reasonable images. But, as I said above, the first layer of stormHH, stormHH[0][0:1:71][0:1:15] appears to be all zero vales, but other layers stormHH[1:1:29][0:1:71][0:1:15] look good.\nSincerely,\nNathan\nOn Feb 5, 2018, at 7:00 AM, syoung20 <syou...@masonlive.gmu.edu> wrote:\nHi Nathan,\nIt looks like the storm height mean has data that will work but the spatial reference is not attached. I tried to assign it a spatial reference in GIS but that doesn't work either. Would this be an easier fix?https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.html\nThank you,\nSherry Young\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nThe data look fine, and the values are reasonable.\nThe values for the variable “nlat” range from -37.5 to 37.5 in one degree increments.\nThe values for the variable “nlon” range from -177.5 to 177.5 in five degree increments.\nThe metadata records for both of these variables utilize the appropriate climate forecast (CF) conventions.\nI suspect that the problem is not in the coordinate data, but rather it’s an issue with ArcGIS not being able to deal with the 3D array. I think that most GIS applications (including ArcGIS) don’t handle this kind of thing gracefully.\nYou might try downloading each ncat layer as a separate file and see if that will allow ArGIS to get it together:\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[0][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[0]\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[1][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[1]\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[2][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[2]\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[3][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[3]\nAnd so on.\nIf that idea works, then you could automate the retrieval so you didn’t have to click your way through it..\nIn a bash shell you could do it like this:\nfor i in {0..29}\ndo\ncurl -s -g \"https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.nc?stormHH[$i][0:1:71][0:1:15],nlon[0:1:71],nlat[0:1:15],ncat2[$i]” > 3A25.20050801.7.ncat-$i.HDF.nc\ndone\nYou would have to configure curl to authenticate but that’s pretty quick work:\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_em_curl_em_a_k_a_em_lib_curl_em\nSincerely,\nNathan\n> On Feb 5, 2018, at 7:58 AM, syoung20 <syou...@masonlive.gmu.edu> wrote:\n>\n> Hi Nathan,\n>\n> I need to open this in GIS. I got values on the storm height mean file but it isn't spatially referenced because the lat and long aren't given, it comes through as 0 to 71 for long etc. Is there a way to change this to actual lat and long? I can open it in GIS but just not once a coordinate system is designated.\n>\n> Thank you,\n> Sherry\n> From: Nathan Potter <n...@opendap.org>\n> Sent: Monday, February 5, 2018 11:33:12 AM\n> To: syoung20\n> Cc: Nathan Potter\n- show quoted text -"
    },
    {
     "page": "TRMM Data NetCDF",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: Nathan Potter <n...@opendap.org>\nSubject: Re: [support] TRMM Data NetCDF\nDate: February 5, 2018 at 11:12:01 AM PST\nTo: syoung20 <syou...@masonlive.gmu.edu>\nCc: Nathan Potter <n...@opendap.org>\nHi Sherry,\nIf you look at the request form:\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3A25/2005/3A25.20050801.7.HDF.html\nAnd scroll down to the “ncat2” variable in the little associated text box you should see:\nunits: level\ncomment: Second number of categories for histograms (30). Check more information from the references.\nreferences: http://pps.gsfc.nasa.gov/Documents/filespec.TRMM.V7.pdf\nUnfortunately the referenced PDF is of little value in this regard.\nTo try to find you additional information I did the following:\nI looked at the URL of the service and noted this key bit: “gesdisc”\nSo I went to the GES DISC site here:\nhttps://disc.gsfc.nasa.gov\nAnd I dropped the collection name “TRMM_3A25” into the search bar and it led me to this page:\nhttps://disc.gsfc.nasa.gov/datasets/TRMM_3A25_V7/summary?keywords=TRMM_3A25\nAnd there in the documentation tab I found a link for “README Document”:\nhttps://disc2.gesdisc.eosdis.nasa.gov/data/TRMM_L3/TRMM_3A25/doc/README.TRMM_V7.pdf\nWhich does at least mention ncat2, but says little more:\n• The dimension ncat2 refers to the second number of categories for histograms (a total of 30)\nI suspect that cryptic bit will not really help you.\nThere are other links on the TRMM_3A25 page including an email link for help at the bottom.\nI’m not sure how much more help I can be on this, as I am just one of the authors of the Hyrax server software and not one of the domain scientists that understand this data set.\nI hope that this will get you on your way!\nSincerely,\nNathan\nOn Feb 5, 2018, at 10:24 AM, syoung20 <syou...@masonlive.gmu.edu> wrote:\nHi Nathan,\nThank you, that works. But now I'm confused, what does each ncat contain?\nSherry\nFrom: Nathan Potter <n...@opendap.org>\nSent: Monday, February 5, 2018 12:26:36 PM\nTo: syoung20\nCc: Nathan Potter; User Support\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Hello.",
     "name": "GwanYeong Kim",
     "body": "Hello,I found a bug in libdap4. (https://github.com/OPENDAP/libdap4)Can I report bugs here?Or should I report a bug in the github libdap4?Thanks."
    },
    {
     "page": "Hello.",
     "name": "jgallagher",
     "body": "Here or github\n--\nJames Gallagher\njgallagher at opendap.org\n- show quoted text -"
    },
    {
     "page": "Hello.",
     "name": "GwanYeong Kim",
     "body": "this : https://github.com/OPENDAP/libdap4/issues/39"
    },
    {
     "page": "Hello.",
     "name": "jgallagher",
     "body": "We’re looking into this.\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Hello.",
     "name": "jgallagher",
     "body": "On Jan 30, 2018, at 23:09, GwanYeong Kim <gy74...@gmail.com> wrote:\nHello,\nI have minimized the crash file(xml file).\nI hope it helps in the analysis.\n```\nkaras$:~/libdap4/tests$ cat Null_D4ParserSax2_min.xml\n<?l?><t xmlns=\"http://xml.opendap.org/ns/DAP/4.0#\"><Dimension name=\"\" size=\"%n\">\n```\nexcellent! thanks.\nJames\nThanks.\n2018-01-31 9:57 GMT+09:00 Uday Kari <uk...@opendap.org>:\nHi GwanYeong,\nWe have reproduced the issue and are looking into it.\nThanks,\nUday Kari\nOn 1/30/18 10:16 AM, GwanYeong Kim wrote:\nHello.\nthank you for confirmation.\nThis file is an xml file generated to look for memory corruption.\nCan not you reproduce the crash?\nI am using fuzzing to find bugs.\nGenerally, fuzzer generates garbage code.\nSecurity experts look for the bits that cause a particular crash and determine if they are exploited.\nIt does not matter if they are normal files.\nThanks.\n--------- Original Message ---------\nFrom: James Gallagher\nTo: Uday Kari ,GwanYeong Kim\nDate: Wed Jan 31 01:40:35 GMT+09:00 2018\nSubject: Re: [support] Hello.\nOn Jan 30, 2018, at 09:32, Uday Kari <uk...@opendap.org> wrote:\nHi GwanYeong,\nCan you please re-post the codes? The zip file seems corrupt with no content.\nCorrection, the zip file has one file in it, but it appears to be a DMR xml file but is corrupted. Here’s what I see when I look at it with a text editor:\nedamame:hyrax_git jimg$ more Null_D4ParserSax2\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<aset name=\"te_3\" dapVersionaa=\"0\" dmrVersion=\"100\" xml:base=\"ml\"\nxmlns=\"http://xml.opendap.org/ns/DAP/4.0#\"\nxmlns:dap=\"h#\">\net\"\n#\">\n<Dimension name=\"row\" size=\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nn\">\n<\n>\nThere’s no way that will parse. Are you trying to show us a bu that’s triggered by this input or is this input corrupted? If it’s the former, there are several problems with this file (root element is not correct, it’s not a well formed xml doc, et c.), can you send us a file with just one problem that will trigger the memory error?\nJames\nThanks,\nUday Kari\nukari at opendap.org\nFrom: GwanYeong Kim <gy74...@gmail.com>\nSubject: Re: [support] Hello.\nDate: January 28, 2018 at 19:27:08 MST\nTo: James Gallagher <jgall...@opendap.org>\nCc: sup...@opendap.org\nthis : https://github.com/OPENDAP/libdap4/issues/39\n2018-01-29 11:21 GMT+09:00 James Gallagher <jgall...@opendap.org>:\nHere or github\n--\nJames Gallagher\njgallagher at opendap.org\nOn Jan 28, 2018, at 19:09, GwanYeong Kim <gy74...@gmail.com> wrote:\nHello, I found a bug in libdap4. (https://github.com/OPENDAP/libdap4) Can I report bugs here? Or should I report a bug in the github libdap4? Thanks.\n--\nJames Gallagher\njgall...@opendap.org\n--\nJames Gallagher\njgall...@opendap.org\n<Null_D4ParserSax2_min.xml>\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Hello.",
     "name": "GwanYeong Kim",
     "body": "Hello,\nI think size contains the wrong format string character, which causes an error.\nThanks.\n```\nsize=\"%n\"\n```"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: files with symbolic links don't show up",
     "name": "jgallagher",
     "body": "On Dec 13, 2017, at 12:46, Kent Yang <mya...@hdfgroup.org> wrote:\nHi James/Nathan,\nI recently tested the Hyrax 1.14.0 release RPMs at my CentOS 7 virtual box. I found that even if I changed\nBES.Catalog.catalog.FollowSymLinks=No\nTo\nBES.Catalog.catalog.FollowSymLinks=Yes\nAnd\nBES.FollowSymLinks=No\nTo\nBES.FollowSymLinks=Yes\nI still don’t see the symbolically linked served files.\nI then built libdap and bes from the source code and followed the same steps above. I can see the symbolically linked served files. I am wondering why symbolic links don’t work when installing Hyrax via RPMs. Have you tested out this feature?\nCheck that the paths you’re linking into the RootDirectory are readable by the ‘bes’ user - the RPM packages contain a BES daemon that runs as ‘bes’ while the source builds generally run as whoever runs besctl or besd.\nIt’s tedious, but you must check all the parts of the paths. In fact, based on your report, it’s likely that the problem is not with the directory you’ve linked to, but one of its parent directories.\nLet me know if this resolves the issue.\nJames\nThanks,\nKent\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Downloading HPBL data",
     "name": "Yara Abu Awad",
     "body": "Hello,\nI'm trying to download HPBL data from this link:\nhttps://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/20thC_ReanV2c/Monthlies/gaussian/monolevel/hpbl.mon.mean.nc.html\nBut I'm not sure what format the inputs should be in and I'm getting this error:\nError {\ncode = 3;\nmessage = \"Invalid Parameter Exception: DArrayDimension.setProjection: Bad Projection Request: stop >= size: 2013:1968\";\n};\nWhat I needis data from 2013 onwards. Can you please advise?\nAlso, is there a data dictionary / data description somewhere?\nMany thanks in advance for your help,\nYara\n--\nYARA ABU AWAD | Postdoctoral Research Fellow\nDepartment of Environmental Health | Harvard T.H. Chan School of Public Health\n401 Park Drive | 4th floor West, office 408-A | Boston, MA 02215\no: 617-384-8804 | c: 857-277-2631\nyabu...@hsph.harvard.edu"
    },
    {
     "page": "Downloading HPBL data",
     "name": "Nathan Potter",
     "body": "Hi Yara,\nThis form (produced by an instance of the THREDDS Data Server, aka TDS), accepts only index based constraints where “0” is the index of the first element. In order to subset by value you need to look at the values of the MAP arrays (time, lat, and lon) to determine which indices correspond to your desired value based boundaries.\nYou can easily get the values of lat (latitude) by selecting just the lat array and then clicking the “Get ASCII” button. The form builds a request URL:\nhttps://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/20thC_ReanV2c/Monthlies/gaussian/monolevel/hpbl.mon.mean.nc.ascii?lat[0:1:93]\nAnd then opens that in a new window.\n(Caveat: Your emailer may well not include the closing square bracket “]” when it sees these links and without it none of the, will work. You may need to copy and paste the link into a browser or a terminal window by hand)\nFor example, if you wanted latitudes such that 50>lat>40 then you would request 21:1:25 where 21 is the first index, the stride is 1, and 25 is the last index:\nhttps://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/20thC_ReanV2c/Monthlies/gaussian/monolevel/hpbl.mon.mean.nc.ascii?lat[21:1:25]\nYou can do the same for longitude (lon) and time.\nThe metadata (data dictionary) for each variable appears in the text box associated with the variable in the form. This is especially important for the time variable as its units are described as \"units: hours since 1800-1-1 00:00:0.0”. So in order for you to convert your desired time range into the index space you will need to figure out what your start date is in the units \"hours since 1800-1-1 00:00:0.0”. This can be accomplished in a couple of steps by using an online tool like:\nhttps://www.epochconverter.com\nSince these tools typically have the epoch fixed at \"1970-1-1 00:00:0.0” you would first find the value of the \"1800-1-1 00:00:0.0” relative to \"1970-1-1 00:00:0.0” (-5364662400 seconds) and then the value of your desired start time, say \"2013-1-1 00:00:0.0” (1356998400 seconds) and then (5364662400 + 1356998400 = 6721660800 seconds since 1800-1-1 00:00:0.0 ) Convert this to hours by dividing by 60 to get minutes and again by 60 to get the desired hours since \"1800-1-1 00:00:0.0\": 1867128\nI used curl and an inline awk script to find the values that match:\ncurl https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/20thC_ReanV2c/Monthlies/gaussian/monolevel/hpbl.mon.mean.nc.ascii?time[0:1:1967] | awk '{if(NR==6){ gsub(\",\",\"\",$0); n=split($0,a,\" \");for(i=1;i<=n;i++){if(a[i]>=1867128)print i-1, a[i];} }}’ -\nWhich output:\n1944 1867128.0\n1945 1867872.0\n1946 1868544.0\n1947 1869288.0\n1948 1870008.0\n1949 1870752.0\n1950 1871472.0\n1951 1872216.0\n1952 1872960.0\n1953 1873680.0\n1954 1874424.0\n1955 1875144.0\n1956 1875888.0\n1957 1876632.0\n1958 1877304.0\n1959 1878048.0\n1960 1878768.0\n1961 1879512.0\n1962 1880232.0\n1963 1880976.0\n1964 1881720.0\n1965 1882440.0\n1966 1883184.0\n1967 1883904.0\nSo for time > \"2013-1-1 00:00:0.0” I found the index based time subset would be [1944:1:1967]\nI hope that helps!\nPlease let us know if you need more assistance.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "hemp...@oregonstate.edu",
     "body": "I am trying to run Hyrax on an AWS Ubuntu Server 16.04 instance, however, I am receiving a \"Hyrax - Bad Request (400)\" when trying to access http://localhost:8080/opendap for the first time. I am attaching the two catalina log files found in the $CATALINA_HOME/logs directory. I used Firefox to capture the logs, but this issue is occurring with Chrome as well.\nAny assistance would be greatly appreciated.\nThanks in advance,\n-Corey"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "hemp...@oregonstate.edu",
     "body": "The error message of the bad request is as follows:\n\"The specific error message associated with your request was: Unable to find transmitter dap2\"\nYou will find the same error in the catalina.out log.\n- show quoted text -"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "Nathan Potter",
     "body": "Hi Corey,\nI think your BES is is not working correctly. It may be that components failed to build, or that the configuration is incorrect. Since you are working on Ubuntu I’m pretty sure you are building from source. If that’s not the case not let me know, otherwise I’ll need to start with the basics\n- Which directions are you following to build the software?\n- Did the hyrax-dependencies project build with out error? This is important, and that particular build will not stop on error so you should go back into that project and run make again (with the “prefix” ENV var set as directed) and scan the output for errors.\n- Did the libdap4 project build and pass all tests (i.e. “make check”)? Did you install it?\n- When you built the BES where you able to successfully run “make check”?\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "Nathan Potter",
     "body": "Hi Corey,\nOne more thing:\nPlease send me the response content for:\nhttp://localhost:8080/opendap/version\n(Which might not work, but if it does it will be informative)\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "hemp...@oregonstate.edu",
     "body": "Nathan,\n-I am following the README from the Hyrax Github repository (https://github.com/OPENDAP/hyrax) to clone the source and build Hyrax.\n-The hyrax-dependencies projects seems to have built without error.\n-Make check passes all tests for libdap.\n-Make check fails 2 of 30 tests for bes.\nSee the attached files.\nYou were correct in that the BES was not working correctly. I realized that the volume I was using was too small to build Hyrax causing most of the dependencies to not build. I would still get a besctl binary in the /build/bin folder, but it was never really working when I was trying to start the BES. I've remedied that issue and I have seemingly compiled Hyrax successfully, however, I have a new issue. I am now getting the following error when trying to run ./besctl start:\n\"Starting the BES\nDeveloper Mode: Not testing if BES is run by root\nCaught plugin exception during initialization of dap module:\n/home/ubuntu/hyrax/build/lib/bes/libdap_module.so: undefined symbol: _ZN6libdap3DASC1Ev\nCould not initialize the modules to get the log contexts.\nDeveloper Mode: Running as root - setting group and user ids\nCaught plugin exception during initialization of dap module:\n/home/ubuntu/hyrax/build/lib/bes/libdap_module.so: undefined symbol: _ZN6libdap3DASC1Ev\nThe beslistener status is not 'BESLISTENER_RUNNING' (it is '0') the master pid was not changed.\nbesdaemon: server cannot mount at first try (core dump). Please correct problems on the process manager /home/ubuntu/hyrax/build/bin/beslistener\nBES PID file exists but process not running, cleaning up\nFAILED: The BES daemon did not appear to start\"\nAny ideas? Let me know if any logs would be useful.\nThanks in advance,\n-Corey\n- show quoted text -"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "Nathan Potter",
     "body": "Hi Corey,\nIt looks like somewhere you forgot a key step - probably you configured and built libdap before you conditioned your shell environment by sourcing “spath.sh”.\nHere is a recovery plan:\n0) Get thee to the top of the hyrax directories\ncd /home/ubuntu/hyrax; # Or wherever it is that you put all the source code.\n1) Uninstall what you have done:\ncd bes; make uninstall; cd ..\ncd libdap4; make uninstall; cd ..\n2) Now, before you do ANYTHING else source “spath.sh” in your current shell!\nsource spath.sh; echo $prefix\n3) If “echo $prefix” doesn’t look correct then let’s discuss.\n4) Make sure that the “hyrax-dependencies” have been built and are installed in “$prefix/deps”\ncd hyrax-dependencies; make\n5) Reconfigure and rebuild libdap4:\ncd libdap4; autoreconf -vif; ./configure --prefix=$prefix --enable-developer;\nmake -j9; make -j9 check; make install; cd ..\n6) And now the BES:\ncd bes; autoreconf -vif; ./configure --prefix=$prefix --with-dependencies=$prefix/deps --enable-developer;\nmake -j9; make -j9 check; make install; cd ..\n7) Try launching the BES again…\nbesctl start\nSorry about the (re)grind here, but the error makes it clear that libdap is not located where the BES thinks it should be.\nCheers,\nNathan\n- show quoted text -\n> <bes_make_check.txt><libdap_make_check.txt>\n- show quoted text -"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "hemp...@oregonstate.edu",
     "body": "Nathan,\n1) Uninstall what you have done:\nI did you one better and re-cloned the repo.\n2) Now, before you do ANYTHING else source “spath.sh” in your current shell!\nsource spath.sh; echo $prefix\n$prefix = \"/home/ubuntu/hyrax/build\"\n3) If “echo $prefix” doesn’t look correct then let’s discuss.\n4) Make sure that the “hyrax-dependencies” have been built and are installed in “$prefix/deps”\nhyrax-dependencies; make\nI see all dependencies installed successfully to $prefix/deps\n5) Reconfigure and rebuild libdap4:\ncd libdap4; autoreconf -vif; ./configure --prefix=$prefix --enable-developer;\nmake -j9; make -j9 check; make install; cd ..\nAll successful\n6) And now the BES:\ncd bes; autoreconf -vif; ./configure --prefix=$prefix --with-dependencies=$prefix/deps --enable-developer;\nmake -j9; make -j9 check; make install; cd ..\nAll successful\n7) Try launching the BES again…\nbesctl start\nbes started successfully.\nI am now able to access http://localhost:8080/opendap. Everything seems to be in working order!\nYour help has been very much appreciated. Thank you!\nCheers,\n-Corey\n- show quoted text -"
    },
    {
     "page": "Hyrax - Bad Request (400)",
     "name": "Nathan Potter",
     "body": "Hi Corey,\nThat’s great! Sorry about the restart from scratch deal, but sometimes that the fastest way to get back on track. Please let me know if you have any other questions, or want to know where to look for examples of something you are trying to do.\nCheers,\nNathan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Question Data Access Form",
     "name": "Rosul, Lauren Marie",
     "body": "Hello,\nI am in need of a complete data set for hurricane Ike in order to visualize the model results in ArcGIS software. I am specifically interested in the model results from https://ioos.us/comt/projects/tropical_inundation . I am unfamiliar with this type of data access form is there a way to request the entire set in an appropriate format for my needs?\nThank You For your help,\n-Lauren Rosul\nLauren Rosul\nM.S. Geoscience\nlmr...@uncw.edu\nSocio Environmental Analysis Lab\nDepartment of Earth and Ocean Science\nUniversity of North Carolina Wilmington"
    },
    {
     "page": "Question Data Access Form",
     "name": "Nathan Potter",
     "body": "Hi Lauren,\nI looked at the IOOS site. All of the Ike data that I inspected appeared to be stored in “unstructured mesh” formats. I am not sure what you feel is an appropriate format, but I know that the underlying server ( an instance of the THREDDS Data Server, aka TDS) is capable of both subsetting the data and returning the results in various formats.\nThese datasets are quite large. At the bottom of the Data Request Form page you can see a structural description of the dataset. Here is an example from FVCOM\nDataset {\nFloat64 time[time = 361];\nString Times[time = 361];\nFloat32 lon[node = 417642];\nFloat32 lat[node = 417642];\nFloat32 lonc[nele = 826866];\nFloat32 latc[nele = 826866];\nFloat32 siglay[siglay = 10][node = 417642];\nFloat32 depth[node = 417642];\nFloat32 nv[three = 3][nele = 826866];\nFloat32 zeta[time = 361][node = 417642];\nFloat32 u[time = 361][siglay = 10][nele = 826866];\nFloat32 v[time = 361][siglay = 10][nele = 826866];\nFloat32 ua[time = 361][nele = 826866];\nFloat32 va[time = 361][nele = 826866];\nFloat32 maxele[node = 417642];\nInt32 mesh_topology;\n} noaa/ioos/comt/tropical_inundation.ike_usf_fvcom_3dwave.ncml;\nNote that the variables u and v are each 11,939,945,040 bytes (11.9 GB), not to mention the rest of the variables. I believe that this makes downloading the entire dataset impractical.\nThe folks at UNIDATA, who produce the TDS software, and the Brian McKenna the site admin, will be better able to assist you with getting the data you need. I have included them in the recipients lists for this response. They will be able to help you formulate a subset query that also returns the data in an encoding that you will find useful.\nThanks, and I apologize for the delayed response.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Question Data Access Form",
     "name": "Roy Mendelssohn - NOAA Federal",
     "body": "Hi Lauren:\nBased on what I at say:\nhttps://ioos.us/comt/projects/tropical_inundation/slosh--mdl--egm3?t=Effect%20of%20wind%20input%20parameterizations%20on%20hurricane%20wave%20estimation%20Hurricane%20Ike%20landfall%20ADCIRC+SWAN%20Sensitivity%20Unstructured%20surge-wave%20models%20Comparison%20of%202-D%20and%203-D%20model%20responses%20Hurricane%20IkeHurricane%20Rita\nThere are WMS endpoints for the data, and depending on what you are trying to do that might be a better solution for ArcGIS. Or if you need the data also, not just the image, you can try installing the Environmental Data Connector into ArcGIS, which would allow you to access the OPenDAP endpoint. This is available at:\nhttp://asascience.com/software/downloads/\nBut in either case, I would second Nathan's warning. These are very large datasets. I would strongly suggest that you spend some time becoming more familiar with the datasets, and think about the size of the datasets and what you really need, otherwise almost any request is likely to crash the program. One nice thing about using WMS or OPeNDAP, in each request you can request just the variable you need for just the locations you need. But even then, if you are not careful, the request can be very large, because it is a very fine grid.\nHTH,\n-Roy\n- show quoted text -\n> _______________________________________________\n> NOTE: All exchanges posted to Unidata maintained email lists are\n> recorded in the Unidata inquiry tracking system and made publicly\n> available through the web. Users who post to any of the lists we\n> maintain are reminded to remove any personal information that they\n> do not want to be made public.\n>\n>\n> thredds mailing list\n> thr...@unidata.ucar.edu\n> For list information or to unsubscribe, visit: http://www.unidata.ucar.edu/mailing_lists/\n**********************\n\"The contents of this message do not reflect any position of the U.S. Government or NOAA.\"\n**********************\nRoy Mendelssohn\nSupervisory Operations Research Analyst\nNOAA/NMFS\nEnvironmental Research Division\nSouthwest Fisheries Science Center\n***Note new street address***\n110 McAllister Way\nSanta Cruz, CA 95060\nPhone: (831)-420-3666\nFax: (831) 420-3980\ne-mail: Roy.Men...@noaa.gov www: http://www.pfeg.noaa.gov/\n\"Old age and treachery will overcome youth and skill.\"\n\"From those who have been given much, much will be expected\"\n\"the arc of the moral universe is long, but it bends toward justice\" -MLK Jr."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Data Request",
     "name": "Tolley, Stuart",
     "body": "Dear Sir/Madame,\nI am currently trying to download data using your dataset access form, however I am finding it very difficult to understand. The data I am trying to access is:\nProject= CMIP5\nmodel=HadGEM2-es\nexperiment=RCP4.5\nThe main variable I'm looking for is pH.\nThe specific area I need it in is a region of sea off the British Coastline:\nLatitude= 51-52\nLongitude= 1.7-2.55.\nThe time I require if for the year 2050 and 2100.\nCould you please help me download this data!\nRegards,\nStuart Tolley"
    },
    {
     "page": "Data Request",
     "name": "Nathan Potter",
     "body": "Hi Stuart,\nYou can find more about how to use the form here: https://opendap.github.io/documentation/QuickStart.html#_an_easy_way_using_the_browser_based_opendap_server_dataset_access_form\nI can provide more specific help if you send me the URL of the form page. We provide the software (Hyrax) but many institutions around the world deploy it and I can’t know which dataset you are attempting to access without the URL.\nSincerely,\nNathan Potter\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Data Request",
     "name": "Nathan Potter",
     "body": "Hi Stuart,\nI do not have access to ESGF data which limits my ability to provide specific detailed instructions.\nTo simplify your challenge I suggest you try using a client program that understands the data access protocol.\nI believe Ferret can work, as can Panoply and Matlab. Panoply is free and might just make it very easy.\nhttps://www.giss.nasa.gov/tools/panoply/\nAnd I think (depending on how ESGF implement their OpenID portal) that Panoply will be able to navigate the authentication transaction.\nIf you try Panoply simply dismiss the file chooser that launches at start up. Go to the “File\" menu and select “Open Remote Dataset…” and enter the URL:\nhttp://esgf-data1.ceda.ac.uk/thredds/dodsC/cmip5.output1.MOHC.HadGEM2-ES.rcp45.mon.ocnBgchem.Omon.r4i1p1.ph.20120110.aggregation.1\nAnd see what happens - with luck, after you authenticate, you should be able to select variables from within the dataset and do things like plot, subset, etc.\nMore comments below...\n> On Jan 26, 2018, at 2:48 AM, Tolley, Stuart <st...@exeter.ac.uk> wrote:\n>\n> Hi Nathan,\n>\n> Unfortunately I'm still pretty confused with the strides and the arrays etc. I'm an undergraduate student and it's all a bit baffling.\n>\n> This is the data I wish to download:\n>\n> <pastedImage.png>\n>\n> When I click OpenDAP download it just attempts to download something which cannot be supported by my computer. When I click a link for THREDDS Catalog I can gain access to the OpenDAP data access form. So if I want to find out the pH in 2100 for an area of ocean between latitudes: 51-52 and longitudes: 1.7-2.55 how should it look? should it be 51:1:52?\nWithout data access (because I do not have authentication credentials in ESGF) I do not have access t the data and so I cannot verify that you subset is correct. But from look at you question I think you’re not understanding the difference between index based subsetting and values based subsetting. These servers support index based subsetting. So, I am going to paste in a previous response I made for a different user that outlines how to figure out your subset values by hand. The URLs and data are different, but the concepts are the same. So for YOUR example the base URL should be, I believe,\nhttp://esgf-data1.ceda.ac.uk/thredds/dodsC/cmip5.output1.MOHC.HadGEM2-ES.rcp45.mon.ocnBgchem.Omon.r4i1p1.ph.20120110.aggregation.1\nYou can get a quick sense of the structure by looking at the DDS:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.dds\n( http://esgf-data1.ceda.ac.uk/thredds/dodsC/cmip5.output1.MOHC.HadGEM2-ES.rcp45.mon.ocnBgchem.Omon.r4i1p1.ph.20120110.aggregation.1.dds )\n(NB: from here forward the example uses the example URL and not the one you supplied, you will be seeing different data structures and most likely different variable names, you will have to adjust accordingly)\nWhich shows you how it’s represented:\nDataset {\nFloat32 Longitude[Longitude = 2160];\nFloat32 Lon_bounds[Longitude = 2160][nv = 2];\nFloat32 Latitude[Latitude = 960];\nFloat32 Lat_bounds[Latitude = 960][nv = 2];\nFloat32 Time_bounds[Time = 1][nv = 2];\nFloat32 Time[Time = 1];\nGrid {\nArray:\nFloat32 SLA_ERR[Time = 1][Longitude = 2160][Latitude = 960];\nMaps:\nFloat32 Time[Time = 1];\nFloat32 Longitude[Longitude = 2160];\nFloat32 Latitude[Latitude = 960];\n} SLA_ERR;\nGrid {\nArray:\nFloat32 SLA[Time = 1][Longitude = 2160][Latitude = 960];\nMaps:\nFloat32 Time[Time = 1];\nFloat32 Longitude[Longitude = 2160];\nFloat32 Latitude[Latitude = 960];\n} SLA;\n} ssh_grids_v1609_2005082012.nc;\nI worked the issue using the “manual” method, tedious but doable. It’s far preferable to write a small shell script or python program to do the following for you.\nTo find the extents you’ll have to inspect the Latitude and Longitude arrays to determine the indices of your bounding values. You wanted:\n> 284.167 to 255.167 for longitude and 34.1667 to 12.1667 for latitude\nIt’s simple to get the server to return data as ascii values. This is done by modifying the URL, and then, since the data are large, restricting the request to return just the Longitude values (Note how I modified the dataset URL with the suffix “.ascii” and then added a “?\" followed by the name of the longitude array “Longitude\"\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude\nBut thats a bunch of points so we can look at a smaller ranges:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1520:1:1540]\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1720:1:1730]\nWhich I determined by estimation.\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1530]\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1729]\nThe closest longitude indices that include your desired interval are 1530 and 1729\nSo the longitude subset is:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1530:1:1729]\nSimilarly we can determine the latitude bounds:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude\nSimilarly, I poked at it until I got:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude[552]\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude[685]\nSo the latitude subset is:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude[552:1:685]\nNow we can make the data request. You can use the data request form here:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.html\nand select the SLA grid at the bottom and then insert the Longitude subset expression 1530:1:1729 and the Latitude subset expression 552:1:685 into the form and then use the buttons at the top to make the request.\nAlternately you can do it by building the data request URLs as follows:\nASCII https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?SLA[0:1:0][1530:1:1729][552:1:685]\nNetCDF-3 https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.nc?SLA[0:1:0][1530:1:1729][552:1:685]\nNetCDF-4 https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.nc4?SLA[0:1:0][1530:1:1729][552:1:685]\nAnd use those with a browser or the “curl” program from the command line.\nIf you have a large number of these to do I think you will be best served writing a small shell script or python program to do this for you!\nI hope that helps…\nSincerely,\nNathan\n> please explain this to me as simply as you can I have not had much experience with this sort of thing.\n>\n> Additionally, is Ferret via Oracle VirtualBox the best way to open this data?\n>\n> Thanks,\n>\n> Stuart\n> From: Nathan Potter <n...@opendap.org>\n> Sent: Thursday, January 25, 2018 2:54:35 PM\n> To: Tolley, Stuart; THREDDS community\n> Cc: Nathan Potter\n> Subject: Re: [support] Data Request\n>\n> Hi Stuart,\n>\n> Unfortunately I do not have access credentials for any of the OpenID providers listed at that site and thus am unable to view that dataset. However from the URL I can see that the software that is providing access is not Hyrax, but rather the THREDDS Data Server (TDS), the form interface is roughly the same as the one the we provide with Hyrax so the documentation link I sent is relevant.\n>\n> The form utilizes array index based subsetting, not value based subsetting. For your request you would need to examine the longitude and latitude arrays to determine the indices of the bounding box you wish to subset. This can be done quickly by selecting one the coordinate variables (pick for example the variable for latitude) by selecting just that in the form and then clicking on the “Get ASCII” button. (You may need to do a bit of scripting to find the indices of the bounds) Repeat for longitude. Once you can express them as start:stride:stop you can select the variable you wish to retrieve (pH for example) and plug those index values into the appropriate part of the form. Using the form you can then retrieve the results as either an ASCII download or as a DAP2 binary object.\n>\n> The TDS provides several other (possibly more intuitive) ways to request data, but I am not entirely familiar with them so I am including the TDS support list so that they can chime in and assist you.\n>\n> Cheers,\n>\n> Nathan\n>\n>\n> > On Jan 25, 2018, at 6:35 AM, Tolley, Stuart <st...@exeter.ac.uk> wrote:\n> >\n> > Hi Nathan,\n> >\n> > This is the URL I am hoping to use: http://esgf-data1.ceda.ac.uk/thredds/dodsC/cmip5.output1.MOHC.HadGEM2-ES.rcp45.mon.ocnBgchem.Omon.r4i1p1.ph.20120110.aggregation.1\n> >\n> > Thank you in advance for any assistance you can provide.\n> >\n> > Regards,\n> >\n> > Stuart\n> > From: Nathan Potter <n...@opendap.org>\n> > Sent: Thursday, January 25, 2018 1:57:48 PM\n> > To: Tolley, Stuart\n> > Cc: Nathan Potter; sup...@opendap.org\n> > Subject: Re: [support] Data Request\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Nathan Potter",
     "body": "Hi Huber,\nSo you do have to be the root user to start the BES. The BES quickly changes it’s runtime PID to be the user “bes”, but because of the daemonic nature of the BES it must be started as the root user:\nsudo service best start\nor\nsudo besctl start\nshould do it. Remember - it does not continue to run as root. You just have to be root to launch it.\nThanks,\nNathan\n> On Sep 24, 2017, at 8:06 AM, Huber Gilt Lopez <hg...@igp.gob.pe> wrote:\n>\n> Hello Nathan\n>\n> Thank you for your help, these are the outputs from your request.\n>\n> 1)\n>\n> [root@hyrax ~]# service besd status\n> /usr/bin/besctl status\n> Could not find the BES PID file\n>\n> [root@hyrax ~]# service besd start\n> /usr/bin/besctl start\n> Starting the BES\n> The besctl command cannot write to /var/run/bes.\n> Check that the directory exists and that the user running\n> besctl has write permissions for it.\n>\n> 2)\n>\n> I only can run it as root user, but this an another question. I've got the following messages when I used an other user. How can I run the \"besd\" service as a non-privileged user?\n>\n> /usr/bin/besctl start\n> Starting the BES\n> FAILED: Must be root to run BES\n> FAILED: The BES daemon did not appear to start\nYou cannot START the best as a non-root user. But it never RUNS as the root user, it always changes to a different user downstream.\n>\n> 3)\n>\n> [root@hyrax ~]# ps aux | grep bes\n> root 1094 0.0 0.0 112680 968 pts/0 S+ 09:52 0:00 grep --color=auto bes\n>\n> After, I created manually the directory /var/run/bes, I can start the service.\n>\n> [root@hyrax ~]# ps aux | grep bes\n> bes 1247 0.4 0.4 349420 25336 ? Ss 09:47 0:00 /usr/bin/besdaemon -i /usr -c /etc/bes/bes.conf -r /var/run/bes\n> bes 1249 0.4 0.4 350256 25840 ? Ss 09:47 0:00 /usr/bin/beslistener -c /etc/bes/bes.conf -d /var/log/bes/bes.log,-ascii,-besdaemon,-csv,-dap,-ff,-fits,-fojson,-fonc,-fong,-gateway,-gdal,-h4,-h5,-nc,-ncml,-ppt,-reader,-server,-usage,-w10n,-www,-xd -i /usr -r /var/run/bes\n> root 1263 0.0 0.0 112680 968 pts/0 S+ 09:48 0:00 grep --color=auto bes\n>\n> [root@hyrax ~]# grep -E \"^(BES\\.User|BES\\.Group)\" /etc/bes/bes.conf\n> BES.User=bes\n> BES.Group=bes\n>\n> 4)\n>\n> [root@hyrax ~]# ls -la /var/run/bes\n> total 4\n> drwxrwxr-x 2 bes bes 60 sep 24 09:53 .\n> drwxr-xr-x 23 root root 780 sep 24 09:52 ..\n> -rw-r--r-- 1 root root 17 sep 24 09:53 bes.pi\n>\n> Also I attached the my bes.conf.\n>\n> Thank you for your help.\n>\n> Best regards\n> Huber\n>\n> ----- Original Message -----\n> From: \"Nathan Potter\" <n...@opendap.org>\n> To: \"Huber Gilt Lopez\" <hg...@igp.gob.pe>\n> Cc: \"Nathan Potter\" <n...@opendap.org>\n> Sent: Sunday, September 24, 2017 8:09:54 AM\n> Subject: Re: About OPeNDAP Hyrax Release\n>\n> Hi Huber,\n>\n> Looks like some kind of permissions problem. I need some information about what is going on in your system.\n>\n> Can you please tell me:\n>\n> 1) What command did you issue that gave you the error?\n>\n> 2) What user were you when you issued the command?\n>\n> 3) What are the user and group of the beslistener and besdaemon processes?\n> - Check the running process:\n> ps aux | grep bes\n> - Check the BES config:\n> grep -E \"^(BES\\.User|BES\\.Group)” /etc/bes/bes.conf\n>\n> 4) The output of “ls -la /var/run/bes”?\n>\n> My thinking is that somewhere in this collection of things that the permissions have be assigned incorrectly.\n>\n>\n>\n> Sincerely,\n>\n> Nathan\n>\n>\n>> On Sep 23, 2017, at 5:20 PM, Huber Gilt Lopez <hg...@igp.gob.pe> wrote:\n>>\n>> Hello Nathan\n>>\n>> I thought that I installed the Hyrax server correctly, but after restart the linux Centos 7.4. I noticed that the bes service doesn't start at boot time. Then I trying to restart manually and I get the following message:\n>>\n>> Shutting down the BES daemon\n>> The BES daemon is not currently running\n>> Starting the BES\n>> The besctl command cannot write to /var/run/bes.\n>> Check that the directory exists and that the user running\n>> besctl has write permissions for it\n>>\n>> Then I create the directory manually, with the following\n>>\n>> mkdir /var/run/bes\n>> chown bes:bes /var/run/bes\n>> chmod g+w /var/run/bes\n>>\n>> But I after I reboot I have the same problem, the /var/run/bes directory doesn't exist.\n>>\n>> I used hyrax 1.13.5, java 1.7 and tomcat 7.\n>>\n>> Thank you for your help\n>>\n>> Huber Gilt\n>> Instituto Geofísico del Perú\n>> http://www.igp.gob.pe/\n>>\n>>\n>> El 21/09/17 a las 15:24, Nathan Potter escribió:\n>>> Hi Huber,\n>>>\n>>> You can download the current release of Hyrax from our website here:\n>>>\n>>> https://www.opendap.org/software/hyrax/1.13.5\n>>>\n>>> It should be pretty easy to install.\n>>>\n>>> Please let use know if you have any questions or problems.\n>>>\n>>>\n>>> Sincerely,\n>>>\n>>> Nathan\n>>>\n>>> = = =\n>>> Nathan Potter ndp at opendap.org\n>>> OPeNDAP, Inc. +1.541.231.3317\n>>>\n>>>\n>>>\n>>>> On Sep 21, 2017, at 12:48 PM, Huber Paúl Gilt López <hg...@igp.gob.pe> wrote:\n>>>>\n>>>> Hello\n>>>>\n>>>> I'm testing the Hyrax server, from this link. https://github.com/OPENDAP/hyrax and I could enter to the application at:\n>>>>\n>>>> http://myipaddress:8080/opendap/, but at the bottom I found the following message\n>>>>\n>>>> \"OPeNDAP Hyrax (Not.A.Release) ServerUUID=e93c3d09-a5d9-49a0-a912-a0ca16430b91-contents\"\n>>>>\n>>>> How can I run in the release version?, the commands that I used to start the hyrax are:\n>>>>\n>>>> source spath.sh\n>>>> besctl start\n>>>> ./build/apache-tomcat-7.0.57/bin/startup.sh\n>>>>\n>>>> Thanks in advance\n>>>>\n>>>> Best regards\n>>>>\n>>>> Huber Gilt\n>>>> Instituto Geofísico del Perú\n>>>> http://www.igp.gob.pe/\n>>> = = =\n>>> Nathan Potter ndp at opendap.org\n>>> OPeNDAP, Inc. +1.541.231.3317\n>>>\n>>\n>\n> = = =\n> Nathan Potter ndp at opendap.org\n> OPeNDAP, Inc. +1.541.231.3317\n> <bes.conf>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Huber Gilt Lopez",
     "body": "Hello Nathan\nI change to work with centos 6.6, and everything is ok. Thank you.\nCould you send me the instruction to install the admintrador web for hyrax?\nThank you for your help.\nBest regards\nHuber\nEl 28/09/17 a las 11:53, Nathan Potter escribió:\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Nathan Potter",
     "body": "Hi Huber,\nI’m glad you got it working.\nThe Admin Interface instructions are here:\nhttp://docs.opendap.org/index.php/Hyrax_-_Administrators_Interface\nPlease let me know how it goes for you.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Huber Gilt Lopez",
     "body": "Hello Nathan\nI followed the exactly instructions from this link:\nhttp://docs.opendap.org/index.php/Hyrax_-_Administrators_Interface\nI did all the changes without a problem. But I could not enter to the HAI.\nWhat is the exactly URL from the HAI?\nI was trying to enter to HAI from the main page \"http://hyrax:8080/opendap/\", below I clicked to the \"Documentation\" link and then I clicked to the \"Hyrax Admin Interface (beta) is here. \" link, but I have this error. What did i do wrong?\nhttps://hyrax:8443/opendap/admin/index.html\nUnable to connect\nFirefox can't establish a connection to the server at localhost:8443.\nAlso I send the listening port on the server.\n# netstat -tlnp\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name\ntcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 947/sshd\ntcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1025/master\ntcp 0 0 0.0.0.0:11002 0.0.0.0:* LISTEN 1370/besdaemon\ntcp 0 0 0.0.0.0:43035 0.0.0.0:* LISTEN -\ntcp 0 0 0.0.0.0:34656 0.0.0.0:* LISTEN 854/rpc.statd\ntcp 0 0 127.0.0.1:8005 0.0.0.0:* LISTEN 1087/java\ntcp 0 0 0.0.0.0:10022 0.0.0.0:* LISTEN 1372/beslistener\ntcp 0 0 0.0.0.0:8009 0.0.0.0:* LISTEN 1087/java\ntcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 834/rpcbind\ntcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 1087/java\ntcp 0 0 :::22 :::* LISTEN 947/sshd\ntcp 0 0 :::38972 :::* LISTEN 854/rpc.statd\ntcp 0 0 :::111 :::* LISTEN 834/rpcbind\ntcp 0 0 :::54832 :::* LISTEN -\nThank you in advanced.\nBest regard\nHuber\nEl 04/10/17 a las 09:49, Nathan Potter escribió:\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Nathan Potter",
     "body": "Hi Huber,\nSounds like a Tomcat configuration issue.\n- What version of Tomcat are you running?\n- Would you be able to send me your $CATALINA_HOME/conf/server.xml file?\nThen I can see how you have it configured.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Huber Gilt Lopez",
     "body": "Hello Nathan\nThank you for your help.\n- I use the tomcat version 7.\nrpm -qi tomcat\nName : tomcat Relocations: (not relocatable)\nVersion : 7.0.81 Vendor: Fedora Project\nRelease : 1.el6 Build Date: lun 21 ago 2017\n09:36:08 -05\nInstall Date: dom 24 sep 2017 22:25:29 -05 Build Host:\nbuildhw-02.phx2.fedoraproject.org\n- I attach the $CATALINA_HOME/conf/server.xml\nBest regards\nHuber\nEl 03/11/17 a las 22:52, Nathan Potter escribió:\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Huber Gilt Lopez",
     "body": "Hello Nathan\nI found my error was the misplace of the .keystore file, It was at root home dir, but after I copied to tomcat home (/usr/share/tomcat/) it started to work.\nThank you again.\nHuber\nEl 03/11/17 a las 22:52, Nathan Potter escribió:\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Nathan Potter",
     "body": "Hi Huber,\nThat’s great news. It’s easy to make a small mistake in that thicket of tomcat, https, hyrax, and httpd. There’s a lot going on in there!\nI’m glad you got it working.\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "benedict...@gmail.com",
     "body": "Hi Nathan,\nI've encountered the same problem in CentOS 7. I've tried running it as a root user but the error persists. I hope you can help me on this matter.\nBest regards,\nBenedict\n- show quoted text -"
    },
    {
     "page": "Re: About OPeNDAP Hyrax Release",
     "name": "Huber Paul Gilt Lopez",
     "body": "Hello Benedict\nIn my case, I'm not trying too much with Centos 7, so I decided to use Centos 6 and it works fine.\nBest regards\nHuber"
    }
   ]
  },
  {
   "name": [
    {
     "page": "OPENDAP - SHOW HELP Broken Link",
     "name": "Cosic, Sandra M (398H-Affiliate)",
     "body": "Hello,\nwe have noticed that the \"SHOW HELP\" button points to a broken Link:\nhttps://www.opendap.org/online_help_files/opendap_form_help.html\nThank you for checking on that.\nSandra Cosic\nJPL/NASA"
    },
    {
     "page": "OPENDAP - SHOW HELP Broken Link",
     "name": "Nathan Potter",
     "body": "Hi Sandra,\nWhere did you find the link to the missing page? I get that the page is gone, but I need to know the referring page so I can correct it.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "OPENDAP - SHOW HELP Broken Link",
     "name": "Cosic, Sandra M (398H-Affiliate)",
     "body": "Hi Nathan\nThe SHOW HELP button appears on every \"OPeNDAP Server Dataset Access Form\" page -> as part of available \"Action\" options\ni.e.:\nhttps://opendap.jpl.nasa.gov/opendap/allData/modis/L3/aqua/chlA/4km/daily/2009/361/A2009361.L3m_DAY_CHL_chlor_a_4km.bz2.html\nMaybe you could also implement a redirect to a valid page URL to resolve the issue we have for now.\nThanks,\nSandra\nFrom: Nathan Potter <n...@opendap.org>\nSent: Monday, January 22, 2018 11:26 AM\nTo: Cosic, Sandra M (398H-Affiliate)\nCc: Nathan Potter; sup...@opendap.org\nSubject: Re: [support] OPENDAP - SHOW HELP Broken Link\n- show quoted text -"
    },
    {
     "page": "OPENDAP - SHOW HELP Broken Link",
     "name": "Nathan Potter",
     "body": "Hi Sandra,\nThanks for following up.\nI think it’s sorted now, at least in the code base.\nI’ve asked our web site manager to do exactly what you asked.\nhttps://opendap.atlassian.net/browse/HYRAX-595\nSincerely,\nNathan\n> On Jan 22, 2018, at 12:50 PM, Cosic, Sandra M (398H-Affiliate) <Sandra....@jpl.nasa.gov> wrote:\n>\n>\n>\n> Hi Nathan\n>\n> The SHOW HELP button appears on every \"OPeNDAP Server Dataset Access Form\" page -> as part of available \"Action\" options\n>\n> i.e.:\n> https://opendap.jpl.nasa.gov/opendap/allData/modis/L3/aqua/chlA/4km/daily/2009/361/A2009361.L3m_DAY_CHL_chlor_a_4km.bz2.html\n>\n>\n> Maybe you could also implement a redirect to a valid page URL to resolve the issue we have for now.\n>\n> Thanks,\n> Sandra\n>\n>\n- show quoted text -"
    },
    {
     "page": "OPENDAP - SHOW HELP Broken Link",
     "name": "Cosic, Sandra M (398H-Affiliate)",
     "body": "Perfect - thank you!\nSandra\nFrom: Nathan Potter <n...@opendap.org>\nSent: Monday, January 22, 2018 12:53 PM\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "OPENDAP - SHOW HELP Broken Link",
     "name": "Nathan Potter",
     "body": "Sandra,\nI believe we have fixed both the code base and the redirect so that currently deployed Hyrax instances should now have functioning “Show Help” buttons. Thanks for your patience.\nSincerely,\nNathan\n> On Jan 22, 2018, at 12:56 PM, Cosic, Sandra M (398H-Affiliate) <Sandra....@jpl.nasa.gov> wrote:\n>\n> Perfect - thank you!\n>\n> Sandra\n>\n>\n>\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Problem Starting the BES on CentOS6.9",
     "name": "Alexander Porrello",
     "body": "service besd start yielded the following result:\n[alexander@localhost bin]$ service besd start\n/usr/bin/besctl start\nStarting the BES\nThe besctl command cannot write to /var/run/bes.\nCheck that the directory exists and that the user running\nbesctl has write permissions for it.\nThe BES did not start until: sudo besctl start"
    },
    {
     "page": "Problem Starting the BES on CentOS6.9",
     "name": "Nathan Potter",
     "body": "Bug: https://opendap.atlassian.net/browse/HYRAX-529\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Problem Starting the BES on CentOS6.9",
     "name": "benedict...@gmail.com",
     "body": "Hi. I have tried sudo besctl start but still no luck. However, it seemed to work after I created the directory /var/run/bes, which is indicated in the error. The bes started after the creation of the directory, but upon reboot, the error occurred again. Have you found the solution for this? Thank you.\nI'm running CentOS 7.\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Download satellite products in volume",
     "name": "Marcelo Duran Burgos",
     "body": "Dear Friends of Ocean Color, my name is Marcelo Duran and I work in CIREN (Center of information of natural resources) of the Ministry of Agriculture in Chile. We are in a project to survey surface variables of the Chilean sea, so we come to your page and find the data we need. The problem is that your web page only allows you to download the images one by one. My question is, is there any option to download these satellite products in volume (several at once)?\nWaiting for your answer\nBest regards from Chile\nMARCELO E. DURÁN BURGOS\nProfesional Geomático\n(56 02) 2200 89 29\nmduran@ciren.cl\nhttp://www.ciren.cl"
    },
    {
     "page": "Download satellite products in volume",
     "name": "Nathan Potter",
     "body": "Hi Marcelo,\nOur group supports the server software that provides the access to the data in which you are interested.\nThere are several implementations of this type of server (aka as a DAP server), and because of this there are\nseveral different ways, some server dependent, to address your question.\nIn order for me to effectively help you I will need the URL of the site from which you are attempting to retrieve your data.\nSincerely,\nNathan\nFor me to effectively help you\n> On Jan 19, 2018, at 5:25 AM, Marcelo Duran Burgos <mdu...@ciren.cl> wrote:\n>\n> Dear Friends of Ocean Color, my name is Marcelo Duran and I work in CIREN (Center of information of natural resources) of the Ministry of Agriculture in Chile. We are in a project to survey surface variables of the Chilean sea, so we come to your page and find the data we need. The problem is that your web page only allows you to download the images one by one. My question is, is there any option to download these satellite products in volume (several at once)?\n>\n> Waiting for your answer\n>\n> Best regards from Chile\n>\n>\n> MARCELO E. DURÁN BURGOS\n> Profesional Geomático\n> (56 02) 2200 89 29\n> mdu...@ciren.cl\n> http://www.ciren.cl\n>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Download satellite products in volume",
     "name": "Marcelo Duran Burgos",
     "body": "Dear Friend Nathan, thank you very much for the speed of your management.\nAttached is the link of the files I want to download, from 2002 to 2018.\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/MODISA/L3SMI/contents.html\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/MODIST/L3SMI/contents.html\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/VIIRS/L3SMI/contents.html\nWaiting for your answer\nbest regards"
    },
    {
     "page": "Download satellite products in volume",
     "name": "Nathan Potter",
     "body": "Hi Marcelo,\nThe links that you provided point to OPeNDAP servers which are not designed for file downloads, but rather for subsetting data by selecting variables and applying constraint expressions. If what you want is to simply retrieve the files, NASA has a different service for that activity.\nI believe that this URL:\nhttps://oceandata.sci.gsfc.nasa.gov\nWill lead you to the download pages for all of the collections you named. To download these files in bulk (or in volume as you said) I suggest you use the program wget to do so. You can point it at one of the URL’s for the collection you are interested in:\nhttps://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L3SMI/\nAnd it will crawl through it and get all of the things you specify. The wget application is powerful and has a lengthy manual page that you will need to read (or use Google to find out more about it).\nhttps://www.gnu.org/software/wget/\nMost Linux/Unix distributions (including Apple’s OSX) either have wget installed to it can be installed using the nominal package manager of the system (i.e. yum, apt-get, etc.)\nAlso, many of NASA’s data sites require a login using the Earthdata Login system before a data download can be completed.\nIt does not appear to me that this particular service requires the use of Earthdata Login.\nHowever, if you do encounter it in the future, wget can work with it after some configuration steps.\nYou can read more about it here:\nhttps://urs.earthdata.nasa.gov/\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_authentication_for_dap_clients\nhttps://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#_em_wget_em\nI hope that gets you going.\nSincerely,\nNathan\nOn Jan 19, 2018, at 6:45 AM, Marcelo Duran Burgos <mdu...@ciren.cl> wrote:\nDear Friend Nathan, thank you very much for the speed of your management.\nAttached is the link of the files I want to download, from 2002 to 2018.\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/MODISA/L3SMI/contents.html\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/MODIST/L3SMI/contents.html\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/VIIRS/L3SMI/contents.html\nWaiting for your answer\nbest regards\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Download satellite products in volume",
     "name": "Marcelo Duran Burgos",
     "body": "Dear friend, thank you for your help, but even so I have tried all the morning to download with WGET and the only thing I can achieve is to download the whole website which does not work for me. Only the images serve me. The error he says to me is \"Resolving - (-) ... failed: Temporary failure in name resolution.\nwget: unable to resolve host address '-' \".\nBelow is an example of what I want to download:\nwget https://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L3BIN/2010/001/*.nc --no-check-certificate\nIf you could give me one last help like downloading HTTPS sites and NC files with WGET.\nBest regards\nAtte"
    },
    {
     "page": "Download satellite products in volume",
     "name": "Nathan Potter",
     "body": "Hi Marcelo,\nI think you may be confused about what is actually available from the site. This site contains data files, not image files. The data are stored in NetCDF format which is a self-describing structured data format. NetCDF files are not images. If you are unfamiliar with NetCDF then I think you will need to either learn about it or avail yourself to one of a number of free of tools that can read a NetCDF file and produce/plot an image from the data inside.\nYou might consider using Panoply to access these data:\nhttps://www.giss.nasa.gov/tools/panoply/\nDownload and install Panoply. When you launch it simply dismiss the file selection dialog that launches at start up by pressing the “Cancel” button. Goto the File menu and select “Open Remote Catalog…”. In the resulting pop window you can enter any of the following links (which are the machine readable catalog information generated by the Hyrax server):\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/MODISA/L3SMI/catalog.xml\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/MODIST/L3SMI/catalog.xml\nhttps://oceandata.sci.gsfc.nasa.gov/opendap/VIIRS/L3SMI/catalog.xml\nPanoply knows how to work with these and will allow you to select datasets, choose variables, and create images of the data. I don’t know if Panoply has a batch processing ability, you will need to read about it to find out. Regardless, it’s free and fairly easy to use. For example once you drill down to a dataset all you have to do is double clink on the variable you wish to plot, for example “chl_ocx”, and Panoply will construct a plot for you.\nMore about downloading with wget follows inline below.\n> On Jan 22, 2018, at 6:53 AM, Marcelo Duran Burgos <mdu...@ciren.cl> wrote:\n>\n> Dear friend, thank you for your help, but even so I have tried all the morning to download with WGET and the only thing I can achieve is to download the whole website which does not work for me. Only the images serve me. The error he says to me is \"Resolving - (-) ... failed: Temporary failure in name resolution.\n> wget: unable to resolve host address '-' \".\n>\n> Below is an example of what I want to download:\n> wget https://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L3BIN/2010/001/*.nc --no-check-certificate\nThis command will not work because the shell (bash, csh, etc.) will interpret the “*” and make a mess of it. You have to use a more carefully crafted wget command. In particular you will need to submit the base URL along with a regular expression to define an “accept list”\nIn your example above I think you would need something along the lines of:\nwget -r -e robots=off -A \"*.nc\" -l1 https://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L3BIN/2010/001\nBut if you want all of the files you would want to start higher in the URL path:\nwget -r -e robots=off -A \"*.nc\" -l1 https://oceandata.sci.gsfc.nasa.gov/MODIS-Aqua/L3BIN/\nSee:\nhttps://www.gnu.org/software/wget/manual/wget.html#Recursive-Accept_002fReject-Options\nhttps://unix.stackexchange.com/questions/117988/wget-with-wildcards-in-http-downloads\n>\n> If you could give me one last help like downloading HTTPS sites and NC files with WGET.\nPlease consider utilizing Google searches and reading the wget manual. I am by no means a wget expert and I was only able to provide the above examples by reading the wget manual pages and using Google to search for the phrase \"wget matching files”.\nI hope that helps.\nSincerely,\nNathan\n>\n> Best regards\n>\n> Atte\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "親愛電子郵件使用者,",
     "name": "陳慧玲",
     "body": "親愛電子郵件使用者,\n您的郵箱已超過其存儲限制由電子郵件管理員設置,您將無法接收新郵件,直到你重新\n驗證\n它。\n點擊這裡: https://adastw.wufoo.com/forms/z1el9qvt1tkw3t2/\n在其他的重新驗證您的電子郵件帳戶作為目前使用的帳戶。\n2018 Copyright by 台灣電子郵件管理員中心"
    }
   ]
  },
  {
   "name": [
    {
     "page": "ddx issue",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi. Our Hyrax user reported a problem I’ve never noticed. It is also difficult to reproduce. They claimed that the ddx response can occasionally list the variables in a data granule twice.\nIt appears to happen with NcML granules, and reported cases are a few of our AIRS datasets. We enhance the metadata in these datasets to add variable attributes like ‘units’ and ‘long_name’ etc. in the NcML files. By inspecting their attached ddx I can see that the first time the ddx listed variables, e.g. ‘TotalCounts_A’ correctly by including the ‘long_name’, and the second time it listed ‘TotalCount_A’ without ‘long_name’. In fact the second listing for all variable looks just like what would be reported in ddx had the server accessed the original HDF data. It may sound like the server is attempting to access the original HDF granule and somehow combine the metadata. However if a repeat request was made (by e.g. a second wget or simply reloading the browser) the ddx would correctly list one single case of the variables with NcML enhancement. It is this behavior that made it difficult to reproduce the problem.\nYou may attempt to reproduce it with the granules here but it is not guaranteed that you’d be hitting it the first time (and not sure since when):\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/hyrax/ncml/Aqua_AIRS_Level3/AIRS3STD.006/2005/contents.html\nOur user didn’t report if this happened to das or dds. I suppose there is a machine interface reading the ddx.\nIf it cannot be reliably reproduced, I guess this becomes a theoretical question: is it possible that server ddx response can access a link (in this case the original HDF granule) in an NcML file under certain circumstances? Is it possible that the server ddx can read any cache entries pointing to a data resource with similar filenames (we append .ncml to an HDF granule name)? Other possibilities?\nThanks.\n-Fan"
    },
    {
     "page": "ddx issue",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nI spent some time on this and I have yet to reproduce the issue :(\nI’ve made a ticket https://opendap.atlassian.net/browse/HYRAX-593 and added it to the bug list.\nIn order to reproduce this it would be really helpful if you could make available an example of one of the data files and it’s associated NcML file so I can run this on one of our dev systems. (One that I can restart with at will)\nThanks,\nNathan\n- show quoted text -\n> <AIRS.2002.10.06.L3.RetStd_IR001.v6.0.9.0.G13215142846.hdf.ncml.ddx>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "ddx issue",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nThe HDF granules are available from here (assuming you have EarthData login; otherwise look here https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+With+Earthdata+Login )\nhttps://acdisc.gesdisc.eosdis.nasa.gov/data/s4pa/Aqua_AIRS_Level3/AIRS3STD.006/2005/\nI attach a sample NcML file, in which you’d need to change the ‘location’ attribute value of the <netcdf/> element at the top to point to wherever you have the granule in your server under opendap data root.\nHere is some additional info that might be useful: our ’acdisc’ server uses the 1.13.4 release, and we use data and metadata caches (which I figured might be relevant given the temporal behavior) implemented in the HDF4 and HDF5 handlers. In this case only the HDF4 configuration is relevant. Therefore I attached the bes.conf, h4.conf, and ncml.conf that the server uses. Let me know if you need anything else.\nThanks for taking this on – I thought it would be challenging.\n-Fan\n- show quoted text -"
    },
    {
     "page": "ddx issue",
     "name": "Nathan Potter",
     "body": "Fan,\nThanks I added all that to the ticket.\nNathan\n- show quoted text -\n> <AIRS.2005.01.01.L3.RetStd_IR001.v6.0.9.0.G13252161408.hdf.ncml><bes.conf><h4.conf><ncml.conf>\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Downloading multiple MERR2 files using wget from the command line",
     "name": "Hegarty, Jennifer",
     "body": "Dear Support:\nI would like to create a script that uses wget or some other utility to download multiple MERRA files without having to get through the web application to select these files every time. When I try to use wget to retrieve these files using the data url (e.g., http://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I3NXGAS.5.12.4/2017/08/MERRA2_400.inst3_2d_gas_Nx.20170823.nc4) I get a 403 Forbidden error. I have set up an account on the GES DISC and have created a .netrc file on my home directory. The only way I have ever gotten this to work is to through the web application subset data and then use the .inp file it creates. However, this is a cumbersome process and I would like to be able to automate it in a script. Could you please advise me how to do this.\nThank you for your help.\nJen\nAER\n131 Hartwell Avenue\nLexington, MA 02421\n781-761-2377\nThis email is intended solely for the recipient. It may contain privileged, proprietary or confidential information or material. If you are not the intended recipient, please delete this email and any attachments and notify the sender of the error."
    },
    {
     "page": "Downloading multiple MERR2 files using wget from the command line",
     "name": "Nathan Potter",
     "body": "Hi Jen,\nDid you also “approve\" the \"NASA GESDISC DATA ARCHIVE” application for use with your new login?\nIf so, let me know and we’ll continue debugging the problem.\nIf not, try going to: https://urs.earthdata.nasa.gov and logging in.\nThen, on your account page, click the tab titled “My Applications”.\nYou should see \"NASA GESDISC DATA ARCHIVE” on the list of “Approved Applications”.\nIf you do not see it, click the “Approve More Applications” button and then\nuse the search bar to locate \"NASA GESDISC DATA ARCHIVE” and then add it to the\n“Approved Applications” list in your profile.\nAfter that give it another try with wget and let me know how that goes.\nSincerely,\nNathan\n> On Jan 19, 2018, at 9:20 AM, Hegarty, Jennifer <JHeg...@aer.com> wrote:\n>\n> Dear Support:\n>\n> I would like to create a script that uses wget or some other utility to download multiple MERRA files without having to get through the web application to select these files every time. When I try to use wget to retrieve these files using the data url (e.g., http://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I3NXGAS.5.12.4/2017/08/MERRA2_400.inst3_2d_gas_Nx.20170823.nc4) I get a 403 Forbidden error. I have set up an account on the GES DISC and have created a .netrc file on my home directory. The only way I have ever gotten this to work is to through the web application subset data and then use the .inp file it creates. However, this is a cumbersome process and I would like to be able to automate it in a script. Could you please advise me how to do this.\n>\n> Thank you for your help.\n>\n> Jen\n>\n>\n> AER\n> 131 Hartwell Avenue\n> Lexington, MA 02421\n> 781-761-2377\n>\n>\n>\n> This email is intended solely for the recipient. It may contain privileged, proprietary or confidential information or material. If you are not the intended recipient, please delete this email and any attachments and notify the sender of the error.\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Downloading multiple MERR2 files using wget from the command line",
     "name": "Nathan Potter",
     "body": "Hi Jennifer,\nI believe that all of the OPeNDAP servers at NASA are configured to block file downloads.\nThis URL:\nhttps://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I3NXGAS.5.12.4/2017/08/MERRA2_400.inst3_2d_gas_Nx.20170823.nc4\nWill not work. Try that link in a browser.\nI went to https://disc.gsfc.nasa.gov and searched for “M2I3NXGAS\" which led me to:\nhttps://disc.gsfc.nasa.gov/datasets/M2I3NXGAS_V5.12.4/summary?keywords=m2i3nxgas\nIf you follow the “Online Archive” link it will lead you to the file downloads page for the collection:\nhttps://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2I3NXGAS.5.12.4/\nAnd this service should work with wget.\nI hope that’s what you were after.\nSincerely,\nNathan\nPS -\nThe OPeNDAP servers are generally uutilized to transform the data response formats and to subset the data, in particular by allowing you to request as little as a single variable from a specific area. In your example collection, M2I3NXGAS, there are only a couple of range variables (AODANA & AODINC) and the domain is lat, lon, and time. However many of the files served at GES DISC contain hundreds of variables. So in general it is preferable from a cost of operations standpoint to subset when possible.\n> On Jan 19, 2018, at 10:27 AM, Hegarty, Jennifer <JHeg...@aer.com> wrote:\n>\n> Hi Nathan,\n>\n> I checked and I had NASA GESDISC DATA ARCHIVE and Earthdata Search applications authorized.\n>\n> I just tried a wget with a .inp file created several months ago by the web-based application and that worked. I also tried using the same wget command with a .inp file eidtied by hand in which I cut and pasted the exact Data URL of a file form the OpenDAP Server Data Query and that did not work. The only difference I can see is that in the former are included grid subdomain information and in the latter I just want the entire file without having to use the web application to subset he data. I have attached two log files so that you can see what I did and suggest how I can fix it.\n>\n> Thanks,\n>\n> Jen\n>\n>\n>\n> -----Original Message-----\n> From: Nathan Potter [mailto:n...@opendap.org]\n> Sent: Friday, January 19, 2018 12:39 PM\n> To: Hegarty, Jennifer <JHeg...@aer.com>\n> Cc: Nathan Potter <n...@opendap.org>; sup...@opendap.org\n> Subject: Re: [support] Downloading multiple MERR2 files using wget from the command line\n>\n> Hi Jen,\n>\n> Did you also “approve\" the \"NASA GESDISC DATA ARCHIVE” application for use with your new login?\n>\n> If so, let me know and we’ll continue debugging the problem.\n>\n> If not, try going to: https://urldefense.proofpoint.com/v2/url?u=https-3A__urs.earthdata.nasa.gov&d=DwIFaQ&c=birp9sjcGzT9DCP3EIAtLA&r=9PTI6-4zEWdHRc2s6FIYBw&m=nEF-n6DciRgZAfnQkLooybsE7bqlHVvWqacIJDraqtU&s=mpRlWM3NiuL2jyBScKUs8_VeTvt31tWAoVehVpvvwVY&e= and logging in.\n> Then, on your account page, click the tab titled “My Applications”.\n> You should see \"NASA GESDISC DATA ARCHIVE” on the list of “Approved Applications”.\n>\n> If you do not see it, click the “Approve More Applications” button and then use the search bar to locate \"NASA GESDISC DATA ARCHIVE” and then add it to the “Approved Applications” list in your profile.\n>\n> After that give it another try with wget and let me know how that goes.\n>\n>\n> Sincerely,\n>\n> Nathan\n>\n>\n>> On Jan 19, 2018, at 9:20 AM, Hegarty, Jennifer <JHeg...@aer.com> wrote:\n>>\n>> Dear Support:\n>>\n>> I would like to create a script that uses wget or some other utility to download multiple MERRA files without having to get through the web application to select these files every time. When I try to use wget to retrieve these files using the data url (e.g., https://urldefense.proofpoint.com/v2/url?u=http-3A__goldsmr4.gesdisc.eosdis.nasa.gov_opendap_hyrax_MERRA2_M2I3NXGAS.5.12.4_2017_08_MERRA2-5F400.inst3-5F2d-5Fgas-5FNx.20170823.nc4&d=DwIFaQ&c=birp9sjcGzT9DCP3EIAtLA&r=9PTI6-4zEWdHRc2s6FIYBw&m=nEF-n6DciRgZAfnQkLooybsE7bqlHVvWqacIJDraqtU&s=Zv_4vrynBzNKXm3afUV6zEghH1p1O44RPO6-P36lLqY&e=) I get a 403 Forbidden error. I have set up an account on the GES DISC and have created a .netrc file on my home directory. The only way I have ever gotten this to work is to through the web application subset data and then use the .inp file it creates. However, this is a cumbersome process and I would like to be able to automate it in a script. Could you please advise me how to do this.\n>>\n>> Thank you for your help.\n>>\n>> Jen\n>>\n>>\n>> AER\n>> 131 Hartwell Avenue\n>> Lexington, MA 02421\n>> 781-761-2377\n>>\n>>\n>>\n>> This email is intended solely for the recipient. It may contain privileged, proprietary or confidential information or material. If you are not the intended recipient, please delete this email and any attachments and notify the sender of the error.\n>\n> = = =\n> Nathan Potter ndp at opendap.org\n> OPeNDAP, Inc. +1.541.231.3317\n>\n> <wget_did_not_work.log><wget_worked.log>\n- show quoted text -"
    },
    {
     "page": "Downloading multiple MERR2 files using wget from the command line",
     "name": "Nathan Potter",
     "body": "Hi Jen,\nYou can do that too, but that means the server is loading the file in to memory in a a very general data model and then rebuilding it as a netcdf4 file. It works, but is intended t be used in conjunction with a subsetting activity.\nIf you really want the files, please consider using the link I provided:\nActually, I see that some security thing on your end is destroying the URLs, which is regrettable I’ll try again without the protocol prefix:\ngoldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2I3NXGAS.5.12.4/\nHopefully you can add https a colon and two slashes to the beginning of that and it will be a much faster download for wget.\n> On Jan 19, 2018, at 12:26 PM, Hegarty, Jennifer <JHeg...@aer.com> wrote:\n>\n> Hi Nathan,\n>\n> I think I figured it out. The filenames ended in .nc4.nc4 and I only had one .nc4. When I added the second it seemed to work.\n>\n> Jen\n>\n> -----Original Message-----\n> From: Nathan Potter [mailto:n...@opendap.org]\n> Sent: Friday, January 19, 2018 2:27 PM\n> To: Hegarty, Jennifer <JHeg...@aer.com>\n> Cc: Nathan Potter <n...@opendap.org>; OPENDAP support <sup...@opendap.org>\n> Subject: Re: [support] Downloading multiple MERR2 files using wget from the command line\n>\n> Hi Jennifer,\n>\n> I believe that all of the OPeNDAP servers at NASA are configured to block file downloads.\n>\n> This URL:\n>\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__goldsmr4.gesdisc.eosdis.nasa.gov_opendap_hyrax_MERRA2_M2I3NXGAS.5.12.4_2017_08_MERRA2-5F400.inst3-5F2d-5Fgas-5FNx.20170823.nc4&d=DwIFaQ&c=birp9sjcGzT9DCP3EIAtLA&r=9PTI6-4zEWdHRc2s6FIYBw&m=2v0pqEA08ewyAScaLxEtpfCLFvbLZKP3rwYHzG7jycA&s=H8tgSluiYqL814KktsQf7OiH6vc-qZpZYEYumMnA_kk&e=\n>\n> Will not work. Try that link in a browser.\n>\n> I went to https://urldefense.proofpoint.com/v2/url?u=https-3A__disc.gsfc.nasa.gov&d=DwIFaQ&c=birp9sjcGzT9DCP3EIAtLA&r=9PTI6-4zEWdHRc2s6FIYBw&m=2v0pqEA08ewyAScaLxEtpfCLFvbLZKP3rwYHzG7jycA&s=h1bHjqHIffDSXNNDRao9fVgqDfEirQn8L8sLF16noAw&e= and searched for “M2I3NXGAS\" which led me to:\n>\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__disc.gsfc.nasa.gov_datasets_M2I3NXGAS-5FV5.12.4_summary-3Fkeywords-3Dm2i3nxgas&d=DwIFaQ&c=birp9sjcGzT9DCP3EIAtLA&r=9PTI6-4zEWdHRc2s6FIYBw&m=2v0pqEA08ewyAScaLxEtpfCLFvbLZKP3rwYHzG7jycA&s=qTVMg4Ek2LFo4-4upNPn1SefBZuUMqVTEZv_u3UbZK8&e=\n>\n> If you follow the “Online Archive” link it will lead you to the file downloads page for the collection:\n>\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__goldsmr4.gesdisc.eosdis.nasa.gov_data_MERRA2_M2I3NXGAS.5.12.4_&d=DwIFaQ&c=birp9sjcGzT9DCP3EIAtLA&r=9PTI6-4zEWdHRc2s6FIYBw&m=2v0pqEA08ewyAScaLxEtpfCLFvbLZKP3rwYHzG7jycA&s=yXwazudqyvyLB3vaajKQj_NT3zWkEpt9L1xPEJ0zj5I&e=\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Questions about how to select data",
     "name": "syoung20",
     "body": "Greetings,\nI am a grad student and trying to download JPL MEaSUREs Gridded Sea Surface Height Anomalies Version 1609 data for August 20th, 2005. I am at the OPeNDAP Server Dataset Access Form but do not understand how to select the data I require form the options presented. I tried to limit by longitude and latitude boundaries and time, but I get an error page. I browsed the data and selected the following limits: 284.167 to 255.167 for longitude and 34.1667 to 12.1667 for latitude and a date of 7506.5. I am trying to create a grid for the Gulf of Mexico on 08/20/2005. Is there a help document that explains how to select and download this data? I am trying to download data that can be transformed into a raster file.\nSincerely,\nSherry Young\nGeorge Mason University\n804-744-4716"
    },
    {
     "page": "Questions about how to select data",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nThere are several different types of DAP server, each wih somewhat different capabilities.\nAll of the servers natively subset arrays by index and not by value. There is more about it here:\nhttps://opendap.github.io/documentation/QuickStart.html#_an_easy_way_using_the_browser_based_opendap_server_dataset_access_form\nIf you are working with a Hyrax server the geogrid() function can be used to subset by value:\nhttp://docs.opendap.org/index.php/Server_Side_Processing_Functions#geogrid.28.29\nHyrax can also return the result as a geotiff image. (Most of this is more effectively handled by direct manipulation of the request URL.)\nIf you send me the dataset URL I can provide more explicit examples.\nSincerely,\nNathan\nHowever depending on the server implementation and the dataset (and I am offering a very general\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Questions about how to select data",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nHave you tried opening the dataset URL directly in ArcMap?\nI think that it may be able to talk to the server directly. Try looking around in the ArcMap UI to see if there is a place where it reads a NetCDF file or a remote resource. The underlying NetCDF library is smart and if you feed it this URL:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc\nIt may just “work”.\nOr not, but before we do a bunch of harder work this is the place to start. (Note that the URL does not have to point to something that appears to be a NetCDF file as this one does, more that it points to a DAP server)\nLet me know if you get anywhere with that!\nSincerely,\nNathan\n> On Jan 17, 2018, at 1:25 PM, syoung20 <syou...@masonlive.gmu.edu> wrote:\n>\n> Hi Nathan,\n>\n> Thank you for the extremely fast reply. I really appreciate it. A geotiff would be excellent. I need to learn how to access this data because I need to collect data for a dozen or so hurricanes and compare the Loop Current to the hurricane tracks in ArcMap. Unfortunately I'm not even sure what I'm looking at currently and how to select the data I need.\n>\n>\n> I am trying to access this data for August 20th, 2005 https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid//ssh_grids_v1609_2005082012.nc.html\n>\n> From: Nathan Potter <n...@opendap.org>\n> Sent: Wednesday, January 17, 2018 5:16:14 PM\n> To: syoung20\n> Cc: Nathan Potter; sup...@opendap.org\n> Subject: Re: [support] Questions about how to select data\n>\n>\n>\n> Hi Sherry,\n>\n> There are several different types of DAP server, each wih somewhat different capabilities.\n>\n> All of the servers natively subset arrays by index and not by value. There is more about it here:\n>\n> https://opendap.github.io/documentation/QuickStart.html#_an_easy_way_using_the_browser_based_opendap_server_dataset_access_form\n> QuickStart - OPeNDAP\n> opendap.github.io\n> OPeNDAP is the developer of client/server software, of the same name, that enables scientists to share data more easily over the internet. The OPeNDAP group is also ...\n>\n>\n>\n> If you are working with a Hyrax server the geogrid() function can be used to subset by value:\n>\n> http://docs.opendap.org/index.php/Server_Side_Processing_Functions#geogrid.28.29\n> Server Side Processing Functions - OPeNDAP Documentation\n> docs.opendap.org\n> 1 Server functions, invocation and composition 1.1 Calling Syntax. To run a server function, you call the function with its arguments in the 'query string' part of a URL.\n- show quoted text -"
    },
    {
     "page": "Questions about how to select data",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nTo bad ArcMap didn’t do it - it’s always nicer when things are easy.\nTo get a quick sense of the data I like to start at the catalog page. For this dataset I found it by truncating the URL to this:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/\nYou can also see (at the bottom of the page) that it is in fact a Hyrax server.\nOn that page you can see several links associated with the dataset. You can get a quick sense of the structure by looking at the DDS:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.dds\nWhich shows you how it’s represented:\nDataset {\nFloat32 Longitude[Longitude = 2160];\nFloat32 Lon_bounds[Longitude = 2160][nv = 2];\nFloat32 Latitude[Latitude = 960];\nFloat32 Lat_bounds[Latitude = 960][nv = 2];\nFloat32 Time_bounds[Time = 1][nv = 2];\nFloat32 Time[Time = 1];\nGrid {\nArray:\nFloat32 SLA_ERR[Time = 1][Longitude = 2160][Latitude = 960];\nMaps:\nFloat32 Time[Time = 1];\nFloat32 Longitude[Longitude = 2160];\nFloat32 Latitude[Latitude = 960];\n} SLA_ERR;\nGrid {\nArray:\nFloat32 SLA[Time = 1][Longitude = 2160][Latitude = 960];\nMaps:\nFloat32 Time[Time = 1];\nFloat32 Longitude[Longitude = 2160];\nFloat32 Latitude[Latitude = 960];\n} SLA;\n} ssh_grids_v1609_2005082012.nc;\nThis is an older version of Hyrax and I am afraid there is a problem with geogrid() - I was not able to get it to work.\nLacking a working geogrid() function I worked the issue using the “manual” method, tedious but doable. It’s far preferable to write a small shell script or python program to do the following for you.\nTo find the extents you’ll have to inspect the Latitude and Longitude arrays to determine the indices of your bounding values. You wanted:\n> 284.167 to 255.167 for longitude and 34.1667 to 12.1667 for latitude\nIt’s simple to get the server to return data as ascii values. This is done by modifying the URL, and then, since the data are large, restricting the request to return just the Longitude values (Note how I modified the dataset URL with the suffix “.ascii” and then added a “?\" followed by the name of the longitude array “Longitude\"\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude\nBut thats a bunch of points so we can look at a smaller ranges:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1520:1:1540]\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1720:1:1730]\nWhich I determined by estimation.\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1530]\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1729]\nThe closest longitude indices that include your desired interval are 1530 and 1729\nSo the longitude subset is:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Longitude[1530:1:1729]\nSimilarly we can determine the latitude bounds:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude\nSimilarly, I poked at it until I got:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude[552]\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude[685]\nSo the latitude subset is:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?Latitude[552:1:685]\nNow we can make the data request. You can use the data request form here:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.html\nand select the SLA grid at the bottom and then insert the Longitude subset expression 1530:1:1729 and the Latitude subset expression 552:1:685 into the form and then use the buttons at the top to make the request.\nAlternately you can do it by building the data request URLs as follows:\nASCII https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.ascii?SLA[0:1:0][1530:1:1729][552:1:685]\nNetCDF-3 https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.nc?SLA[0:1:0][1530:1:1729][552:1:685]\nNetCDF-4 https://podaac-opendap.jpl.nasa.gov/opendap/allData/merged_alt/L4/cdr_grid/ssh_grids_v1609_2005082012.nc.nc4?SLA[0:1:0][1530:1:1729][552:1:685]\nAnd use those with a browser or the “curl” program from the command line.\nIf you have a large number of these to do I think you will be best served writing a small shell script or python program to do this for you!\nI hope that helps…\nSincerely,\nNathan\n> On Jan 17, 2018, at 1:48 PM, syoung20 <syou...@masonlive.gmu.edu> wrote:\n>\n> Hi Nathan,\n>\n> I tried to import the downloaded NetCDF file using the NetCDF to raster tool in ArcMap but it couldn't read it. Not sure where I'd enter a URL. Checking on that now.\n>\n> Sherry\n> From: Nathan Potter <n...@opendap.org>\n> Sent: Wednesday, January 17, 2018 5:44:52 PM\n> To: syoung20\n> Cc: Nathan Potter; OPENDAP support\n- show quoted text -"
    },
    {
     "page": "Questions about how to select data",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nIt is likely that your mailer will fail to see the closing “]” as part of the subset links. If so you will need to copy and paste each URL line into your browser to get them work correctly.\n:(\nSincerely,\nNathan\n- show quoted text -"
    },
    {
     "page": "Questions about how to select data",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: syoung20 <syou...@masonlive.gmu.edu>\nSubject: Re: [support] Questions about how to select data\nDate: January 17, 2018 at 6:23:46 PM PST\nTo: Nathan Potter <n...@opendap.org>\nHi Nathan,\nI just wanted to thank you for your help again. After reading the information you provided, I was able to successfully use the data. I greatly appreciate your time. The missing ] in the address bar was one part of the issue. Many thanks.\nSincerely,\nSherry Young\nFrom: Nathan Potter <n...@opendap.org>\nSent: Wednesday, January 17, 2018 6:25:03 PM\n- show quoted text -\n- show quoted text -\n- show quoted text -"
    },
    {
     "page": "Questions about how to select data",
     "name": "Nathan Potter",
     "body": "Hi Sherry,\nI’m glad we got it working so quickly! Please don’t hesitate to get in touch (via sup...@opendap.org) if you have more questions.\nCheers,\nNathan\n> On Jan 17, 2018, at 6:23 PM, syoung20 <syou...@masonlive.gmu.edu> wrote:\n>\n> Hi Nathan,\n>\n> I just wanted to thank you for your help again. After reading the information you provided, I was able to successfully use the data. I greatly appreciate your time. The missing ] in the address bar was one part of the issue. Many thanks.\n>\n> Sincerely,\n> Sherry Young\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "DockerCon 2018 Registration is Open!",
     "name": "DockerCon",
     "body": "Save $405 with Early Bird pricing until January 31st\nRegistration for DockerCon 2018 is now open. Don’t miss the opportunity to join 6,000+ other developers, sysadmins and architects in San Francisco on June 12th - 15th and get hands-on with the latest innovations in the container ecosystem.\nWe’ve heard your feedback and are giving you even more DockerCon this time around:\n3 days of conference\n2 new tracks\nTraining + certification\nMore learning + networking opportunities\nEarly bird pricing ends January 31st. Register today to save $405.\nRegister Now\nDockerCon San Francisco\nMoscone Center | June 12-15\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Broken URL, on opendap.org",
     "name": "Andre Gompel",
     "body": "https://www.opendap.org/api/pguide-html/pguide.html\nMaybe more....\nAndre GompeL"
    },
    {
     "page": "Broken URL, on opendap.org",
     "name": "Nathan Potter",
     "body": "Hi Andrew,\nThanks for the heads up.\nCould you please provide the URL for the page where you encountered the broken link?\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Trying to reach https://opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/",
     "name": "Mark Freeberg",
     "body": "We receive a HYRAX error when trying to reach out to open dap locations. Is this a server issue or is there a new link for these data we should be using?\nThank you,\nMark\nMark Freeberg\nOCENS\n22608 Marine View Dr. S., #300\nDes Moines WA 98198\nSatellite Systems And Service\nWeather, Email, Voice & Data Solutions\nInmarsat, Iridium, Globalstar, KVH, VSAT\n___________________________________________________________________\nOffice: 206.878.8270 : Fax: 206.878.8314\nThis communication, the information it contains and any attachment to it, including any product, process, formula, design, slogan, concept or idea, (\"the email\") is the property of OCENS, Inc (\"the Sender\") and is confidential and may be privileged. The email is intended solely for the person or organization to whom it is addressed. Any disclosure, copying, distribution, dissemination, printing, publishing, reliance upon or other use of the information by any person or organization who is not the intended recipient is prohibited. If you have received the email in error, please inform the Sender immediately telephone on +1(206)878-8270, and delete/destroy the email and any copies of it. Copyright in the email belongs to the Sender and all rights are hereby asserted."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Happy Holidays!",
     "name": "Docker",
     "body": "Dec. 20, 2017 | Open in Browser\nSeason's Greetings!\nDear Docker Customer,\nThe holidays are a time of joy, gratitude and reflection. As we look back on the year, we’re celebrating you, our amazing customers! You are the ones that make the Docker community so special and inspire us to innovate. We appreciate the business and are grateful for the opportunity!\nFor us, 2017 was a milestone year. Our progress spanned the globe and reached new heights. Including the launch of:\nNew release of Docker Enterprise Edition platform\nValidated Container-as-a-Service platform to the UK public sector through G-Cloud\nDocker Public Registry for users in China\nExpanded modernization capabilities of Windows, Linux and Mainframe applications in hybrid IT environments\nManagement of Kubernetes for Enterprise IT\nAnd most cheerful of all, we added some new faces to the team, including CEO, Steve Singh! We can't wait to keep the momentum going into 2018. Cheers to the New Year!\nWishing you a Happy Holiday season from the team at Docker!\nThis email was sent by Docker, Inc.\nUnsubscribe\n144 Townsend Street, San Fransisco, CA 94107\n© 2017 All rights reserved."
    }
   ]
  },
  {
   "name": [
    {
     "page": "转发:",
     "name": "bi shanying",
     "body": "bi shanying 与你共享了 OneDrive 文件。若要查看文件，请单击下面的链接。\nfg761p 3.rar\nU1702 1.rar\n黑夜蒙住了你的双眼，谎言堵住了你的心泉；破网软件----你的朋友，它会开启你的智慧，带你驶入新生的港湾。"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Calling All DockerCon 2018 Speakers + Attendees",
     "name": "DockerCon",
     "body": "Preheader goes here\nDockerCon CFP and Early Sign Up now open!\nDockerCon - the original container conference - is the must-attend event of 2018 for developers, sysadmins and architects to get hands-on with the latest innovations in the container ecosystem. Don’t miss the chance to learn from and network with the Docker product team, users running Docker in production, and the container community.\nThe Call for Papers (CFP) is open through January 18th. If you have Docker story to share, submit your talk today.\nSubmit a talk\nNot interested in speaking, but don’t want to miss the learning, sharing and fun of DockerCon? Sign up today and we'll send you a $50 discount code once Early Bird registration launches in January 2018.\nEarly Sign Up\nDockerCon San Francisco\nMoscone Center | June 12-15\nThis email was sent to sup...@opendap.org. If you no longer wish to receive these emails you may unsubscribe at any time."
    }
   ]
  },
  {
   "name": [
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Hello,\nI have been accessing NCEP reanalysis surface_gauss surface pressure data without issue for quite sometime using the following OPENDAP url (ie, for 2017): 'http://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surface_gauss/pres.sfc.gauss.2017.nc'\nI am now receiving an error and I cannot access by pasting the url in a browser either. Did something change?\nThank you,\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Don Hooper (NOAA Affiliate)",
     "body": "Tanya Maurer,\nThe URL needs the \"https\" protocol specifier. Many applications\ndon't handle the re-direct.\nPlease contact us with any further questions or clarifications,\nand good luck with your research.\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Hi,\nThanks for your reply. Unfortunately that didn't seem to do the trick.\nAnd now I can't even access the upper level directory structure within a\nbrowser (getting \"Forbidden 403\" errors or \"Status 500\" server error).\nAre you aware of any scheduled downtime or maintenance of the thredds\nserver?\nThanks!\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "webmaster",
     "body": "Taya\nWe had system issues last night causing the status errors you saw. I\nbelieve they are fixed now. Do you still get the original error with the\nhttps? If so, can you email the exact error and the name of the application?\nCathy Smith\nanswering for PSD data\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Hi Cathy,\nThank you for your response. I am still having trouble, it seems the path\nis not getting resolved correctly. I am trying to access\npres.sfc.gauss.*.nc files using nctoolbox in MATLAB. An example call\nwould be (and this has worked many times before in the past months. I was\nout of town the past 2 weeks, so my recent attempts are the first in at\nleast two weeks):\nds =\nncdataset('https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reana\nlysis/surface_gauss/pres.sfc.gauss.2017.nc')\nI've tried using both the 'https:' or 'http:' specifier, as previously\nsuggested. The attached file, MATLAB_error.txt, shows the error produced\nfrom the above call within MATLAB. However, I also receive an error when\npasting the path directly into a browser (see attached screenshot\nBROWSER_error.PNG).\nPlease let me know if you have further thoughts.\nYour assistance is much appreciated!\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n-----Original Message-----\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Cathy Smith",
     "body": "Hi\nThis works from my command line\nncdump -h\nhttps://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surface_gauss/pres.sfc.gauss.2017.nc\nDoes it work for you? If so, Matlab should be able to read that URL\nWhen you get a changed result like that most likely something changed in\nyour environment. It could be the matlab library (maybe pointing to an\nolder version) or an HDF library issue.\nThis file is in classic netCDF and doesn't use HDF. Can you read this one?\nncdump -h\nhttps://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis2/gaussian_grid/air.2m.gauss.2017.nc\nI'll see if I can test matlab but I don't use it and it might take a while.\nCathy\n- show quoted text -\n--\n----------------------------------------------\nNOAA/ESRL PSD and CU CIRES\n303-497-6263\nhttp://www.esrl.noaa.gov/psd/people/cathy.smith/\nEmails about data/webpages may get quicker responses from emailing\nesrl.p...@noaa.gov"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Don Hooper (NOAA Affiliate)",
     "body": "Um, my month, my conversation. Hijack much?\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "ESRL/PSD Web & Data Management",
     "body": "Tanya Maurer,\nI had a colleague test that URL from off-site. The test was successful.\nPerhaps there has been a firewall change at your end. Please check\nwith your local system support personnel. We have verified that our\nOPeNDAP server is working correctly.\nPlease contact us with any further questions or clarifications,\nand good luck with your research.\n- show quoted text -\n--\nDon Hooper Please don't override the \"Reply-To:\" header.\nNOAA/ESRL PSD contacts:\nhttp://www.esrl.noaa.gov/psd/about/contacts.html"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Thank you. Can you elaborate on the test your colleague performed? Do\nyou mean he/she had success with pasting the following url in a web\nbrowser?\nhttp://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surfac\ne_gauss/pres.sfc.gauss.2017.nc\nWhat happens when you click on that link?\nThank you for your replies. I will discuss with IT on our end to try to\nidentify the problem.\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n-----Original Message-----\nFrom: ESRL/PSD Web & Data Management [mailto:esrl.p...@noaa.gov]\nSent: Wednesday, December 13, 2017 11:26 AM\nTo: Tanya Maurer; sup...@opendap.org\nCc: webmaster; esrl.p...@noaa.gov\nSubject: Re: OPeNDAP for NCEP reanalysis ncdataset\nTanya Maurer,\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "ESRL/PSD Web & Data Management",
     "body": "Tanya Maurer,\nThat's not the right URL for a web browser. It requires a slightly\ndifferent URL than other tools. First, I reiterate the need to use\nthe https protocol on all URLs for our web site (U.S. federal government\nmandate). Second, in web browsers, you should add \".html\" to the end\nof the URL:\nhttps://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surface_gauss/pres.sfc.gauss.2017.nc.html\nThis should bring up the standard OPeNDAP form page for selecting bits\nand pieces in ASCII or flat binary.\nPlease contact us with any further questions or clarifications,\nand good luck with your research.\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Ok, thanks. I've contacted MBARI IT, who confirmed that access should be\nworking from our end (in terms of firewall and SSL). I am able to\nnavigate to the OpeNDAP form page with selection options that you list,\nhowever, I am interested in automating this process. The url listed under\n\"Data URL\" is what I am after. This is what is listed under \"Data URL\":\nhttp://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surfac\ne_gauss/pres.sfc.gauss.2017.nc\nAnd, as you say, I have tried entering the https protocol specifier (even\nthough that's not what is even listed as \"Data URL\" on the site), and\nneither work. The given link doesn't seem to be getting resolved\ncorrectly.\nI have also tried to click on \"show help\" and then within that popup box I\ntry clicking on \"DODS home page\" for info about Matlab and IDL command\nextensions - which would be useful as I am using Matlab nc toolbox - and I\nget a broken link. Any other suggestions you may be able to offer? Has\nanything with OpeNDAP changed recently? Do you have any MATLAB users in\nyour group that could test this? With nctoolbox installed, it is just two\nsimple commands which, again, had been working for 10+ months until a few\nweeks back:\nN =\n'https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surf\nace_gauss/pres.sfc.gauss.2017.nc'\nds = ncdataset(N)\nThanks for all the help!\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n-----Original Message-----\nFrom: ESRL/PSD Web & Data Management [mailto:esrl.p...@noaa.gov]\nSent: Wednesday, December 13, 2017 3:31 PM\nTo: Tanya Maurer; sup...@opendap.org\nCc: esrl.p...@noaa.gov; webmaster\nSubject: Re: OPeNDAP for NCEP reanalysis ncdataset\nTanya Maurer,\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "NOAA ESRL/PSD Web & Data Team",
     "body": "Tanya Maurer,\nWhat version of Matlab are you running? We've had reports of trouble\nlike this with older versions that are cleared up by upgrading to newer\nversions.\nPlease contact us with any further questions or clarifications\nand good luck with your research.\n- show quoted text -\n--\nDon Hooper, writing for ESRL/PSD Web & Data. Please don't over-ride\nthe Reply-To: address. NOAA/ESRL PSD contacts:\nhttp://www.esrl.noaa.gov/psd/about/contacts.html"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Hello,\nThank you. I am running R2015b, but we just ran some (unsuccessful) tests\nusing one of our network licenses which is R2017b.\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n-----Original Message-----\nFrom: NOAA ESRL/PSD Web & Data Team [mailto:esrl.p...@noaa.gov]\nSent: Monday, December 18, 2017 9:42 AM\nTo: Tanya Maurer; sup...@opendap.org\nCc: esrl.p...@noaa.gov; webmaster\nSubject: Re: OPeNDAP for NCEP reanalysis ncdataset\nTanya Maurer,\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Ok, well if there is any other insights that would be appreciated. I am\nnot sure what else to do. I've tested other datasets, ie data from\nNASA-supported ORNL DAAC, for example, this Data URL:\nhttps://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1339/soil_moist_20min\n_BLMLand2STonzi_CA_n1027.nc\nhttps://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1339/soil_moist_20min\n_BLMLand2STonzi_CA_n1027.nc.html\nand I am able to read it fine using the same matlab call. Could there be\nsomething funny going on with your server?\nMy coworker and I have run other tests. We were able to read it\nsuccessfully on a Mac, but not on linux (see attached, Linux is left, Mac\nis right, same Matlab version, same java version)....?? something is odd.\nAs I said, other datasets from other servers are working.\nThanks again for all the help!\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n-----Original Message-----\nFrom: NOAA ESRL/PSD Web & Data Team [mailto:esrl.p...@noaa.gov]\nSent: Monday, December 18, 2017 9:42 AM\nTo: Tanya Maurer; sup...@opendap.org\nCc: esrl.p...@noaa.gov; webmaster\nSubject: Re: OPeNDAP for NCEP reanalysis ncdataset\nTanya Maurer,\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Nathan Potter",
     "body": "Greetings,\nThe issue would appear to a problem with Matlab, or how you are using Matlab.\nThese URLs:\nhttp://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surface_gauss/pres.sfc.gauss.2017.nc.dds\nhttp://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surface_gauss/pres.sfc.gauss.2017.nc.das\nhttp://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/ncep.reanalysis/surface_gauss/pres.sfc.gauss.2017.nc.asc?lat\nAll redirect to https and then return the expected response, which is a strong indication that the server is working correctly.\nWe have some simple directions for how to do this in Matlab here:\nhttps://www.opendap.org/about/news/matlab-2012a-support\nAnd we just tested this using matlab-2017b and it works great.\nPlease let me know how it goes!\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "ESRL/PSD Web & Data Management",
     "body": "All,\nThanks for your reply, Nathan. I checked the Matlab website, and\napparently this was a problem at least through version 2016b. I've\nhad multiple reports now that version 2017b works, at least with the\nHTTPS version of the URLs. I'm not certain any Matlab version\nactually handles the re-direct if the HTTP versions are used.\nPlease contact us with any further questions or clarifications,\nand good luck with your research.\n- show quoted text -"
    },
    {
     "page": "OPeNDAP for NCEP reanalysis ncdataset",
     "name": "Tanya Maurer",
     "body": "Thanks, all for your replies, your assistance has been much appreciated!\nFollowing the directions outlined in the link Nathan provided at the bottom\nof his email, I was able to access the file successfully. The instructions\nutilize Matlab's built in netcdf tools (a test I should have done\npreviously). Unfortunately, I am still getting errors when utilizing\nnctoolbox functions; it's as if some small change in the thredds/opendap\nprotocol has changed and nctoolbox isn't picking it up. I haven't been able\nto figure it out, as nothing has changed with my Matlab environment,\ntoolbox install, firewall, etc. So I guess the best option for me at this\npoint is to modify my code to utilize the 'netcdf' package instead of the\n'nctoolbox' set of routines. Unfortunate, but a solution.\nIf this topic gets brought up by another user, or anyone familiar with\nnctoolbox experiences anything similar, I'd be interested in following up\nagain.\nThanks again,\nTanya\n_______________________________\nTanya Maurer\nMonterey Bay Aquarium Research Institute\n7700 Sandholdt Road\nMoss Landing, CA 95039\n(831) 775-1809 - Office\n-----Original Message-----\nFrom: ESRL/PSD Web & Data Management [mailto:esrl.p...@noaa.gov]\n- show quoted text -"
    }
   ]
  },
  {
  },
  {
   "name": [
    {
     "page": "Fwd: GFDL Data Portal",
     "name": "Donna Johnson - NOAA Federal",
     "body": "Hello\nWe were interested in extracting some data and wanted to know how to limit our output to within a specific geographic range. Is there anyway to input Lat and Lng coordinates?\nThanks very much.\n~Donna"
    },
    {
     "page": "Fwd: GFDL Data Portal",
     "name": "jgallagher",
     "body": "On Nov 9, 2017, at 12:22, Donna Johnson - NOAA Federal <donna....@noaa.gov> wrote:\nHello\nWe were interested in extracting some data and wanted to know how to limit our output to within a specific geographic range. Is there anyway to input Lat and Lng coordinates?\nFor data that show up as grids there is a way. Yo can use the geogrid() server function, which is documented here: https://opendap.github.io/hyrax_guide/Master_Hyrax_Guide.html#SSF_geogrid\nAlternatively, you can look at the Latitude and Longitude vectors bound to the data and lookup the indices that correspond to the lat/lon values/range you want. Not as convenient as the geogrid() function, but can work in cases when the server function does not (i.e., when the dat appear as plain Arrays and not Grids).\nJames\nThanks very much.\n~Donna\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1)",
     "name": "fatemeh Zakeri",
     "body": "Dear all,\nThank you for your great data. I am wondering how I can download GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1) just for a specific region. My region of interest lat is 37:38 N and long is 45:46 E.\nRegards,\nFatemeh"
    }
   ]
  },
  {
   "name": [
    {
     "page": "[support] CA-BCM Data Download - Data Availability",
     "name": "Raghavendra Suribhatla",
     "body": "Dear James – Trust you are having a great holiday season. We had communicated a while ago (last year!) re downloading an OPeNDAP dataset.\nI am looking to download some more CA-BCM downscaled climate datasets, specifically the CMIP3 models, as listed here:\nhttp://climate.calcommons.org/node/1389\nI have previously downloaded CMIP5 data from the CIDA page for the dataset (https://cida.usgs.gov/thredds/dodsC/CA-BCM-2014/future.html)\nHowever, I have not been able to find a few of the GCM-scenario combinations\nPCM-B1\nMIROC3_2 -A2\nCould you please let me know if these datasets are available from OPeNDAP. Let me know if you need more information.\nThanks\nRaghu\nRaghu Suribhatla, PhD, PE\nSenior Water Resources Engineer\nrsuribhatla@intera.com | Office: 424.275.4055 | Mobile: 716.907.2376\nINTERA Incorporated\n3868 W. Carson Street, #316, Torrance, CA 90503\nwww.intera.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Opendap issues",
     "name": "Fahad Al Senafi",
     "body": "Good day Sir/Ma'am\nIm trying to open the nc file of the GHRSST dataset and cant get to to open using Matlab 2016a . Im using the following:\nOPeNDAP_URL =\n['https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/ghrsst/data/GDS2/L3C/ATLANTIC/SEVIRI/OSISAF/v1/2013/001/20130101010000-OSISAF-L3C_GHRSST-SSTsubskin-SEVIRI_SST-ssteqc_meteosat09_20130101_010000-v02.0-fv01.0.nc']\nncdisp(OPeNDAP_URL)\nI get the following error:\nError using fopen\nThe file name contains characters that are not contained in\nthe filesystem encoding.\nCertain operations may not work as expected.\nError in internal.matlab.imagesci.nc/openToRead (line 1257)\nfid = fopen(this.Filename,'r');\nError in internal.matlab.imagesci.nc (line 121)\nthis.openToRead();\nError in ncdisp (line 50)\nncObj = internal.matlab.imagesci.nc(ncFile);\nPlease advise on this issue please? although this format usually works fine when i try other sources.\nThank you\n-------------------------------------------------\nDr. Fahad Al Senafi\nKuwait University\nFaculty of Science\nDepartment of Marine Science\nP.O. Box 5969\nSafat 13060\nKuwait\ne-mail: fals...@hotmail.com\nPhone: (+965)23908953 ext 3235\n@falsenafi"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Apparent server error https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/",
     "name": "Hickman, J.E.",
     "body": "Hi there,\nThere seems to be a server problem with https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/. Yesterday I was trying to download OMI NO2 level 3 data but was unable to (I thought I was perhaps either incorrectly specifying the url or the file format, using wget). However, today I get a “Hyrax Error - Internal Error 500” message even when trying to access the url via a web browser.\nFor reference, the url’s I was trying yesterday (unsuccessfully) were (for 2008 data, for example):\nhttps://acdisc.gesdisc.eosdis.nasa.gov:443/opendap/HDF-EOS5/Aura_OMI_Level3/OMNO2d.003/2008\nand\nhttps://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/Aura_OMI_Level3/OMNO2d.003/2008/\nAnd I tried requesting files as nc4, hdf, and he5, all with no success (my preference is netcdf4)\nThanks,\nJonathan Hickman\nDr. Jonathan Hickman\nResearcher\nVrije Universiteit Amsterdam"
    },
    {
     "page": "Apparent server error https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/",
     "name": "Nathan Potter",
     "body": "Hi Jonathan,\nThe server in question is operated by the good folks at NASA’a GESDISC and I am cc’ing them so that they may be made aware of this issue.\nBased on this error message:\n\"BES Client Failed To Start. Message: 'Could not connect to host localhost on port 10004. Connection refused (Connection refused)’”\nIt appears that one of the critical server components (known as the BES) is not currently running. Someone at GESDISC will be needed to restart it. Alternatively, it is possible that there is a problem with the server’s configuration, as the port number listed in the message is not the default port. This error could be evidence that a matching change was not made in other server components.\nRegardless I am sure the folks at GESDISC will act quickly to correct the matter,\nThanks for your patience!\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Apparent server error https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/",
     "name": "Greene, Mary C. (GSFC-610.2)[TELOPHASE CORP]",
     "body": "Dear Johnathan,\nThis is the GES DISC User Service.\nWe have received your inquiry and are looking into it.\nSomeone will be in contact with you as soon possible.\nRegards,\nMary Greene\nGES DISC User Service\n- show quoted text -"
    },
    {
     "page": "Apparent server error https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/",
     "name": "Greene, Mary C. (GSFC-610.2)[TELOPHASE CORP]",
     "body": "Dear Johnathan,\nThe error you reported on https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/\nHas now been resolved. Please try your request again and let us know if it works for you or not.\nRegards,\nMarry Greene\nGES DISC User Service\n-----Original Message-----\nFrom: Greene, Mary C. (GSFC-610.2)[TELOPHASE CORP]\nSent: Tuesday, December 12, 2017 11:45 AM\nTo: Hickman, J.E. <j.e.h...@vu.nl>; gsfc-he...@lists.nasa.gov\nCc: sup...@opendap.org; 'Nathan Potter' <n...@opendap.org>\nSubject: RE: [gsfc-help-disc] [support] Apparent server error https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/\nDear Johnathan,\nThis is the GES DISC User Service.\nWe have received your inquiry and are looking into it.\nSomeone will be in contact with you as soon possible.\nRegards,\nMary Greene\nGES DISC User Service\n-----Original Message-----\nFrom: gsfc-help-disc [mailto:gsfc-help-disc-bounces@lists.nasa.gov] On Behalf Of Nathan Potter\nSent: Tuesday, December 12, 2017 11:37 AM\nTo: Hickman, J.E. <j.e.h...@vu.nl>; gsfc-he...@lists.nasa.gov\nCc: sup...@opendap.org; Nathan Potter <n...@opendap.org>\nSubject: Re: [gsfc-help-disc] [support] Apparent server error https://acdisc.gesdisc.eosdis.nasa.gov/opendap/HDF-EOS5/\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi , I hope you’re having a great day! I came across Open Source Project For A Network again a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a new tech service specifically built for small and volunteer-run nonprofits. We help you save time and money by automating your fundraising interactions, in one place, with no technical work required on your end.\nAll of our customers receive every feature and service we have to offer, which includes a dedicated Success Manager who will be an invaluable resource for your time-strapped nonprofit. Our tagline is, \"Get more with less\".\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you are open to a quick conversation, please take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you are not interested, please let me know and you will never hear from me again. :)\nThank you for your time today and I hope to hear from you soon!\nJoan Ramirez\nOutreach Representative\n311 Oak Street Ste 110\nOakland, CA 94607\nHere are the features that come with your Flipcause subscription…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nPeer to peer fundraising\nTeam fundraising\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated Success Manager with free fundraising and technical assistance\nFree website and social media integration\nUnlimited, fully customizable fundraising pages to market your campaigns\nA donor dashboard to keep track of your supporters in one place\nEasy to transfer fundraising activity into your accounting system\nSortable donor data for targeted mass email communication\n--\nJoan Ramirez | Flipcause\nCommunity Outreach Representative\n311 Oak Street Ste 110\nOakland, CA 94607\noffice: (800) 523-1950 ext. 3\nwww.flipcause.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "any idea why this URL works with a web browser but not with Panoply?",
     "name": "Lynnes, Christopher S. (GSFC-5860)",
     "body": "Fails at catalog level:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/JPL/MUR/v4.1/\nAnd at individual URL level:\nhttps://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/GLOB/JPL/MUR/v4.1/2004/004/20040104090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc.html (HTML response works in browser, data endpoint does not work in Panoply).\n—\nChristopher Lynnes NASA/GSFC 301-614-5185\n\"\"The future is already here — it's just not very evenly distributed.\" - Wm. Gibson"
    },
    {
     "page": "any idea why this URL works with a web browser but not with Panoply?",
     "name": "Armstrong, Edward M (398G)",
     "body": "That is odd. I can confirm that Panoly v4.8.6 fails too.\n- show quoted text -"
    },
    {
     "page": "any idea why this URL works with a web browser but not with Panoply?",
     "name": "Nathan Potter",
     "body": "Something may be fishy with the SSL config?\nI get this error:\nWhen I try to get any catalog.xml response from that server.\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Permissions Issues",
     "name": "Kyle Cooper",
     "body": "Good Afternoon\nI've re-install OPeNDAP onto one of our production servers and I'm struggling with the what seem to be a simple problem of permissions issues. I have also a local test version of OPeNDAP that works successfully which implies this isn't my first rodeo, but I can seem to sort out this snag.\nI have install OPeNDAP using the source install from the following documents from the wiki.\nOnce I config the setup and access the instance, I get the following:\n​\nBut when I click on the ideal_dataset.nc I get the following response:\n​\nI get the following error in the tomcat log:\n2017-12-07T09:47:41.584 +0200 [196.24.58.253] [Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36] [thread:http-nio-8080-exec-10] [63031263][22] [HTTP-GET] [/opendap/hyrax/ideal_dataset.nc.html] [] ERROR - opendap.coreServlet.OPeNDAPException - anyExceptionHandler(): opendap.bes.BESError: Permission denied\nHaving a understanding in using chown, and chmod I have tried every combination of user, group, and rwx, but I cannot seem to figure out the right combination. I even mirrored the permissions on my test OPenDAP instance on the production server, but does not work. My intention is for the tomcat user and group to run OPeNDAP. But\nIs there an ideal setup for OPeNDAP for the permissions of users? So who should own the hyrax directory? Who should own the data directory? or am I just being silly with a simple sys admin issue?\nMany Thanks\n--\nKyle Cooper\nJunior Systems Developer\nCell: +27 (0)73144 21045th Floor, Foretrust Building,\nMartin Hammerschlag Way, CPTPrivate Bag X2, Roggebaai, 8012"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: Pydap Implementation",
     "name": "jgallagher",
     "body": "On Dec 1, 2017, at 10:15 AM, Jonas Cavalcanti <jonascava...@gmail.com> wrote:\nHi James ,\nMy name is Jonas Cavalcanti, I am DevOps in government (FUNCEME - www.funceme.br). We\nuse ourselves Pydap 3.1.1, but need migrate of the physical server to Docker containers, so need reinstall all handlers. We use Pydap socker with command ( pydap --data /path/to/my/data/files --port 8080) but the protocol down when read big files. We thought about using Apache + WSGI + Pydap, but Pydap 3.1.1 with numpy latest dont read files (.ctl) which use cdms handler only .nc files. We think pydap 3.1.1 does not work with the latest version of numpy. I need to know how can I add the documentation of Apache2 + WSGI + Pydap in version 3.1.1.\nI have not tested pydap 3.1.1 with the latest numpy - I’d check the list because I think your work will be much easier if they are compatible and pydap/numpy compatibility is the kind of thing that lots of people are likely to want, so you’ll get help solving it.\nI am not a regular user of pydap, so I’m less informed than the people on the pydap list.\nJames\nRegards.\n--\nJonas Cavalcanti\nFone: +5585986872577 | Skype: jonas.neto9\nRedHat:\nRHCSA | RHCE | RH Server Hrdening | RHJB248\nID:150-087-658\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Downscaled climate data",
     "name": "Nkhonjera, German",
     "body": "Hi,\nI am in South Africa and would like to download the climate data for an area in South Africa, how can I go about it? I am an academic staff member at the University of Johannesburg. The data I want to download is entirely for research purposes. I look forward to hear from you. Thank you.\nGerman Nkhonjera\nThis email and all contents are subject to the following disclaimer:\nhttp://disclaimer.uj.ac.za"
    }
   ]
  },
  {
   "name": [
    {
     "page": "OPeNDAP error for downloading MEWEP data",
     "name": "Parisa Hosseinzadehtalaei",
     "body": "Dear Sir/Madam,\nI am sending this email to inform you internal server error occurred when I was downloading data from OPeNDAP. Could you fix it? or how can I fix it?\nThanks in advance,\nParisa ​"
    },
    {
     "page": "OPeNDAP error for downloading MEWEP data",
     "name": "Nathan Potter",
     "body": "Hello Parisa,\nIn order to assist you I will need the URL of the server that you were trying to access, and preferably the URL that failed for you.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "next hyrax and hyrax-docker releases",
     "name": "Gareth....@csiro.au",
     "body": "Greetings,\nI see coding has been done in the last few months. When do you anticipate the next hyrax release?\nDo you plan to make a further hyrax-docker release to docker hub? I see you have made tomcat run as a user and I’d like to have that. Should I wait for a container release or vary my setup to build my own container?\nBest regards,\nGareth Williams\nCSIRO IMT Scientific Computing Services\nPrivate Bag 10\nClayton South Vic 3170"
    }
   ]
  },
  {
   "name": [
    {
     "page": "[Staging #JKC-446580]: unstable OPeNDAP server at NCEI",
     "name": "Unidata THREDDS Support",
     "body": "Greetings Yi Dai,\nThere should not be any problems accessing the individual netCDF files via OPeNDAP.\nThe default binary response limit for the OPeNDAP server in the TDS is 500MB, but\nthis can be changed by the server admin.\nCan you give us some more details about what happens when things do now work,\nor, even better, the name of a dataset that does not work? Do you see an error\nmessage?\nCheers,\nSean\n> Greetings Yi Dai,\n>\n> The server in question is an instance of the THREDDS Data Server from\n> Unidata (cc'd above) and operated by our friends at NOAA (also cc'd).\n>\n> Lacking the actual data access URLs that you are using I can only\n> speculate about the issue. I do see a pretty noticeable fact, that the\n> dataset is big.\n>\n> It contains 13 ~40MB variables:\n>\n> =\n> https://www.ncei.noaa.gov/thredds/dodsC/cdr/gridsat/files/2014/GRIDSAT-B1.2014.09.16.06.v02r01.nc.dds\n>\n> The server in question undoubtably has limits as to the size of the\n> requests that it can produce, and the problem you are encountering may\n> simply mean that you have to make multiple requests using the server's\n> subsetting features to reduce the size of the returned data for each\n> request.\n>\n> =\n> https://opendap.github.io/documentation/UserGuideComprehensive.html#Constraint_Expressions\n>\n> Which may be as simple as asking for one data variable at a time.\n>\n> Please let us know if you need additional assistance.\n>\n>\n> Sincerely,\n>\n> Nathan\n>\n>\n> > On Nov 6, 2017, at 9:24 AM, Dai, Yi <yd...@rsmas.miami.edu> wrote:\n> >\n> > Dear officer,\n> >\n> > I'm a graduate student at University of Miami. I tried to use some OPeNDAP\n> > data (such as https://www.ncei.noaa.gov/thredds/dodsC/cdr/gridsat/files/2014/GRIDSAT-B1.2014.09.16.06.v02r01.nc). I used matlab 2017b to read the data.\n> >\n> > But this website is not very stable: it works for some timesteps, but\n> > stops later. Is this a normal problem for using OPeNDAP data?\n> >\n> > Thanks,\n> > Yi Dai\n>\n>\nTicket Details\n===================\nTicket ID: JKC-446580\nDepartment: Support THREDDS\nPriority: Normal\nStatus: Open\n===================\nNOTE: All email exchanges with Unidata User Support are recorded in the Unidata inquiry tracking system and then made publicly available through the web. If you do not want to have your interactions made available in this way, you must let us know in each email you send to us."
    },
    {
     "page": "[Staging #JKC-446580]: unstable OPeNDAP server at NCEI",
     "name": "Dai, Yi",
     "body": "Hi Sean,\nI think the problem is just that the website is not stable. When I tried the code today, it works perfectly. The data I tried to get is from the website: https://www.ncei.noaa.gov/thredds/catalog/cdr/gridsat/files/2014/catalog.html (an example). I read the data in a loop. At each step I only read a small amount of data, so the binary response limit might not be the reason of the problem. The error is that matlab (version 2017b) cannot read the 2-D field data (such as irwin_cdr), but be able to read 1-D data (such as lon and lat).\nI searched online and found that some old version of matlab does not support (https) OPeNDAP data, so I updated to the newest version. Yes it can read (https) data, but still not stable.\nThanks,\nYi\nFrom: Unidata THREDDS Support <support...@unidata.ucar.edu>\nSent: Tuesday, November 7, 2017 1:35:18 PM\nTo: Dai, Yi\nCc: n...@opendap.org; sup...@opendap.org; ncei...@noaa.gov\nSubject: [Staging #JKC-446580]: unstable OPeNDAP server at NCEI\nGreetings Yi Dai,\nThere should not be any problems accessing the individual netCDF files via OPeNDAP.\nThe default binary response limit for the OPeNDAP server in the TDS is 500MB, but\nthis can be changed by the server admin.\nCan you give us some more details about what happens when things do now work,\nor, even better, the name of a dataset that does not work? Do you see an error\nmessage?\nCheers,\nSean\n> Greetings Yi Dai,\n>\n> The server in question is an instance of the THREDDS Data Server from\n> Unidata (cc'd above) and operated by our friends at NOAA (also cc'd).\n>\n> Lacking the actual data access URLs that you are using I can only\n> speculate about the issue. I do see a pretty noticeable fact, that the\n> dataset is big.\n>\n> It contains 13 ~40MB variables:\n>\n> =\n> https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.ncei.noaa.gov%2Fthredds%2FdodsC%2Fcdr%2Fgridsat%2Ffiles%2F2014%2FGRIDSAT-B1.2014.09.16.06.v02r01.nc.dds&data=02%7C01%7Cydai%40rsmas.miami.edu%7Cd75cd2aeb95d4e64453308d5260e4d66%7C2a144b72f23942d48c0e6f0f17c48e33%7C0%7C0%7C636456765230775999&sdata=pJyMGeSRSjuasb5MYN79pwJTo%2BZ0mBC6MIa1mTsFkFQ%3D&reserved=0\n>\n> The server in question undoubtably has limits as to the size of the\n> requests that it can produce, and the problem you are encountering may\n> simply mean that you have to make multiple requests using the server's\n> subsetting features to reduce the size of the returned data for each\n> request.\n>\n> =\n> https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopendap.github.io%2Fdocumentation%2FUserGuideComprehensive.html%23Constraint_Expressions&data=02%7C01%7Cydai%40rsmas.miami.edu%7Cd75cd2aeb95d4e64453308d5260e4d66%7C2a144b72f23942d48c0e6f0f17c48e33%7C0%7C0%7C636456765230775999&sdata=bjvVap%2By4Nv0mNWpg%2BbvuLFZmhDYZIRhO9Wt46FL6mo%3D&reserved=0\n>\n> Which may be as simple as asking for one data variable at a time.\n>\n> Please let us know if you need additional assistance.\n>\n>\n> Sincerely,\n>\n> Nathan\n>\n>\n> > On Nov 6, 2017, at 9:24 AM, Dai, Yi <yd...@rsmas.miami.edu> wrote:\n> >\n> > Dear officer,\n> >\n> > I'm a graduate student at University of Miami. I tried to use some OPeNDAP\n> > data (such as https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.ncei.noaa.gov%2Fthredds%2FdodsC%2Fcdr%2Fgridsat%2Ffiles%2F2014%2FGRIDSAT-B1.2014.09.16.06.v02r01.nc&data=02%7C01%7Cydai%40rsmas.miami.edu%7Cd75cd2aeb95d4e64453308d5260e4d66%7C2a144b72f23942d48c0e6f0f17c48e33%7C0%7C0%7C636456765230775999&sdata=IcFZ4B0qQB2zy4chRzkopxmXaVfQwWPeqg60iqyjTN8%3D&reserved=0). I used matlab 2017b to read the data.\n> >\n> > But this website is not very stable: it works for some timesteps, but\n> > stops later. Is this a normal problem for using OPeNDAP data?\n> >\n> > Thanks,\n> > Yi Dai\n>\n>\nTicket Details\n===================\nTicket ID: JKC-446580\nDepartment: Support THREDDS\nPriority: Normal\nStatus: Open\n===================\nNOTE: All email exchanges with Unidata User Support are recorded in the Unidata inquiry tracking system and then made publicly available through the web. If you do not want to have your interactions made available in this way, you must let us know in each email you send to us."
    },
    {
     "page": "[Staging #JKC-446580]: unstable OPeNDAP server at NCEI",
     "name": "Unidata THREDDS Support",
     "body": "Greetings Yi,\nPerhaps then the NCEI server has been experiencing some issues - I'll let them comment on that possibility.\nCheers,\nSean\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "curious catalog.xml",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi. There is a missing slash ‘/’ in the THREDDS catalog ‘catalog.xml’ coming out of some of our servers. For example, comparing this URL\nhttps://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/catalog.xml\nwith this one\nhttps://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/catalog.xml\nnoticing the first one has:\nID=\"/opendap/hyraxMERRA2/\"/>\nWhereas the seond looks alright.\nWonder which part of the OLFS configuration caused this behavior. Thanks.\n-Fan"
    },
    {
     "page": "curious catalog.xml",
     "name": "Nathan Potter",
     "body": "Fan,\nI can’t replicate this on my dev system (no surprise there) so I think I need to see the entire configuration, like:\ncd /etc; tar -czvf ~/goldsmr5_olfs.tgz olfs\nfrom each machine. There is a directory called /etc/olfs/logs that may contain user information - I do not need that directory, but I need everything else.\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "curious catalog.xml",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nI have them tarred up here:\nhttps://hydro1.gesdisc.eosdis.nasa.gov/private/ffang/goldsmr4.olfs.tgz\nhttps://hydro1.gesdisc.eosdis.nasa.gov/private/ffang/goldsmr5.olfs.tgz\nThanks.\n-Fan\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nIf you wouldn’t mind can you tell me, for both systems:\n- OS and version\n- Java version\n- Tomcat version\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nThe versions are below. In the THREDDS catalog I originally referred to, there is also this line:\n<thredds:dataset name=\"\" ID=\"/opendap/hyrax/\">\nThe empty ‘name’ in the ‘<thredds:dataset/>’ element seems to coexist with the wrong ‘ID’ in the child ‘<thredds:catalogRef/>’. The value of the ‘name’ shall be “/”. How is the ‘name’ formed?\nCentOS 6.9\nTomcat 8\nJVM Version: 1.8.0_151-b12\n-Fan\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nI think the issue is a change in behavior between minor versions of Tomcat (I might be wrong, but please humor me), and so I need to know the actual version number, like 7.0.57 or 8.0.47 or 8.5.23. If you installed Tomcat using yum, it should tell you with a “yum info tomcat”:\nInstalled Packages\nName : tomcat\nArch : noarch\nVersion : 7.0.77\nOr something to that effect.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nI don’t mean to humor you but our SystemAdmin claims that we use the same tomcat to the minor version (7.0.82 below) on the two systems. Should they change their claim I’ll let you know.\n-Fan\nName : apache-tomcat Relocations: (not relocatable)\nVersion : 7.0.82 Vendor: (none)\nRelease : 1.GESDISC.el6 Build Date: Wed 04 Oct 2017 05:14:05 PM GMT\nInstall Date: Tue 10 Oct 2017 02:31:54 PM GMT Build Host: gs6102dsc-s4psci.gesdisc.eosdis.nasa.gov\nGroup : Productivity/Networking/Web/Servers Source RPM: apache-tomcat-7.0.82-1.GESDISC.el6.src.rpm\nSize : 8297874 License: Apache Software License.\nSignature : RSA/SHA1, Wed 04 Oct 2017 05:15:02 PM GMT, Key ID 0b8bbdcb3e1d0073\nURL : http://tomcat.apache.org\nSummary : Open source software implementation of the Java Servlet and JavaServer Pages technologies.\nDescription :\nApache Tomcat is an open source software implementation of the Java Servlet and JavaServer Pages technologies. The Java Servlet and JavaServer Pages specifications are developed under the Java Community Process.\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Nathan Potter",
     "body": "Hi Fan,\nI have an idea!\n1) Please send me $CATALINA_HOME/conf/context.xml from both systems.\n2) I’m thinking the problem might be a change in Tomcat - and I am not sure how it’s happening but you might want to read this ticket: https://opendap.atlassian.net/browse/HYRAX-361 And in particular read the comment I made today. In it there is a work around for the HYRAX-361 problem that involves changing the Tomcat configuration. I would love to find out if making that change to the Tomcat config on the goldsmr5 system causes a change in this problematic THREDDS catalog behavior.\nThanks,\nNathan\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nThanks for the investigation. I am asking our system guy to add the attributes to the <context> element as you specified in the ticket for goldsmr5. I’ll report back when I have a result.\nI attach the two context.xml. They are the same, so it is still a puzzle why our goldsmr4 doesn’t have this behavior.\n-Fan\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Nathan Potter",
     "body": "Thanks Fan! I’ll be interested to see what happens with the configuration change.\n- show quoted text -\n> <goldsmr4_context.xml><goldsmr5_context.xml>\n- show quoted text -"
    },
    {
     "page": "curious catalog.xml",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi Nathan,\nSorry for the delay – our system guy logged himself out.\nThis fixed the problem on goldsmr5. Though it does not explain why our other servers work, I can live with it for now. Thanks for your hard work and quick turnaround!\n-Fan\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Data Query",
     "name": "Sluleko Gwala (212532511)",
     "body": "Sir/Madam\nI am asking if you can help me with downloading AVHRR Monthly Sea Surface Temperature (SST) and climatology data for the year 2010, 2014,2015 and 2016. I have tried the pathfinder V 5.0 , V 5.1 and V 5.2. As you know, V5.0 and V5.1 provide data that span from 1981-2009 which include daily, monthly and annual data among others, while V5.2 do have data spanning from 2009-2016 but none of them indicate monthly SST measurements but instead only daily SST. Could you please direct me in the site where i can download Monthly AVHRR SST measurements please.\nRespectfully,\nSluleko Gwala"
    },
    {
     "page": "Data Query",
     "name": "PO.DAAC User Services",
     "body": "Hi Sluleko,\nThank you for your inquiry.\nPlease review the stream of emails by following this link: https://podaac.jpl.nasa.gov/forum/viewtopic.php?f=6&t=20&p=143&hilit=avhrr#p143, you will find the answer. Let's know if you still have questions.\nRegards,\nWenhao Li\n---------------------------------------------------------\nJet Propulsion Laboratory / Physical Oceanography DAAC\nEmail: pod...@podaac.jpl.nasa.gov\nWebsite: http://podaac.jpl.nasa.gov\n---------------------------------------------------------\nTicket History\nSluleko Gwala 212532511 (Client) Posted On: 25 October 2017 11:50 AM\n- show quoted text -\nTicket Details\nTicket ID: 78235\nDepartment: PO.DAAC\nType: General\nStatus: Open\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?"
    },
    {
     "page": "Data Query",
     "name": "Sluleko Gwala (212532511)",
     "body": "Thank you very much, i am great-full for your attempt to help out even though i didn't get monthly measurements as i hoped for but i will try to collocate daily data into monthly.\nIt is important that i get a monthly climatological data that span beyond 2008 for this SST to work accordingly in my study. Unfortunately, i don't seem to find SST climatological data that span beyond 2008. Can you please advise on that.\nFrom: PO.DAAC User Services <support...@earthdata.nasa.gov>\nSent: Wednesday, October 25, 2017 7:28:52 PM\nTo: Sluleko Gwala (212532511)\nCc: pod...@podaac.jpl.nasa.gov; sup...@opendap.org\nSubject: [#78235]: Data Query\nHi Sluleko,\nThank you for your inquiry.\nPlease review the stream of emails by following this link: https://podaac.jpl.nasa.gov/forum/viewtopic.php?f=6&t=20&p=143&hilit=avhrr#p143, you will find the answer. Let's know if you still have questions.\nRegards,\nWenhao Li\n---------------------------------------------------------\nJet Propulsion Laboratory / Physical Oceanography DAAC\nEmail: pod...@podaac.jpl.nasa.gov\nWebsite: http://podaac.jpl.nasa.gov\n---------------------------------------------------------\nTicket History\nSluleko Gwala 212532511 (Client) Posted On: 25 October 2017 11:50 AM\nSir/Madam\nI am asking if you can help me with downloading AVHRR Monthly Sea Surface Temperature (SST) and climatology data for the year 2010, 2014,2015 and 2016. I have tried the pathfinder V 5.0 , V 5.1 and V 5.2. As you know, V5.0 and V5.1 provide data that span from 1981-2009 which include daily, monthly and annual data among others, while V5.2 do have data spanning from 2009-2016 but none of them indicate monthly SST measurements but instead only daily SST. Could you please direct me in the site where i can download Monthly AVHRR SST measurements please.\nRespectfully,\nSluleko Gwala\nTicket Details\nTicket ID: 78235\nDepartment: PO.DAAC\nType: General\nStatus: Open\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?"
    },
    {
     "page": "Data Query",
     "name": "PO.DAAC User Services",
     "body": "Hi Sluleko,\nYou can try to use GHRSS L4 foundation SST, such as MUR (https://podaac.jpl.nasa.gov/dataset/MUR-JPL-L4-GLOB-v4.1), OSTIA (https://podaac.jpl.nasa.gov/dataset/OSTIA-UKMO-L4-GLOB-v2.0), or CMC0.02 (https://podaac.jpl.nasa.gov/dataset/DMI_OI-DMI-L4-GLOB-v1.0) from PO.DAAC site, or climatology SST from NCEP (http://www.emc.ncep.noaa.gov/research/cmb/sst_analysis/).\nRegards,\nWenhao Li\n---------------------------------------------------------\nJet Propulsion Laboratory / Physical Oceanography DAAC\nEmail: pod...@podaac.jpl.nasa.gov\nWebsite: http://podaac.jpl.nasa.gov\n---------------------------------------------------------\nTicket History\nSluleko Gwala 212532511 (Client) Posted On: 25 October 2017 11:50 AM\nSir/Madam\nI am asking if you can help me with downloading AVHRR Monthly Sea Surface Temperature (SST) and climatology data for the year 2010, 2014,2015 and 2016. I have tried the pathfinder V 5.0 , V 5.1 and V 5.2. As you know, V5.0 and V5.1 provide data that span from 1981-2009 which include daily, monthly and annual data among others, while V5.2 do have data spanning from 2009-2016 but none of them indicate monthly SST measurements but instead only daily SST. Could you please direct me in the site where i can download Monthly AVHRR SST measurements please.\nRespectfully,\nSluleko Gwala\nWen-Hao Li (Staff) Posted On: 25 October 2017 01:28 PM\nHi Sluleko,\nThank you for your inquiry.\nPlease review the stream of emails by following this link: https://podaac.jpl.nasa.gov/forum/viewtopic.php?f=6&t=20&p=143&hilit=avhrr#p143, you will find the answer. Let's know if you still have questions.\nRegards,\nWenhao Li\n---------------------------------------------------------\nJet Propulsion Laboratory / Physical Oceanography DAAC\nEmail: pod...@podaac.jpl.nasa.gov\nWebsite: http://podaac.jpl.nasa.gov\n---------------------------------------------------------\nSluleko (Client) Posted On: 26 October 2017 06:25 PM\nThank you very much, i am great-full for your attempt to help out even though i didn't get monthly measurements as i hoped for but i will try to collocate daily data into monthly.\nIt is important that i get a monthly climatological data that span beyond 2008 for this SST to work accordingly in my study. Unfortunately, i don't seem to find SST climatological data that span beyond 2008. Can you please advise on that.\nSluleko (Client) Posted On: 26 October 2017 06:25 PM\nThank you very much, i am great-full for your attempt to help out even though i didn't get monthly measurements as i hoped for but i will try to collocate daily data into monthly.\nIt is important that i get a monthly climatological data that span beyond 2008 for this SST to work accordingly in my study. Unfortunately, i don't seem to find SST climatological data that span beyond 2008. Can you please advise on that.\nTicket Details\nTicket ID: 78235\nDepartment: PO.DAAC\nType: General\nStatus: Open\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?"
    },
    {
     "page": "Data Query",
     "name": "Nathan Potter",
     "body": "Hi Sluleko,\nWhere you able to get the data you desired?\nThanks,\nNathan\n> On Oct 26, 2017, at 5:30 PM, PO.DAAC User Services <support...@earthdata.nasa.gov> wrote:\n>\n> Hi Sluleko,\n>\n> You can try to use GHRSS L4 foundation SST, such as MUR (https://podaac.jpl.nasa.gov/dataset/MUR-JPL-L4-GLOB-v4.1), OSTIA (https://podaac.jpl.nasa.gov/dataset/OSTIA-UKMO-L4-GLOB-v2.0), or CMC0.02 (https://podaac.jpl.nasa.gov/dataset/DMI_OI-DMI-L4-GLOB-v1.0) from PO.DAAC site, or climatology SST from NCEP (http://www.emc.ncep.noaa.gov/research/cmb/sst_analysis/).\n>\n> Regards,\n>\n> Wenhao Li\n>\n> ---------------------------------------------------------\n> Jet Propulsion Laboratory / Physical Oceanography DAAC\n> Email: pod...@podaac.jpl.nasa.gov\n> Website: http://podaac.jpl.nasa.gov\n> ---------------------------------------------------------\n>\n> Ticket History\n> Sluleko Gwala 212532511 (Client) Posted On: 25 October 2017 11:50 AM\n>\n> Sir/Madam\n>\n>\n> I am asking if you can help me with downloading AVHRR Monthly Sea Surface Temperature (SST) and climatology data for the year 2010, 2014,2015 and 2016. I have tried the pathfinder V 5.0 , V 5.1 and V 5.2. As you know, V5.0 and V5.1 provide data that span from 1981-2009 which include daily, monthly and annual data among others, while V5.2 do have data spanning from 2009-2016 but none of them indicate monthly SST measurements but instead only daily SST. Could you please direct me in the site where i can download Monthly AVHRR SST measurements please.\n>\n>\n> Respectfully,\n>\n> Sluleko Gwala\n>\n>\n>\n> Wen-Hao Li (Staff) Posted On: 25 October 2017 01:28 PM\n>\n> Hi Sluleko,\n>\n> Thank you for your inquiry.\n>\n> Please review the stream of emails by following this link: https://podaac.jpl.nasa.gov/forum/viewtopic.php?f=6&t=20&p=143&hilit=avhrr#p143, you will find the answer. Let's know if you still have questions.\n>\n> Regards,\n>\n> Wenhao Li\n>\n> ---------------------------------------------------------\n> Jet Propulsion Laboratory / Physical Oceanography DAAC\n> Email: pod...@podaac.jpl.nasa.gov\n> Website: http://podaac.jpl.nasa.gov\n> ---------------------------------------------------------\n>\n> Sluleko (Client) Posted On: 26 October 2017 06:25 PM\n>\n> Thank you very much, i am great-full for your attempt to help out even though i didn't get monthly measurements as i hoped for but i will try to collocate daily data into monthly.\n>\n>\n> It is important that i get a monthly climatological data that span beyond 2008 for this SST to work accordingly in my study. Unfortunately, i don't seem to find SST climatological data that span beyond 2008. Can you please advise on that.\n>\n>\n>\n> Sluleko (Client) Posted On: 26 October 2017 06:25 PM\n>\n> Thank you very much, i am great-full for your attempt to help out even though i didn't get monthly measurements as i hoped for but i will try to collocate daily data into monthly.\n>\n>\n> It is important that i get a monthly climatological data that span beyond 2008 for this SST to work accordingly in my study. Unfortunately, i don't seem to find SST climatological data that span beyond 2008. Can you please advise on that.\n>\n>\n>\n> Ticket Details\n> Ticket ID: 78235\n> Department: PO.DAAC\n> Type: General\n> Status: Open\n> Priority: Normal\n>\n> Helpdesk: https://support.earthdata.nasa.gov/index.php?\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Data Query",
     "name": "Sluleko Gwala (212532511)",
     "body": "Hi Nathan,\nThanks for asking, Unfortunately i didn't get the data i desired. i was left with no choice but to use Aqua Modis as per your suggestion.\nRespectfully,\nSluleko\nFrom: Nathan Potter <n...@opendap.org>\nSent: Tuesday, November 7, 2017 7:17:45 PM\nTo: Sluleko Gwala (212532511)\nCc: Nathan Potter; support...@earthdata.nasa.gov; pod...@podaac.jpl.nasa.gov; sup...@opendap.org support\nSubject: Re: [support] [#78235]: Data Query\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "OPeNDAP not stable",
     "name": "Dai, Yi",
     "body": "Dear officer,\nI'm a graduate student at University of Miami. I tried to use some OPeNDAP data (such as https://www.ncei.noaa.gov/thredds/dodsC/cdr/gridsat/files/2014/GRIDSAT-B1.2014.09.16.06.v02r01.nc). I used matlab 2017b to read the data.\nBut this website is not very stable: it works for some timesteps, but stops later. Is this a normal problem for using OPeNDAP data?\nThanks,\nYi Dai"
    },
    {
     "page": "OPeNDAP not stable",
     "name": "Nathan Potter",
     "body": "Greetings Yi Dai,\nThe server in question is an instance of the THREDDS Data Server from Unidata (cc’d above) and operated by our friends at NOAA (also cc’d).\nLacking the actual data access URLs that you are using I can only speculate about the issue. I do see a pretty noticeable fact, that the dataset is big.\nIt contains 13 ~40MB variables:\nhttps://www.ncei.noaa.gov/thredds/dodsC/cdr/gridsat/files/2014/GRIDSAT-B1.2014.09.16.06.v02r01.nc.dds\nThe server in question undoubtably has limits as to the size of the requests that it can produce, and the problem you are encountering may simply mean that you have to make multiple requests using the server's subsetting features to reduce the size of the returned data for each request.\nhttps://opendap.github.io/documentation/UserGuideComprehensive.html#Constraint_Expressions\nWhich may be as simple as asking for one data variable at a time.\nPlease let us know if you need additional assistance.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: Renaming Dimensions in NCML",
     "name": "jgallagher",
     "body": "On Nov 3, 2017, at 07:54, Friesz (CTR), Aaron <aaron.fr...@usgs.gov> wrote:\nHi James,\nSorry for all of the status requests. Could you let me know what the status is on the related tickets (see below) for renaming dimensions?\nI have to admit that we forgot about these. I’ve started tagging all of the NASA-originated support requests so we can find them easily, although I realize that doesn’t change much in terms of the software.\nI can’t say when we’ll get to these, but they are now at least visible.\nJames\n- https://opendap.atlassian.net/browse/HYRAX-5\n- https://opendap.atlassian.net/browse/HYRAX-12\nThanks,\nAaron\n--\nAaron FrieszData ScientistInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)USGS/EROS CenterSioux Falls, SD 57198\nPhone: 605-594-6526 Email: aaron.fr...@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Is the THREDDS netCDF subsetter website \"broken?\"",
     "name": "Smart, Jeffrey H.",
     "body": "I have tried numerous times to pull a subset (in netCDF format) from this URL:\nhttp://ncss.hycom.org/thredds/ncss/grid/GLBa0.08/expt_91.0/2013/uvel/dataset.html\n& no matter what I do, the site indicates an invalid URL (even if I just use the website’s defaults).\nI also attempted to get a spatial subset using\nhttp://ncss.hycom.org/thredds/ncss/grid/GLBu0.08/expt_91.2/uv3z/dataset.html\nHere is the query\nhttp://ncss.hycom.org/thredds/ncss/GLBu0.08/expt_91.2/uv3z?north=40&west=100&east=130&south=10&disableProjSubset=on&horizStride=1&time_start=2016-08-15T00%3A00%3A00Z&time_end=2017-08-15T00%3A00%3A00Z&timeStride=1&vertCoord=&addLatLon=true&accept=netcdf\ngenerated from this query (10-40 N, 100 to 130 E) using this on-line GUI:\nHere is the error screen that I get:\nJeffrey H. Smart\nOceanic, Atmospheric & Remote Sensing Sciences Group\nForce Projection Sector\nJohns Hopkins Univ/Applied Physics Laboratory, Rm 8-209\n11100 Johns Hopkins Rd, Laurel, MD, 20723-6099\nPH: 443-778-4331"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Website problem",
     "name": "Heydorn, James W. (JSC-XI4)[Jacobs Technology, Inc.]",
     "body": "For the past couple of days now I've been unable to download CATS HDF5 files. I get a 500 error. Please fix. Here's an example URL:\nhttps://opendap.larc.nasa.gov/opendap/CATS/L1B_D-M7.2-V2-08/2017/09/CATS-ISS_L1B_D-M7.2-V2-08.2017-09-01T13-42-41T14-20-28UTC.hdf5\nJames Heydorn\nInformation Technology, Earth Science Remote Sensing and Image Science and Analysis\nAstromaterials Research and Exploration Science (ARES) Division\nNASA Johnson Space Center\nMail Code XI4-B36, Building 36, Room 2018E\noffice: 281-483-3280"
    },
    {
     "page": "Website problem",
     "name": "jgallagher",
     "body": "Hi,\nCan you check on the Hyrax server at: https://opendap.larc.nasa.gov/opendap/\nIt looks like the BES has failed and not restarted.\nThanks\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Website problem",
     "name": "ASDC Support",
     "body": "Dear James Gallagher,\nThank you for contacting us with your concerns.\nThe ASDC had an unplanned power failure, and all systems at the ASDC are down. This has impacted all of our services.\nWe posted messages at EarthData, but we realize that most users bookmark their favorite ordering tool’s location.\nAccess to all ASDC FTP servers, the Eosweb site, subsetters, order tools, and the Data Pool are unavailable.\nNormal service will be restored as soon as possible.\nWe apologize for any inconvenience this may cause you.\nPlease let us know if you have any additional questions.\nWarm Regards,\nMichelle\nNASA Langley ASDC User Services\n**************************************************\nUser and Data Services\nScience Systems & Applications, Inc.\nAtmospheric Science Data Center\nNASA Langley Research Center\nPhone: (757) 864-8656\nGeneral Inquiries: suppor...@earthdata.nasa.gov\nScientific Inquiries: https://wiki.earthdata.nasa.gov/display/atmosphere/Atmospheric+Science+User+Forum\nASDC Website: http://eosweb.larc.nasa.gov\n***************************************************\nTicket History\nJames Gallagher (Client) Posted On: 31 October 2017 03:15 PM\nHi,\nCan you check on the Hyrax server at: https://opendap.larc.nasa.gov/opendap/\nIt looks like the BES has failed and not restarted.\nThanks\n> On Oct 31, 2017, at 11:11, Heydorn, James W. (JSC-XI4)[Jacobs Technology, Inc.] wrote:\n>\n> For the past couple of days now I've been unable to download CATS HDF5 files. I get a 500 error. Please fix. Here's an example URL:\n>\n> https://opendap.larc.nasa.gov/opendap/CATS/L1B_D-M7.2-V2-08/2017/09/CATS-ISS_L1B_D-M7.2-V2-08.2017-09-01T13-42-41T14-20-28UTC.hdf5\n>\n> James Heydorn\n> Information Technology, Earth Science Remote Sensing and Image Science and Analysis\n> Astromaterials Research and Exploration Science (ARES) Division\n> NASA Johnson Space Center\n> Mail Code XI4-B36, Building 36, Room 2018E\n> office: 281-483-3280\n--\nJames Gallagher\njgall...@opendap.org\nTicket Details\nTicket ID: 78396\nDepartment: ASDC\nType: General\nStatus: Closed\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?"
    },
    {
     "page": "Website problem",
     "name": "Heydorn, James W. (JSC-XI4)[Jacobs Technology, Inc.]",
     "body": "Not sure why I got this today considering you fixed it yesterday. Thanks for doing that.\nNow we have two 404 error files. When I contacted the website owner I was told it's ASDC problem. Somebody please fix them:\nhttps://opendap.larc.nasa.gov/opendap/CATS/L1B_D-M7.2-V2-07/2016/10/CATS-ISS_L1B_D-M7.2-V2-07.2016-10-07T22-33-49T23-21-37UTC.hdf5\nhttps://opendap.larc.nasa.gov/opendap/CATS/L1B_N-M7.2-V2-07/2016/10/CATS-ISS_L1B_N-M7.2-V2-07.2016-10-09T01-38-02T02-21-54UTC.hdf5\nFrom: ASDC Support [mailto:suppor...@earthdata.nasa.gov]\nSent: Wednesday, November 01, 2017 6:31 AM\nTo: jgall...@opendap.org\nCc: larc-a...@lists.nasa.gov; sup...@opendap.org; Heydorn, James W. (JSC-XI4)[Jacobs Technology, Inc.] <james.w...@nasa.gov>\nSubject: [#78396]: Re: [support] Website problem\n- show quoted text -"
    },
    {
     "page": "Website problem",
     "name": "jgallagher",
     "body": "James,\nI should have taken a bit more time to explain my response - apologies for that.\nWe (OPeNDAP) are the developers of the data server software that ASDC and other NASA labs use to serve data. We don’t run any of these servers, but our support email address is embedded in the software as the default, so it’s often shown when error responses are returned (although this can be altered by a configuration parameter that’s pretty easy to change).\nIn this case ASDC is running the server and that’s why I forwarded your question to them. Most of the time these problems are power failures, OS update failures, et cetera. Of course, that doesn’t fix your problem, but hopefully they will get this ironed out soon.\nIf the problem persists, please let me know.\nThanks,\nJames\nOn Nov 1, 2017, at 10:28, Heydorn, James W. (JSC-XI4)[Jacobs Technology, Inc.] <james.w...@nasa.gov> wrote:\nNot sure why I got this today considering you fixed it yesterday. Thanks for doing that.\nNow we have two 404 error files. When I contacted the website owner I was told it's ASDC problem. Somebody please fix them:\nhttps://opendap.larc.nasa.gov/opendap/CATS/L1B_D-M7.2-V2-07/2016/10/CATS-ISS_L1B_D-M7.2-V2-07.2016-10-07T22-33-49T23-21-37UTC.hdf5\nhttps://opendap.larc.nasa.gov/opendap/CATS/L1B_N-M7.2-V2-07/2016/10/CATS-ISS_L1B_N-M7.2-V2-07.2016-10-09T01-38-02T02-21-54UTC.hdf5\nFrom: ASDC Support [mailto:su...@earthdata.nasa.gov]\n- show quoted text -\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: MOD16A2 HDF4 file",
     "name": "jgallagher",
     "body": "On Sep 27, 2017, at 15:18, Friesz (CTR), Aaron <aaron.fr...@usgs.gov> wrote:\nHi James,\nDo you know if the updates to the HDF4 handler (discussed below) will be in the next release of Hyrax? Any idea when the 1.13.6 will be released? We're trying determine whether we should build the handler ourselves or if we should wait for the new release.\nthe next release will be 1.14 and it’s backed up. Sadly, NASA ran out of funds for it, so I’m working on it in between other tasks. I _hope_ I can get it out the door in the next two weeks. It will have the fix mentioned.\nJames\nAaron\n---------- Forwarded message ----------\nFrom: Kent Yang <mya...@hdfgroup.org>\nDate: Tue, Sep 19, 2017 at 2:40 PM\nSubject: RE: MOD16A2 HDF4 file\nTo: \"Friesz (CTR), Aaron\" <aaron.fr...@usgs.gov>\nAaron,\nI don’t know if Nathan replied you individually. Basically he contacted me and told me he didn’t create new BES rpm in 1.13.5. So the change is not in the 1.13.5 release. Sorry about the inconvenience. I will contact James and Nathan to see how soon they can have the new 1.13.6 release. If you want to have this now, you may build the HDF4 module from github. We are not in the same company and I am kind of stretched to carry out various ESDIS tasks with limited resources.\nKent\nFrom: Kent Yang\nSent: Tuesday, September 19, 2017 9:11 AM\nTo: 'Friesz (CTR), Aaron'\nCc: 'Nathan Potter'\nSubject: RE: MOD16A2 HDF4 file\nYou need to ask the Hyrax team since they provided the BES RPM. HDF4 handler is only a module. You can see the fix you asked is after Hyrax 1.13.4 as I showed you at github. I CC to Nathan. Hopefully he can help you stress this out. Nathan, Aaron from LP DAAC found that there is no Hyrax 1.13.5 BES RPM. But he needs a fix of the HDF4 handler (https://github.com/OPENDAP/hdf4_handler/commit/d6aa3a6d6ad991c6ecfce5d98903cbf634a10c4b) after Hyrax 1.13.4. Could you help him with this? Thanks,\nKent\nFrom: Friesz (CTR), Aaron [mailto:aaro...@usgs.gov]\nSent: Monday, September 18, 2017 10:17 PM\nTo: Kent Yang\nSubject: Re: MOD16A2 HDF4 file\nIn the announcement (https://www.opendap.org/software/hyrax/1.13.5) I read that the BES RPMs are only available for version 1.13.4. The OLFS seems to be the only thing that's been updated to 1.13.5.\nOn Mon, Sep 18, 2017 at 10:05 PM, Kent Yang <mya...@hdfgroup.org> wrote:\nAaron,\nYou need to update to Hyrax 1.13.5. I checked in my fix on Jun 20th, 2017.\nhttps://github.com/OPENDAP/hdf4_handler/commit/d6aa3a6d6ad991c6ecfce5d98903cbf634a10c4b\nHFRHANDLER-310, make the MOD16A2 scale/offset behavior right. The sca… · OPENDAP/hdf4_handler@d6aa3a6\ngithub.com\n…le_factor is 10000.0 and the rule is multiply. very unusual. Need to handle differently.Also keep the scale_factor and add_offset datatype if the datatype is double. One testsuite needs to be upd...\nThe Hyrax 1.13.4 was released on Jun 05 or May 31, 2017. https://www.opendap.org/software/hyrax/1.13.4\nHyrax-1.13.4 | OPeNDAP™\nwww.opendap.org\nThe Hyrax-1.13.4 release contains bug fixes and some interesting new capabilities. Server Documentation. 1.13.4 release (05 Jun 2017) Welcome to the Hyrax 1.13.4 release.\nKent\nFrom: Friesz (CTR), Aaron <aaron.fr...@usgs.gov>\nSent: Monday, September 18, 2017 9:57 PM\nTo: Kent Yang\nSubject: Re: MOD16A2 HDF4 file\nHey Kent,\nWe're running OPeNDAP Hyrax 1.13.4.\nRequesting the ASCII output for the LE and PLE variables in products MOD16A2 and MOD16A3 returns small floating points rather than the large integer values\nMOD16A2: https://opendap.cr.usgs.gov/opendap/hyrax/MOD16A2.006/h09v05.ncml.ascii?PLE_500m[0:1:0][0:1:2399][0:1:2399]\nMOD16A3: https://opendap.cr.usgs.gov/opendap/hyrax/MOD16A2.006/h09v05.ncml.ascii?PLE_500m[0:1:0][0:1:2399][0:1:2399]\nAaron\nOn Mon, Sep 18, 2017 at 4:46 PM, Kent Yang <mya...@hdfgroup.org> wrote:\nHi Aaron,\nI’ve fixed this already. What version did you check? Could you provide more information?\nKent\nFrom: Friesz (CTR), Aaron [mailto:aaron.fr...@usgs.gov]\nSent: Monday, September 18, 2017 4:19 PM\nTo: Kent Yang\nSubject: Re: MOD16A2 HDF4 file\nHi Kent,\nI kind of forgot about this. It appears that OPeNDAP is still scaling this particular product (MOD16A2) incorrectly. Do you know if the work on this has been completed or what the status of this is?\nThanks,\nAaron\nOn Mon, Jun 19, 2017 at 9:51 AM, Kent Yang <mya...@hdfgroup.org> wrote:\nGot it. Thanks,\nKent\nFrom: Friesz (CTR), Aaron [mailto:aaron.fr...@usgs.gov]\nSent: Monday, June 19, 2017 9:49 AM\nTo: Kent Yang\nSubject: Re: MOD16A2 HDF4 file\nGive this guy a try: https://e4ftl01.cr.usgs.gov/MOLT/MOD16A2.006/2017.05.25/MOD16A2.A2017145.h10v04.006.2017160232658.hdf\nOn Mon, Jun 19, 2017 at 9:44 AM, Kent Yang <mya...@hdfgroup.org> wrote:\nHi Aaron,\nI need to have an MOD16A2 HDF4 file to test. https://opendap.cr.usgs.gov/opendap/hyrax/MOD16A2.006/h10v04.ncml.dds\nI just search earthdata and it is not obvious to me on how to get a file of this product.\nKent\nFrom: Kent Yang\nSent: Wednesday, June 14, 2017 5:14 PM\nTo: 'Friesz (CTR), Aaron'\nCc: James Gallagher\nSubject: RE: OPeNDAP - Auto Scaling MODIS data\nHi Aaron,\nThis is exactly what I want to know. Thanks. So the millions range of LE and PLE with units of J/m^2/day are actually correct! We need to fix the HDF4 handler to handle this case correctly.\nThe rational for us to divide the scale_factor when scale_factor is >1, is that indeed MOD13A,13C and MOD09GA and maybe a bunch of others have the following scale and offset formula:\nFinal _data_value = (raw_data_value - offset)/scale\nIn general, the final data value is always scaled back from a bigger value to a smaller value. This is a special case we rarely encountered. Could you double check if there are similar cases in your data center that the handler applies the wrong scale and offset rule for MODIS products?\nThanks a lot for letting me know.\nKent\nFrom: Friesz (CTR), Aaron [mailto:aaro...@usgs.gov]\nSent: Wednesday, June 14, 2017 4:17 PM\nTo: Kent Yang\nCc: James Gallagher\nSubject: Re: OPeNDAP - Auto Scaling MODIS data\nSo we just confirmed with the product PI. The scale factors for ET and PET are both 0.1, while the scale factors for LE and PLE are both 10000. After the scale factor is applied to the LE and PLE variables, the data values fall in the millions range with units of units of J/m^2/day.\nSome notes:\nWe have two OPeNDAP backends set up...one that's configured to scale data values and another that's configured to serve unscaled data.\nScaled OPeNDAP -> https://opendap.cr.usgs.gov/opendap/MOD16A2.006/h10v04.ncml.html\nUnscaled OPeNDAP -> https://opendap.cr.usgs.gov/UNS_opendap/MOD16A2.006/h10v04.ncml.html\nOn the scaled OPeNDAP, the scale factor in the metadata for LE_500m and PLE_500m is 10000. This is the value I expect. When I request a NetCDF, 10000 is carried over as the scale factor for the LE and PLE layers. I think the NetCDF libraries used in python, HDFView, and Panoply auto-apply the scale factor incorrectly in this case, dividing the data by the scale factor rather than multiplying it. That's outside OPeNDAP. Inside OPeNDAP, when I request an ascii, it appears to be dividing the data by the scale factor rather than multiplying it as well.\nOn the unscaled OPeNDAP, you'll notice that the scale factor for LE_500m and PLE_500m is changed from 10000 to 9.999999747e-05. I would expect the scale factor to still be 10000. That incorrect scale factor gets carried over to the NetCDF outputs which will cause problem when applying the scale factor on those data. When you request an ascii output for the LE and PLE variables the returned data values are unscaled as expected.\nLet me know if you need any more information.\nAaron\nOn Wed, Jun 14, 2017 at 1:35 PM, Kent Yang <mya...@hdfgroup.org> wrote:\nHi Aaron,\nNo problems. You are always welcome to ask me anything related to the Hyrax HDF handler issues.\nFor scale_factor = 10000, are you sure that the scale_factor equation should be data = scale_factor*data + add_offset\nI can see the variable LE_500m and PLE_500m both have the original scale_factor = 10000.0. Could you verify the right value range of PLE_500m and PET_500m after the scale and offset are applied? LE_500 m is MODIS Gridded 500m 8-day Composite latent heat flux (LE) and the units is J/m^2/day\nAfter I have a clearer understanding of the data, I can go back to you with a good explanation.\nKent\nFrom: Friesz (CTR), Aaron [mailto:aaron.fr...@usgs.gov]\nSent: Wednesday, June 14, 2017 11:04 AM\nTo: Kent Yang\nCc: James Gallagher\nSubject: OPeNDAP - Auto Scaling MODIS data\nHi Kent,\nSorry for all of the questions lately. It's one of the weeks I guess. We recently added a new MODIS product to OPeNDAP. Some of the variables have a scale factor of 0.1 and some of the variables have a scale factor of 10000. In all cases though, the data is unpacked using this equation:\ndata = scale_factor*data + add_offset\nThis equation seems to be what OPeNDAP uses to scale the data. However, for the variables with a scale factors of 10000, OPeNDAP seems to convert the value of 10000 to 0.0001 at some point. Here's a link to the MODIS product:\nhttps://opendap.cr.usgs.gov/opendap/hyrax/MOD16A2.006/h10v04.ncml.html\nIn the variable metadata, 10000 is identified as the 'orig_scale_factor' for LE_500m. However, when I request an ascii of the data I get floating point values, and when I request a netcdf the scale_factor attribute is converted to 0.0001. Any insight would be great.\nThanks,\nAaron\n--\nAaron Friesz\nData Scientist\nInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)\nUSGS/EROS Center\nSioux Falls, SD 57198\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nAaron Friesz\nData Scientist\nInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)\nUSGS/EROS Center\nSioux Falls, SD 57198\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nAaron Friesz\nData Scientist\nInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)\nUSGS/EROS Center\nSioux Falls, SD 57198\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nAaron Friesz\nData Scientist\nInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)\nUSGS/EROS Center\nSioux Falls, SD 57198\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nAaron Friesz\nData Scientist\nInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)\nUSGS/EROS Center\nSioux Falls, SD 57198\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nAaron Friesz\nData Scientist\nInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)\nUSGS/EROS Center\nSioux Falls, SD 57198\nPhone: 605-594-6526\nEmail: aaron.friesz.ctr@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nAaron FrieszData ScientistInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)USGS/EROS CenterSioux Falls, SD 57198\nPhone: 605-594-6526 Email: aaron.fr...@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Fwd: [#78389]: Fwd: [support] Website problem",
     "name": "Nathan Potter",
     "body": "Begin forwarded message:\nFrom: ASDC Support <suppor...@earthdata.nasa.gov>\nSubject: [#78389]: Fwd: [support] Website problem\nDate: October 31, 2017 at 11:13:08 AM PDT\nTo: n...@opendap.org\nCc: larc-a...@lists.nasa.gov, james.w...@nasa.gov\nReply-To: suppor...@earthdata.nasa.gov\nDear James and Nathan,\nThe ASDC had an unplanned power failure, and all systems at the ASDC are down.\nAccess to all ASDC FTP servers, the Eosweb site, subsetters, order tools, and the Data Pool are unavailable.\nNormal service will be restored as soon as possible.\nWe apologize for any inconvenience this may cause you.\nPlease let us know if you have an questions.\nRegards,\nJennifer\nNASA Langley ASDC User Services\n***************************************************\nUser Services Email (General Inquiries): suppor...@earthdata.nasa.gov\nAtmospheric Science User Forum (Scientific Inquiries): https://wiki.earthdata.nasa.gov/display/atmosphere/Atmospheric+Science+User+Forum\nASDC Website: https://eosweb.larc.nasa.gov\nTicket History\nNathan Potter (Client) Posted On: 31 October 2017 01:30 PM\nHi James,\nI am not sure exactly who is running that system. I am forwarding this to support at the closest URL to the malfunctioning one I could locate. The fine folks at ASDC may not run that server, but I bet they know who does.\nThanks for the heads up.\nSincerely,\nNathan Potter\n> Begin forwarded message:\n>\n> From: \"Heydorn, James W. (JSC-XI4)[Jacobs Technology, Inc.]\"\n> Subject: [support] Website problem\n> Date: October 31, 2017 at 10:11:39 AM PDT\n> To: \"sup...@opendap.org\"\n>\n> For the past couple of days now I've been unable to download CATS HDF5 files. I get a 500 error. Please fix. Here's an example URL:\n>\n> https://opendap.larc.nasa.gov/opendap/CATS/L1B_D-M7.2-V2-08/2017/09/CATS-ISS_L1B_D-M7.2-V2-08.2017-09-01T13-42-41T14-20-28UTC.hdf5\n>\n> James Heydorn\n> Information Technology, Earth Science Remote Sensing and Image Science and Analysis\n> Astromaterials Research and Exploration Science (ARES) Division\n> NASA Johnson Space Center\n> Mail Code XI4-B36, Building 36, Room 2018E\n> office: 281-483-3280\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317\nJennifer Tindell (Staff) Posted On: 31 October 2017 01:43 PM\nDear James and Nathan,\nThe ASDC had an unplanned power failure, and all systems at the ASDC are down.\nAccess to all ASDC FTP servers, many parts of the Eosweb site, subsetters, order tools, and the Data Pool are unavailable.\nNormal service will be restored as soon as possible.\nWe apologize for any inconvenience this may cause you.\nPlease let us know if you have an questions.\nRegards,\nJennifer\nNASA Langley ASDC User Services\n***************************************************\nUser Services Email (General Inquiries): suppor...@earthdata.nasa.gov\nAtmospheric Science User Forum (Scientific Inquiries): https://wiki.earthdata.nasa.gov/display/atmosphere/Atmospheric+Science+User+Forum\nASDC Website: https://eosweb.larc.nasa.gov\nTicket Details\nTicket ID: 78389\nDepartment: ASDC\nType: General\nStatus: Closed\nPriority: Normal\nHelpdesk: https://support.earthdata.nasa.gov/index.php?\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Fwd: [#78389]: Fwd: [support] Website problem",
     "name": "jgallagher",
     "body": "James Heydorn,\nWe got this from ASDC:\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Requesting help with freeform handler and line endings",
     "name": "Shannon Flynn",
     "body": "Good morning all,\nI hope you can help me understand the Freeform data handler a little better. I'm having trouble with line endings. I'm trying to get OPeNDAP to allow users to subset some ASCII data by using this handler. I'm not trying to convert the original data to another format.\nAs an example, I have this data file (same as https://ghrc.nsstc.nasa.gov/pub/fieldCampaigns/gpmValidation/iphex/disdrometers_and_gauges/rain_gauge_NASA/data/gauge/iphex_raingauge_NASA0045_A_20130919_20131229_gag.txt):\n[10:04 AM user]$ head tester.txt\n2013 IPHEx NASA0045_A MetOne 0.254 mm\nYear Mon Day Jday Hr Min Sec Rain[mm] Lat Lon\n2013 09 19 262 21 18 38 0.254 35.31534 -82.87200\n2013 09 21 264 12 07 51 0.254 35.31536 -82.87196\n2013 09 21 264 12 26 15 0.254 35.31536 -82.87196\n2013 09 21 264 12 46 46 0.254 35.31536 -82.87196\n2013 09 21 264 12 57 50 0.254 35.31536 -82.87196\n2013 09 21 264 13 10 10 0.254 35.31536 -82.87196\n2013 09 21 264 13 32 16 0.254 35.31536 -82.87196\n2013 09 21 264 13 53 37 0.254 35.31536 -82.87196\nFirst I made this format file, which has no end of line markup attempts:\nASCII_input_file_header \"gpmrgnaiphx2 gauge header\"\nyear 1 4 short 0\nfield_program 6 10 char 0\ngauge_id 12 21 char 0\ngauge_type 23 28 char 0\nbucket_resolution 30 34 float 3\nbucket_units 36 37 char 0\nlabels 1 54 char 0\nASCII_input_data \"gpmrgnaiphx2 gauge\"\nyear 1 4 short 0\nmonth 7 8 uchar 0\nday_of_month 11 12 uchar 0\nday_of_year 15 17 short 0\nhour 19 20 uchar 0\nminute 23 24 uchar 0\nsecond 27 28 uchar 0\nrain_rate 31 35 float 3\nlatitude 38 45 float 5\nlongitude 48 56 float 5\nThis resulted in errors when I used chkform on it:\n[10:07 AM user]$ chkform -d tester.txt -if tester.fmt -ol ol.log -el el.err\nWelcome to Chkform release 4.2.3 -- an NGDC FreeForm ND application\nMessages have been recorded in el.err\n[10:07 AM user]$ cat el.err\nThere are 2 errors!\nERROR 1: Expecting an End-Of-Line marker\nEXPLANATION: At position 55 in tester.txt (\"gpmrgnaiphx2 gauge header\")\nERROR 2: Setting data bin\nEXPLANATION: getting header file for tester.txt\nNo more messages\nSo per http://docs.opendap.org/index.php/Wiki_Testing/tblfmt I attempted to use the EOL 'special' format descriptor via different incantations...first as a format descriptor with a variable:\n[10:29 AM user]$ cat tester.fmt\nASCII_input_file_header \"gpmrgnaiphx2 gauge header\"\nyear 1 4 short 0\nfield_program 6 10 char 0\ngauge_id 12 21 char 0\ngauge_type 23 28 char 0\nbucket_resolution 30 34 float 3\nbucket_units 36 37 char 0\nlabels 1 54 char 0\nASCII_input_file_header_RETURN \"linebreak\"\nlinebreak 55 56 char 0\nASCII_input_data \"gpmrgnaiphx2 gauge\"\nyear 1 4 short 0\nmonth 7 8 uchar 0\nday_of_month 11 12 uchar 0\nday_of_year 15 17 short 0\nhour 19 20 uchar 0\nminute 23 24 uchar 0\nsecond 27 28 uchar 0\nrain_rate 31 35 float 3\nlatitude 38 45 float 5\nlongitude 48 56 float 5\n[10:30 AM user]$\nErrors were:\nThere are 3 errors!\nERROR 1: Invalid parameter value\nEXPLANATION: Bad number for variable start position: \"linebreak\"\nERROR 2: Problem with variable description line\nEXPLANATION: ASCII_input_file_header_RETURN \"linebreak\"\nERROR 3: Setting data bin\nEXPLANATION: setting an input format for tester.txt\nTry #2, section 1.4.2 of http://docs.opendap.org/index.php/Wiki_Testing/tblfmt says I can use the \"special\" EOL/RETURN as a variable type, so:\n[10:35 AM user]$ cat tester.fmt\nASCII_input_file_header \"gpmrgnaiphx2 gauge header\"\nyear 1 4 short 0\nfield_program 6 10 char 0\ngauge_id 12 21 char 0\ngauge_type 23 28 char 0\nbucket_resolution 30 34 float 3\nbucket_units 36 37 char 0\nlabels 1 54 char 0\nlineb 55 56 RETURN 0\nASCII_input_data \"gpmrgnaiphx2 gauge\"\nyear 1 4 short 0\nmonth 7 8 uchar 0\nday_of_month 11 12 uchar 0\nday_of_year 15 17 short 0\nhour 19 20 uchar 0\nminute 23 24 uchar 0\nsecond 27 28 uchar 0\nrain_rate 31 35 float 3\nlatitude 38 45 float 5\nlongitude 48 56 float 5\n[10:35 AM user]$\nErrors:\nThere are 3 errors!\nERROR 1: Unknown variable type\nEXPLANATION: RETURN\nERROR 2: Problem with variable description line\nEXPLANATION: lineb 55 56 RETURN\nERROR 3: Setting data bin\nEXPLANATION: setting an input format for tester.txt\nI didn't find any examples using EOL or RETURN or file/record headers at http://test.opendap.org/opendap/data/ff/contents.html .\nI did find https://www.ngdc.noaa.gov/ecosys/cdroms/AVHRR97_d1/software/freeform/documnt/ffu31.pdf which gives an example of using RETURN at the top of page 53 (return line is also shown the same in http://docs.opendap.org/index.php/Wiki_Testing/hdrfmts), and so attempted:\n[10:38 AM user]$ cat tester.fmt\nASCII_input_file_header \"gpmrgnaiphx2 gauge header\"\nyear 1 4 short 0\nfield_program 6 10 char 0\ngauge_id 12 21 char 0\ngauge_type 23 28 char 0\nbucket_resolution 30 34 float 3\nbucket_units 36 37 char 0\nlabels 1 54 char 0\nRETURN \"NEW LINE INDICATOR\"\nASCII_input_data \"gpmrgnaiphx2 gauge\"\nyear 1 4 short 0\nmonth 7 8 uchar 0\nday_of_month 11 12 uchar 0\nday_of_year 15 17 short 0\nhour 19 20 uchar 0\nminute 23 24 uchar 0\nsecond 27 28 uchar 0\nrain_rate 31 35 float 3\nlatitude 38 45 float 5\nlongitude 48 56 float 5\nResulting errors:\nThere are 3 errors!\nERROR 1: Invalid parameter value\nEXPLANATION: Bad number for variable start position: \"NEW LINE INDICATOR\"\nERROR 2: Problem with variable description line\nEXPLANATION: RETURN \"NEW LINE INDICATOR\"\nERROR 3: Setting data bin\nEXPLANATION: setting an input format for tester.txt\nDo any of you have some clues/insight/examples, etc., you could share that will steer me in the right direction?\nAlso, is anyone using this data handler \"in the wild\", or does everyone mostly just convert their data to other formats?\nThanks,\nShannon Flynn\nITSC/GHRC"
    },
    {
     "page": "Requesting help with freeform handler and line endings",
     "name": "jgallagher",
     "body": "Shannon,\nI’ll look into this now and get back to you.\nThis code is definitely used in the wild.\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Requesting help with freeform handler and line endings",
     "name": "jgallagher",
     "body": "Shannon,\nHere’s the correct format file - it took me a while because I’ve never written one with two header lines.\nASCII_input_file_header \"gpmrgnaiphx2 gauge header\"\nyear 1 4 short 0\nfield_program 6 10 char 0\ngauge_id 12 21 char 0\ngauge_type 23 28 char 0\nbucket_resolution 30 34 float 3\nbucket_units 36 37 char 0\ncolumn_labels 38 92 char 0\nASCII_input_data \"gpmrgnaiphx2_gauge\"\nyear 1 4 short 0\nmonth 7 8 uchar 0\nday_of_month 11 12 uchar 0\nday_of_year 15 17 short 0\nhour 19 20 uchar 0\nminute 23 24 uchar 0\nsecond 27 28 uchar 0\nrain_rate 31 35 float 3\nlatitude 38 45 float 5\nlongitude 48 56 float 5\nThe trick is to read the new line (or return) character at the end of the first line of the header and include it in the next variable (see ‘column_labels’ in the above ASCII_input_file_header).\nAlso note that I used an underscore in the ASCII_input_data name - this works better since DAP treats spaces as word separators and thus the space would make writing constraints harder.\nIf this does not work for you, let me know!\nJames\nOn Oct 31, 2017, at 09:51, Shannon Flynn <sef...@uah.edu> wrote:\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Requesting help with freeform handler and line endings",
     "name": "Shannon Flynn",
     "body": "Thank you James, this seems to be working!"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: Question about direct access",
     "name": "Nathan Potter",
     "body": "Hi Hubert,\nBy default the server does not allow direct access to (i.e. downloads of) files that it considers “data”.\nYou can allow this access but changing the configuration of the OLFS.\nThe Prolog:\nIf you have done so yet, you will need to set up a local configuration for the OLFS.\nThis can be quickly accomplished by:\n1) Creating the directory /etc/olfs and making it readable and writable\nby the user that is running your Tomcat instance.\n2) Restart Tomcat\nOn startup the OLFS will detect this new directory and copy the default configuration to it.\nThe Fix:\nEdit the file /etc/olfs/olfs.xml\nLocate this line (~line 125):\n<!-- AllowDirectDataSourceAccess / —>\nAnd uncomment it is that it reads:\n<AllowDirectDataSourceAccess />\nRestart Tomcat and you should be good to go.\nSincerely,\nNathan\n> On Oct 30, 2017, at 7:57 PM, Huber Gilt Lopez <hg...@igp.gob.pe> wrote:\n>\n> Hello Nathan\n>\n> How can I disable this message?\n>\n> The requested URL directly references a data source.\n>\n> You must use the OPeNDAP request interface to get data from the data source.\n>\n> Hyrax : Access Denied (403)\n>\n> Best regards\n>\n> Huber\n>\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "LogVerbose",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "We have this setting in bes.conf:\nBES.LogVerbose=no\nHowever our system admins complained that there was still too much logging in bes.log. They claimed that a “Not Found” error created 5 lines of log such as:\n|&|GMT Mon Oct 30 13:47:53 2017|&|pid: 61695|&|beslistener: BES Not Found Error\n|&|GMT Mon Oct 30 13:47:53 2017|&|pid: 61646|&|ip 127.0.0.1, port 51890|&|request received|&|\n|&|GMT Mon Oct 30 13:47:53 2017|&|pid: 61646|&|ip 127.0.0.1, port 51890|&|set context bes_timeout to 300;|&|request received|&|\n|&|GMT Mon Oct 30 13:47:53 2017|&|pid: 61646|&|ip 127.0.0.1, port 51890|&|set context errors to xml;|&|request received|&|\n|&|GMT Mon Oct 30 13:47:53 2017|&|pid: 61646|&|ip 127.0.0.1, port 51890|&|show catalog for GPM_L3/GPM_3IMERGHHL.03/2015/181/;|&|request received|&|\nand a success can generate upward of 50 lines.\nCan the log be further reduced by a ‘no’ setting of the LogVerbose?\n-Fan"
    },
    {
     "page": "LogVerbose",
     "name": "jgallagher",
     "body": "- show quoted text -\nFunny, I was just looking at that. We are in the process of making those logs more concise.\nMy understanding is that NASA’s SAs requested the logs use ‘|&|’ as a separator, is that correct? Also, do we need the leading separator?\nRegarding what _is_ logged: Would a standard (not verbose) log have one line per request? So for the above 'show catalog …’ request, there would be just one line saying something like:\n|&|GMT Mon Oct 30 13:47:53 2017|&|pid: 61646|&|ip 127.0.0.1, port 51890|&|show catalog for GPM_L3/GPM_3IMERGHHL.03/2015/181/;|&|request received|&|\nOr more concisely:\nGMT Mon Oct 30 13:47:53 2017|&|61646|&|127.0.0.1: 51890|&|show catalog for GPM_L3/GPM_3IMERGHHL.03/2015/181/;\nWe can do this and get it in 1.14.1, just let me know.\nJames\n-Fan\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "LogVerbose",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi James,\nI don’t recall I requested ‘|&|’ as the separator in bes.log. It could be from someone else at NASA. We are ok if keeping the separator, including the leading one. Reducing the log to one (or two) line per request in ‘BES.LogVerbose=no’ setting would be great, as long as it can indicate an error if any. Thanks.\n-Fan\nFrom: James Gallagher <jgall...@opendap.org>\nDate: Monday, October 30, 2017 at 11:42 AM\nTo: \"Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]\" <fan.f...@nasa.gov>\nCc: James Gallagher <jgall...@opendap.org>, \"sup...@opendap.org\" <sup...@opendap.org>\nSubject: Re: [support] LogVerbose\n- show quoted text -"
    },
    {
     "page": "LogVerbose",
     "name": "jgallagher",
     "body": "On Oct 30, 2017, at 10:48, Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC] <fan.f...@nasa.gov> wrote:\nHi James,\nI don’t recall I requested ‘|&|’ as the separator in bes.log. It could be from someone else at NASA. We are ok if keeping the separator, including the leading one. Reducing the log to one (or two) line per request in ‘BES.LogVerbose=no’ setting would be great, as long as it can indicate an error if any. Thanks.\nOK. Here’s the issue in Jira - let me know if you cannot see and/or watch the issue.\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Fwd: [support] Need help to get the compatible version of opendap for the matlab 2013a",
     "name": "Nathan Potter",
     "body": "Hi Serge,\nModern versions of matlab have built in DAP support through the NetCDF enablement.\nIn section 6.1.1 of this document:\nhttps://opendap.github.io/documentation/UserGuideComprehensive.html#_clients\nThere is a brief example that may be helpful to you in this regard.\nLet us know if you need additional assistance.\nSincerely,\nNathan\nBegin forwarded message:\nFrom: serge tomety <ser...@hotmail.fr>\nSubject: [support] Need help to get the compatible version of opendap for the matlab 2013a\nDate: August 28, 2017 at 05:45:20 MDT\nTo: \"sup...@opendap.org\" <sup...@opendap.org>\nTo whom it may concern,\nI am Serge Tomety, PhD student at UCT, I would like to use opendap to download data.\nfor that I Dowloaded loaddap-3.7.3 and install it on the computer. But I got the message that :\n<<loaddap that matlab tried to run was not compatible with your computer.>>\nCould you help me to get the compatible version.\nThank you in advanced.\nSerge\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Subsets MERRA2 data no updated",
     "name": "Javier Peña",
     "body": "Dear Mr./Mrs.,\nI am trying to download the subsets of the following MERRA2 data but\nthey are not updated:\n- inst6_3d_ana_Nv:\nhttps://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2I6NVANA.5.12.4/contents.html\n- inst6_3d_ana_Np:\nhttps://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2I6NPANA.5.12.4/contents.html\nNevertheless, the folIowing subsets are updated:\n- tavg1_2d_slv_Nx:\nhttps://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/contents.html\n- tavg1_2d_ocn_Nx:\nhttps://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXOCN.5.12.4/contents.html\nIf possible I would like to know if there is any problem with these\nsubsets of data and when the updated data are going to be available.\nThank you very much in advance for your help.\nYours sincerely,\nJavier Peña\n--\nJavier Peña\nEREDA S.L.U.\nPaseo del Marqués de Monistrol 7, 28011 Madrid\nTel: +34.915014755\nFax: +34.915014756\nwww.ereda.com\nEn cumplimiento de la Ley Orgánica 15/1999 de Protección de Datos\nPersonales, le informamos que sus datos están incorporados a nuestros\nficheros, con la finalidad de mantener relaciones profesionales y/o\ncomerciales. Si lo desea puede ejercer los derechos de acceso,\nrectificación, cancelación y oposición de sus datos dirigiéndose a:\nEREDA S.L.U., Paseo Marqués de Monistrol 7, 28011 Madrid, o bien\nescribiendo a la dirección de correo electrónico ereda...@ereda.com,\nponiendo en el asunto “BAJA”. El contenido de este correo\nelectrónico/fax y sus documentos adjuntos va dirigido únicamente a la\npersona o entidad que se muestra como destinatario y puede contener\ndatos confidenciales o privilegiados. Si Ud. no es el destinatario y\nrecibe este mail/fax por error, rogamos se ponga en contacto con\nnosotros y lo destruya con todos sus documentos adjuntos sin leerlos\nni hacer ningún uso de los datos que en ellos figuren, ateniéndose a\nlas consecuencias que de un uso indebido de dichos datos puedan\nderivarse.\nIn compliance with the provisions of Organic Law 15/1999 of 13\nDecember 1999, relating to the Protection of Personal Data, EREDA\nS.L.U. guarantees the security and confidentiality of the data\nprovided. The aim of these personal details collection is the\ncommercial contact procedure, advertising and commercial research, all\nof them coincident purposes with those declared before the Information\nProtection Spanish Agency. Consequently, you, as information holder,\ngive permission and authorisation to the File Responsible to include\nthem in the ut supra detailed file. You declare that you are informed\nabout the detailed conditions in the present clause, as well as you\npromise to update your details, and, in any case, you will be able to\nfreely exercise the rights of access, rectification, cancellation and\nopposition (always in accordance with the assumption considered with\nthe legislation currently in force) by writing to\nereda...@ereda.com. If you are not the intended recipient, you are\nhereby notified that any disclosure, copying, distribution, or use of\nthe information contained herein (including any reliance thereon) is\nSTRICTLY PROHIBITED. If you received this transmission in error,\nplease immediately contact the sender and destroy the material in its\nentirety, whether in electronic or hard copy format."
    },
    {
     "page": "Subsets MERRA2 data no updated",
     "name": "Nathan Potter",
     "body": "Hi Javier,\nI checked these data today and the most recent files for each collection was September 30 2017. Since they are all ending at the place it would seem that the data have been updated since you wrote.\nIs that the case?\nThanks,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    }
   ]
  },
  {
  },
  {
   "name": [
    {
     "page": "logrotate practice",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi. It seems that ‘/etc/logrotate.d’ in the release 1.13.4 RPM restarts bes and tomcat every day. This would be fine if we had not other processes relying on tomcat but has since caused troubles to these processes. In a few cases the bes failed to restart also.\nThis is at least in conflict with our own processes that truncates and backup the previous day’s log, which do not require restarting the server. Can we adopt a similar practice in Hyrax, or at least a less brute-force one?\n-Fan"
    },
    {
     "page": "logrotate practice",
     "name": "jgallagher",
     "body": "On Aug 29, 2017, at 09:22, Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC] <fan.f...@nasa.gov> wrote:\nHi. It seems that ‘/etc/logrotate.d’ in the release 1.13.4 RPM restarts bes and tomcat every day. This would be fine if we had not other processes relying on tomcat but has since caused troubles to these processes. In a few cases the bes failed to restart also.\nThis is at least in conflict with our own processes that truncates and backup the previous day’s log, which do not require restarting the server. Can we adopt a similar practice in Hyrax, or at least a less brute-force one?\nSure. (Sorry for the long delay in responding)\nCan you send me the logrotate code/setting you are using?\nJames\n-Fan\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "logrotate practice",
     "name": "Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]",
     "body": "Hi James,\nSorry for the late response. I have below a logrotate setting that we use on a typical server. Let me know if you need more info.\n-Fan\ncat /etc/logrotate.d/bes\n/var/log/bes/bes.log {\nweekly\nrotate 26\ncreate 0644 bes bes\nmissingok\nnotifempty\ncopytruncate\ncompress\n}\ncat /etc/logrotate.conf\n# see \"man logrotate\" for details\n# rotate log files weekly\nweekly\n# keep 52 weeks worth of backlogs\nrotate 52\n# create new (empty) log files after rotating old ones\ncreate\n# use date as a suffix of the rotated file\ndateext\n# default directory to rotate old logs into\nolddir /var/log/old/\n# uncomment this if you want your log files compressed\n#compress\n# RPM packages drop log rotation information into this directory\ninclude /etc/logrotate.d\n# no packages own wtmp and btmp -- we'll rotate them here\n/var/log/wtmp {\nmonthly\ncreate 0664 root utmp\n#minsize 1M\ncompress\nrotate 12\n}\n/var/log/btmp {\nmissingok\nmonthly\ncreate 0600 root utmp\nrotate 12\n}\n# system-specific logs may be also be configured here.\nFrom: James Gallagher <jgall...@opendap.org>\nDate: Monday, October 16, 2017 at 11:49 AM\nTo: \"Fang, Fan (GSFC-610.2)[ADNET SYSTEMS INC]\" <fan.f...@nasa.gov>\nCc: James Gallagher <jgall...@opendap.org>, \"sup...@opendap.org\" <sup...@opendap.org>\nSubject: Re: [support] logrotate practice\n- show quoted text -\n--\nJames Gallagher\njgallagher@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Data of China's Import & Export",
     "name": "Alisa",
     "body": "Hi Sir,\nThis is CMC Co.,Ltd. We can provide you with the latest and most authoritative information of China's export & import data.\nIt will help you to export to China or import from China with the best price.\nPlease refer to the following format:\nWe hope to hear from you soon.\nBest regards\nAlisa\nCMC Co., Ltd\nNo.11-1 Shidaixingkong,Three Gorges Square, Shapingba District, Chongqing, 400040 China\nE-mail: mikequan@vip.sina.com\nTel: +86 23 89311987 Mob : +86 158 0232 2894\nSkype: commerceone20\nWhatsapp: +86 158 0232 2894\nQQ: 2669362635\nWEB: www.commercecq.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Exciting updates from DockerCon Europe 2017",
     "name": "Docker Team",
     "body": "Announcements from DockerCon Europe\nIt has been a great week in Copenhagen, Denmark where we have been interacting with customers, partners, and the community and celebrating the impact Docker has in driving technology change across every industry around the world. At DockerCon Europe 2017 we shared some exciting new developments:\nSeamless Integration of Kubernetes into the Docker Platform\nDocker is offering users the choice to use Kubernetes and/or Docker Swarm for orchestration while maintaining the consistent developer to operator workflow users have come to expect from Docker. This announcement includes:\nDocker Enterprise Edition\nDocker Community Edition on the desktop with Docker for Mac and Windows\nMoby Project and Kubernetes\nBringing Kubernetes to Docker Enterprise Edition (EE) will simplify and advance the management of Kubernetes for enterprise IT and deliver the advanced capabilities of Docker EE, including security and management, to a broader set of applications. To learn more about this announcement, visit www.docker.com/kubernetes.\nSign up now for the Beta\nDocker Expands Modernize Traditional Application (MTA) Program\nWe are happy to share that IBM is joining the Docker MTA Program to migrate apps to the cloud and instantly add modern capabilities like the Watson AI platform.The Modernize Traditional Applications Program by Docker provides the only incremental, non-disruptive approach to modernization; No re-coding, re-educating and re-organizing required.\nReady to learn more?\nRegister for an upcoming Docker EE webinar\nLearn more about Docker EE and view pricing plans\nContact Sales for a demo or questions\nThanks!\nThe Docker Team\nDocker, Inc.\n144 Townsend Street\nSan Francisco, CA 94107\nConnect with us:\n© Docker 2017\nYou have received this email because you signed up to receive information and offers from Docker ®.\nIf you would like to be excluded from further e‐mail communications, unsubscribe here."
    }
   ]
  },
  {
   "name": [
    {
     "page": "500 Internal Server Error For Accessing OPeNDAP_Specific MERRA-2 inst6_3d_ana Datasets",
     "name": "Xiaojing Quan",
     "body": "Dear Opendap user support coordinator:\nI am a registered user of Earthdata. Recently I have been working on downloading the MERRA-2 data products for our project. This morning I have some problem to access the specific MERRA-2 inst3_3d_ana_Np, MERRA-2 inst3_3d_asm_Np by OPENDAP DATA. I believed that the URL I have submitted for these datasets should have worked, since I could access successfully before 17th October, 2017.\nSo I would like to kindly to enquire with you that, whether there would be any possible that the server was broken , or there would be some problems about my user account?\nIf so, are there any possible solutions I could do for fixing it?\nI have attached a picture of broken page for your reference .\nThank you very much for your kind help and suggestion about this issue.\nBest regards,\nXiaojing Quan\n=============================================================\nXiaojing Quan\nPostdoctoral Fellow\nDepartment of Geography and Environmental Studies, Carleton University\nA301C Loeb Building\n1125 Colonel By Drive\nOttawa, ON K1S 5B6\nEmail: xiaoji...@carleton.ca\nPhone:+1 613-520-2600 ext.6294"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi , I hope you’re having a great day! I came across Open Source Project For A Network a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a new tech service specifically built for small and volunteer-run nonprofits. We help you save time and money by automating your fundraising, in one place, with no technical work required on your end.\nYour Flipcause subscription comes with every feature and service we have to offer (see below) which includes a dedicated, four-person Success Team who will set up your fundraising campaigns, create banners and images, modernize your website, and much, much more...\nYour Success Team will do unlimited tasks for you, on demand, at no additional cost, so you can spend more time on your mission and less time on technology!\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you are open to a quick conversation, please take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you are not interested, please let me know and you will never hear from me again. :)\nThank you for your time today and I hope to hear from you soon!\nJoan\nHere are the features that come with your Flipcause subscription…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nPeer to peer fundraising\nTeam fundraising\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated, four-person Success Team with free fundraising and technical assistance\nFree, modernized website with social media integration\nUnlimited, fully customizable fundraising pages to market your campaigns\nDonor dashboard to keep track of your supporters in one place\nEasy to transfer fundraising activity into your accounting system\nSortable donor data for targeted mass email communication\n--\nJoan Ramirez | Flipcause\nCommunity Outreach Representative\n311 Oak Street Ste 110\nOakland, CA 94607\noffice: (800) 523-1950 ext. 3\nwww.flipcause.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Re: OPeNDAP GeoTIFF Aggregation",
     "name": "jgallagher",
     "body": "On Oct 6, 2017, at 08:39, Friesz (CTR), Aaron <aaron.fr...@usgs.gov> wrote:\nHi James,\nWe've recently ran into an issue when trying to aggregate a series of GeoTIFF files. Brad has written up a thorough description of what we are trying to do and what we've tried (see attached document). He and Jason have also put together a bundle of files to hopefully help replicate the problem on your end. The bundle can be found here: https://e4ftl01.cr.usgs.gov/PullDir/OPeNDAP_Aggregation/. It includes:\n- 4 GeoTIFF ARD files.\n- 4 ncml files that rename band_1 to SRB2 - 1 ncml file for 1 tiff file.\n- 2 ncml files that do the time aggregation. One with 3 of the ncml files (this one works) and one with all 4 ncml files (this one doesn't work).\nPlease let us know if you need anything else.\nWhat version of Hyrax are you using?\nThanks,\nAaron\n--\nAaron FrieszData ScientistInnovate!, Inc., Contractor to the U.S.Geological Survey (USGS)USGS/EROS CenterSioux Falls, SD 57198\nPhone: 605-594-6526 Email: aaron.fr...@usgs.gov\nWebsite: https://lpdaac.usgs.gov/\nNOTE – Email address change as of April 4, 2016. Please update your contact list accordingly.\n<GeoTIFF aggregation in OPeNDAP.docx>\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "Re: OPeNDAP GeoTIFF Aggregation",
     "name": "Friesz (CTR), Aaron",
     "body": "Hey James,\nWe are using 1.13.4. We couldn't get time aggregation to work with that. So we built the gdal handler from source and replaced that module in the 1.13.4 install\nAaron"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Indian Data with Higher Time Resolution",
     "name": "ywang",
     "body": "Dear Supporter,\nThis is Wang Yuchen, a Master student from Earthquake Research\nInstitute, the University of Tokyo.\nI am currently looking for tide station data around the Indian Ocean\nthat recorded the tsunami of 2004 Sumatra Earthquake. The data of Male\nand Colombo will be useful to my research. I logged in the Research\nQuality ftp of your website, but only find the hourly data. May I ask if\nthere is any data with higher time resolution?\nThank you very much for your attention.\nBest Regards,\nWang Yuchen"
    },
    {
     "page": "Indian Data with Higher Time Resolution",
     "name": "jgallagher",
     "body": "On Oct 2, 2017, at 23:37, ywang <yw...@eri.u-tokyo.ac.jp> wrote:\nDear Supporter,\nThis is Wang Yuchen, a Master student from Earthquake Research Institute, the University of Tokyo.\nI am currently looking for tide station data around the Indian Ocean that recorded the tsunami of 2004 Sumatra Earthquake. The data of Male and Colombo will be useful to my research. I logged in the Research Quality ftp of your website, but only find the hourly data. May I ask if there is any data with higher time resolution?\nCan you send me the URL you are accessing? We are not the data providers, but are the data server software developers. If I have the URL, I can help you get in contact with the data providers and they can answer your question.\nThanks,\nJames\nThank you very much for your attention.\nBest Regards,\nWang Yuchen\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "AirMSPI Products libdap exception building response: error_code = 1002",
     "name": "Tisdale, Matthew S. (LARC-E301)[STARSS III Affiiate]",
     "body": "The ASDC has some new products that we are trying to add to Hyrax. The products natively have a .hdf file extension, even though they are HDF-EOS5 data products. I changed the file extension to .he5, so they are associated with the correct handler, but I am still getting errors. Panoply can open these products and plot them with no issues. I am not sure if there might be an issue within the data products that I am not seeing or if the issue resides elsewhere.\nI included a link to the files below and the file access is turned on so that they can be downloaded locally if needed.\nhttps://opendap.larc.nasa.gov/opendap/hyrax/.AirMSPI/AirMSPI/\nAny help would be greatly appreciated.\nRegards,\nMatt Tisdale\nNASA ASDC\n(757) 864-3397"
    },
    {
     "page": "AirMSPI Products libdap exception building response: error_code = 1002",
     "name": "jgallagher",
     "body": "On Oct 5, 2017, at 06:51, Tisdale, Matthew S. (LARC-E301)[STARSS III Affiiate] <matthew....@nasa.gov> wrote:\nKent,\nCan you help with information on how Matt might change the configuration of the HDF5 handler for this case?\nThanks,\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "AirMSPI Products libdap exception building response: error_code = 1002",
     "name": "Kent Yang",
     "body": "Hi Matthew,\nCould you point to me where I can download the HDF-EOS5 file? The CF option only supports two HDF-EOS5 projections: geographic and sinusoidal projections? I am planning to support more projections. So this is the right time. To check the non-CF output, you may change H5.EnableCF=false in the h5.conf. By this way I think the data can be accessed but no CF handling on the data.\nKent\nFrom: James Gallagher [mailto:jgall...@opendap.org]\nSent: Thursday, October 05, 2017 11:58 AM\nTo: Tisdale, Matthew S. (LARC-E301)[STARSS III Affiiate]; Kent Yang\nCc: James Gallagher; sup...@opendap.org\nSubject: Re: [support] AirMSPI Products libdap exception building response: error_code = 1002\n- show quoted text -"
    },
    {
     "page": "AirMSPI Products libdap exception building response: error_code = 1002",
     "name": "Kent Yang",
     "body": "Matthew,\nSeems I can download the files from your URL. No need to point to me the original files. Stay tuned.\nKent\nFrom: Kent Yang\nSent: Thursday, October 05, 2017 1:08 PM\nTo: 'James Gallagher'; Tisdale, Matthew S. (LARC-E301)[STARSS III Affiiate]\nCc: James Gallagher; sup...@opendap.org\nSubject: RE: [support] AirMSPI Products libdap exception building response: error_code = 1002\nHi Matthew,\nCould you point to me where I can download the HDF-EOS5 file? The CF option only supports two HDF-EOS5 projections: geographic and sinusoidal projections? I am planning to support more projections. So this is the right time. To check the non-CF output, you may change H5.EnableCF=false in the h5.conf. By this way I think the data can be accessed but no CF handling on the data.\nKent\nFrom: James Gallagher [mailto:j...@opendap.org]\nSent: Thursday, October 05, 2017 11:58 AM\nTo: Tisdale, Matthew S. (LARC-E301)[STARSS III Affiiate]; Kent Yang\nCc: James Gallagher; sup...@opendap.org\nSubject: Re: [support] AirMSPI Products libdap exception building response: error_code = 1002\nOn Oct 5, 2017, at 06:51, Tisdale, Matthew S. (LARC-E301)[STARSS III Affiiate] <matthew....@nasa.gov> wrote:\nKent,\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Start BES not as root",
     "name": "Aron Bartle",
     "body": "Hello Nathan & James, hope all is well.\nA colleague at the data center and I were wondering if there is any way to run BES not as root?\nWalt is trying to deploy on a type of auto-scaling system where the policy disallows any process to be executed as root.\nI see that besdaemon changes to user \"bes\" after its started, but presumably they won't go for that because it still started as root.\nWhat I don't know is why besdaemon needs to run as root at startup... could it detect the current user is the same as \"BES.user\" and allow execution?\nThanks,\n-Aron"
    },
    {
     "page": "Start BES not as root",
     "name": "jgallagher",
     "body": "- show quoted text -\nI think that a daemon has to start as root (because of some of the start up activities - dis-assocaites from a terminal, et c.).\nThanks,\n-Aron\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Kevin Cooper sent a message to Open Dap",
     "name": "NearFinderUS",
     "body": "Kevin Cooper\nHi,\nYour business is registered on this site in the category Computer Software & Services. Are you still working with this?\n- Kevin Cooper\nOpen Dap\nComputer Software & Services, Computer Software Publishers Developers\n125 W Granite St # 202\n4067238663\nUpdate business info\nSee my business\nIs this business or service no longer active?\n* reply this email and we will forward the message to the user automatically.\nUnsubscribe\nYou are receiving this email because a user has contacted your business through NearFinderUS.\nThis email was sent to Open Dap.\n© 2017 NearFinderUS."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Maximize your pipeline",
     "name": "Ashley Jones",
     "body": "Hi Peter,\nSorry for another email. I realize you are busy and that it is difficult to read/respond to all of the emails you receive. I completely get it. I'm hoping you are the right person to talk to about sales developments strategies at The Open Source Project for a Network Data Access Protocol.\nI'm looking to get on your calendar for just 10 minutes in the next week or so to chat and see if we can enhance your current SDR initiatives by supplementing your process. We're confident we can.\nLet me know what you think.\nThanks\n--\nAshley Jones | Account Manager\nEmail: ash...@boomsourcing.com\n2940 W Maple Loop Lehi, UT 84043"
    }
   ]
  },
  {
   "name": [
    {
     "page": "the URL you submitted should have worked",
     "name": "Gustavo Adolfo Naranjo Toro",
     "body": "Hello\nI am using MGET (version 0.8 to 67) the tool \"find Cayula-CornillonFronts in GHRSST L4 SST\" \"GHRSST L4 product:\" MUR-JPL-L4-GLOB-v4.1. \"\nI previously worked without problems now has the following error\n\"\" \"Failed to download THREDDS catalog http://podaac-opendap.jpl.nasa.gov/opendap/allData/ghrsst/data/GDS2/L4/catalog.xml The HTTP request failed with SSLError: [Errno 8] _ssl. c: 504: EOF occurred in violation of protocol Retrying. \"\"\nverifying the URL this has no information available.\nabjunto image of the problem\nTHANKS, I will Stay attentive to you answer\nATTE\nGUSTAVO ADOLFO NARANJO TORO\nNATIONAL UNIVERSITY OF COLOMBIA"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Michael Treglia",
     "body": "Hi,\nHi, my name is Mike Treglia, and I work with The Nature Conservancy’s NYC Program. I have been working to retrieve spatial and temporal subsets of the TopoWx data via OpenDAP, and it seems when I’m requesting data a large number of time points, the request hangs up. I just figured I’d see if there is a limit from OpenDAP on the number of requests within a period of time or anything similar.\nIf it helps, I’m doing this in R, with sample code here: https://gist.github.com/mtreg/e86c516a4ace3ec54e8bd36bcd9da875 - this works great with up time periods extending about 30 days (not sure what the max limit is that I reach), but when I try for ~150 days, for example, I get a NetCDF I/O Failure error in R. (Note: The R netcdf dependencies are best run on MacOS or Linux, as they are not compiled for Windows to access OpenDAP)\nThus, I can certainly batch the data calls, but would like to do so for the largest number of requests possible. And if it’s not a limit on the OpenDAP side, that would be great to know too.\nThanks so much for any support on this issue.\nBest regards,\nMike\nPlease consider the environment before printing this email.\n​\n​ Michael Treglia, Ph.D.\nUrban Spatial Planner\nmichael...@tnc.org\n(646) 647-2045 ​(Office)\n(917) 841-5603 (Mobile)\n(212) 997-8451 (Fax)\nnature.org\n​\nThe Nature Conservancy\nNew York\nNew York City Program\n322 Eighth Avenue, 16th Floor\nNew York, NY, 10001\n​\nInformation contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges."
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Nathan Potter",
     "body": "Hi Mike,\nThere are several implementations of the opendap service, and they have different kinds of limitations. If you could share the URL of the server that you are working with I could be a lot more helpful.\nThanks,\nNathan\n> On Sep 19, 2017, at 7:21 AM, Michael Treglia <michael...@TNC.ORG> wrote:\n>\n> Hi,\n>\n> Hi, my name is Mike Treglia, and I work with The Nature Conservancy’s NYC Program. I have been working to retrieve spatial and temporal subsets of the TopoWx data via OpenDAP, and it seems when I’m requesting data a large number of time points, the request hangs up. I just figured I’d see if there is a limit from OpenDAP on the number of requests within a period of time or anything similar.\n>\n> If it helps, I’m doing this in R, with sample code here: https://gist.github.com/mtreg/e86c516a4ace3ec54e8bd36bcd9da875 - this works great with up time periods extending about 30 days (not sure what the max limit is that I reach), but when I try for ~150 days, for example, I get a NetCDF I/O Failure error in R. (Note: The R netcdf dependencies are best run on MacOS or Linux, as they are not compiled for Windows to access OpenDAP)\n>\n> Thus, I can certainly batch the data calls, but would like to do so for the largest number of requests possible. And if it’s not a limit on the OpenDAP side, that would be great to know too.\n>\n> Thanks so much for any support on this issue.\n> Best regards,\n> Mike\n>\n>\n>\n> Please consider the environment before printing this email.\n> ​\n> ​ Michael Treglia, Ph.D.\n> Urban Spatial Planner\n> michael...@tnc.org\n> (646) 647-2045 ​(Office)\n> (917) 841-5603 (Mobile)\n> (212) 997-8451 (Fax)\n>\n> nature.org\n> ​\n>\n> The Nature Conservancy\n> New York\n> New York City Program\n> 322 Eighth Avenue, 16thFloor\n> New York, NY, 10001\n> ​\n> <image002.png>\n> Information contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges.\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Michael Treglia",
     "body": "Ni Nathan,\nThanks for the quick response! The URL I’m calling from is here: https://cida.usgs.gov/thredds/dodsC/topowx.html (in the R code the html gets left off).\nBest,\nMike\nPlease consider the environment before printing this email.\n​\n​ Michael Treglia, Ph.D.\nUrban Spatial Planner\nmichael...@tnc.org\n(646) 647-2045 ​(Office)\n(917) 841-5603 (Mobile)\n(212) 997-8451 (Fax)\nnature.org\n​\nThe Nature Conservancy\nNew York\nNew York City Program\n322 Eighth Avenue, 16th Floor\nNew York, NY, 10001\n​\nInformation contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges.\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Tuesday, September 19, 2017 11:21 AM\nTo: Michael Treglia <michael...@TNC.ORG>\nCc: Nathan Potter <n...@opendap.org>; dblo...@usgs.gov; sup...@opendap.org\nSubject: Re: [support] Question about retrieval of TopoWx data via OpenDAP\n- show quoted text -"
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Nathan Potter",
     "body": "Hi Mike,\nThat is an instance of the THREDDS Data Server (aka the TDS: http://www.unidata.ucar.edu/http://www.unidata.ucar.edu/software/thredds/current/tds//thredds/current/tds/ ) which implements the OPeNDAP protocol. I believe that it does have response size limitations and that what you may have to do is craft requests that subset the data on the server's end.\nThings to try:\n- Reducing the spatial area by subsetting the requested arrays.\n- Iterating over time points/periods to incrementally retrieve the entire series.\nDoes that make sense?\nSincerely,\nNathan\n> On Sep 19, 2017, at 8:29 AM, Michael Treglia <michael...@TNC.ORG> wrote:\n>\n> Ni Nathan,\n>\n> Thanks for the quick response! The URL I’m calling from is here: https://cida.usgs.gov/thredds/dodsC/topowx.html (in the R code the html gets left off).\n>\n> Best,\n> Mike\n>\n> Please consider the environment before printing this email.\n> ​\n- show quoted text -"
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "David Blodgett",
     "body": "Hi Michael,\nI would guess you are hitting the THREDDS/OPeNDAP single request size limit, but it may be something network related.\nI would suggest chunking in time based on a single request taking no more than about 10 seconds to complete. Don't worry about sending lots of requests if you need to. Our THREDDS server can handle lots of small requests if need be.\n- Dave\nNote the opendap.org folks probably don't need to worry about this since we are running THREDDS.\nOn Sep 19, 2017, at 9:21 AM, Michael Treglia <michael...@TNC.ORG> wrote:\nHi,\nHi, my name is Mike Treglia, and I work with The Nature Conservancy’s NYC Program. I have been working to retrieve spatial and temporal subsets of the TopoWx data via OpenDAP, and it seems when I’m requesting data a large number of time points, the request hangs up. I just figured I’d see if there is a limit from OpenDAP on the number of requests within a period of time or anything similar.\nIf it helps, I’m doing this in R, with sample code here: https://gist.github.com/mtreg/e86c516a4ace3ec54e8bd36bcd9da875 - this works great with up time periods extending about 30 days (not sure what the max limit is that I reach), but when I try for ~150 days, for example, I get a NetCDF I/O Failure error in R. (Note: The R netcdf dependencies are best run on MacOS or Linux, as they are not compiled for Windows to access OpenDAP)\nThus, I can certainly batch the data calls, but would like to do so for the largest number of requests possible. And if it’s not a limit on the OpenDAP side, that would be great to know too.\nThanks so much for any support on this issue.\nBest regards,\nMike\nPlease consider the environment before printing this email.\n​\n​ Michael Treglia, Ph.D.\nUrban Spatial Planner\nmichael...@tnc.org\n(646) 647-2045 ​(Office)\n(917) 841-5603 (Mobile)\n(212) 997-8451 (Fax)\nnature.org\n​\nThe Nature Conservancy\nNew York\nNew York City Program\n322 Eighth Avenue, 16th Floor\nNew York, NY, 10001\n​\n<image002.png>\nInformation contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges."
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Michael Treglia",
     "body": "Thanks Nathan,\nThat makes perfect sense (and thanks for clarifying that it’s an instance of the THREDDS data server).\nI was wondering if there was a known response size limit, and if that is known, great, but otherwise I can just go with what I find works.\nThanks again,\nMike\nPlease consider the environment before printing this email.\n​\n​ Michael Treglia, Ph.D.\nUrban Spatial Planner\nmichael...@tnc.org\n(646) 647-2045 ​(Office)\n(917) 841-5603 (Mobile)\n(212) 997-8451 (Fax)\nnature.org\n​\nThe Nature Conservancy\nNew York\nNew York City Program\n322 Eighth Avenue, 16th Floor\nNew York, NY, 10001\n​\nInformation contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges.\nFrom: Nathan Potter [mailto:n...@opendap.org]\nSent: Tuesday, September 19, 2017 11:36 AM\nTo: Michael Treglia <michael...@TNC.ORG>\nCc: Nathan Potter <n...@opendap.org>; dblo...@usgs.gov; sup...@opendap.org\nSubject: Re: [support] Question about retrieval of TopoWx data via OpenDAP\nHi Mike,\n- show quoted text -\n> From: Nathan Potter [mailt...@opendap.org]\n- show quoted text -"
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Michael Treglia",
     "body": "Thanks Dave – this is helpful!\nI’ve actually had success with batches taking up to ~1500 seconds (for ~3 months of daily max temp data), though I’m not sure if the R dependencies make a separate request per ‘layer’ (i.e, date in this case). But I appreciate the guidance, and I can certainly chunk this to ~10 second requests to make sure I don’t get hung up mid-way through a big process.\nBest regards,\nmike\nPlease consider the environment before printing this email.\n​\n​ Michael Treglia, Ph.D.\nUrban Spatial Planner\nmichael...@tnc.org\n(646) 647-2045 ​(Office)\n(917) 841-5603 (Mobile)\n(212) 997-8451 (Fax)\nnature.org\n​\nThe Nature Conservancy\nNew York\nNew York City Program\n322 Eighth Avenue, 16th Floor\nNew York, NY, 10001\n​\nInformation contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges.\nFrom: David Blodgett [mailto:dblo...@usgs.gov]\nSent: Tuesday, September 19, 2017 11:38 AM\nTo: Michael Treglia <michael...@TNC.ORG>\nCc: sup...@opendap.org\nSubject: Re: Question about retrieval of TopoWx data via OpenDAP\nHi Michael,\nI would guess you are hitting the THREDDS/OPeNDAP single request size limit, but it may be something network related.\nI would suggest chunking in time based on a single request taking no more than about 10 seconds to complete. Don't worry about sending lots of requests if you need to. Our THREDDS server can handle lots of small requests if need be.\n- Dave\nNote the opendap.org folks probably don't need to worry about this since we are running THREDDS.\nOn Sep 19, 2017, at 9:21 AM, Michael Treglia <michael...@TNC.ORG> wrote:\nHi,\nHi, my name is Mike Treglia, and I work with The Nature Conservancy’s NYC Program. I have been working to retrieve spatial and temporal subsets of the TopoWx data via OpenDAP, and it seems when I’m requesting data a large number of time points, the request hangs up. I just figured I’d see if there is a limit from OpenDAP on the number of requests within a period of time or anything similar.\nIf it helps, I’m doing this in R, with sample code here: https://gist.github.com/mtreg/e86c516a4ace3ec54e8bd36bcd9da875 - this works great with up time periods extending about 30 days (not sure what the max limit is that I reach), but when I try for ~150 days, for example, I get a NetCDF I/O Failure error in R. (Note: The R netcdf dependencies are best run on MacOS or Linux, as they are not compiled for Windows to access OpenDAP)\nThus, I can certainly batch the data calls, but would like to do so for the largest number of requests possible. And if it’s not a limit on the OpenDAP side, that would be great to know too.\nThanks so much for any support on this issue.\nBest regards,\nMike\nPlease consider the environment before printing this email.\n​\n​ Michael Treglia, Ph.D.\nUrban Spatial Planner\nmichael.treglia@tnc.org\n(646) 647-2045 ​(Office)\n(917) 841-5603 (Mobile)\n(212) 997-8451 (Fax)\nnature.org\n​\nThe Nature Conservancy\nNew York\nNew York City Program\n322 Eighth Avenue, 16th Floor\nNew York, NY, 10001\n​\n<image002.png>\nInformation contained in this electronic communication and any attachments transmitted within may contain information that is LEGALLY PRIVILEGED, CONFIDENTIAL OR EXEMPT FROM DISCLOSURE UNDER APPLICABLE LAW. It is only for the use of the individual to whom it is addressed. If you are not the intended recipient, you are hereby notified that any review, release, retransmission, copying, dissemination or other use of, or taking any action in reliance upon this communication, is strictly prohibited. If you have received this communication in error, please immediately notify the sender by reply e-mail and permanently delete the material from your computer and destroy any printed copies. Receipt by anyone other than the intended recipient is not a waiver of any attorney-client, attorney work product or other privileges."
    },
    {
     "page": "Question about retrieval of TopoWx data via OpenDAP",
     "name": "Nathan Potter",
     "body": "HiMike,\nIt looks like the default is 500MB per request for binary OPeNDAP responses:\nhttp://www.unidata.ucar.edu/software/thredds/current/tds/reference/ThreddsConfigXMLFile.html#opendap\nBut that may be adjusted by the data provider.\nSincerely,\nNathan\n> On Sep 19, 2017, at 8:39 AM, Michael Treglia <michael...@TNC.ORG> wrote:\n>\n> Thanks Nathan,\n>\n> That makes perfect sense (and thanks for clarifying that it’s an instance of the THREDDS data server).\n>\n> I was wondering if there was a known response size limit, and if that is known, great, but otherwise I can just go with what I find works.\n>\n> Thanks again,\n> Mike\n>\n> Please consider the environment before printing this email.\n> ​\n> ​ Michael Treglia, Ph.D.\n> Urban Spatial Planner\n> michael...@tnc.org\n> (646) 647-2045 ​(Office)\n> (917) 841-5603 (Mobile)\n> (212) 997-8451 (Fax)\n>\n> nature.org\n> ​\n>\n> The Nature Conservancy\n> New York\n> New York City Program\n> 322 Eighth Avenue, 16thFloor\n> New York, NY, 10001\n> ​\n> <image001.png>\n- show quoted text -\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Call Today?",
     "name": "Ashley Jones",
     "body": "Hi Peter,\nDo you have a solid sales growth strategy in place for the next quarter? Are your sales budgets being allocated to the most effective channels? Can you measure which sales development strategies make the biggest impact in your SDR efforts? These are the questions we help our clients answer every day.\nWhen do you have 10 minutes to chat in the next week or so? I promise it will be well worth your time.\nCheers,\n--\nAshley Jones | Account Manager\nEmail: ash...@boomsourcing.com\n2940 W Maple Loop Lehi, UT 84043"
    }
   ]
  },
  {
   "name": [
    {
     "page": "A little help?",
     "name": "Ashley Jones",
     "body": "Hi Peter,\nYour profile stood out to me as the person to speak with about the current strategy for sales development over at The Open Source Project for a Network Data Access Protocol. I could be way off base here but would love to get on your calendar for a 10 minute phone call to see if we can provide a boost to your sales pipeline.\nOur clients are seeing an average of 60% cost savings by having Boomsourcing supplement their current sales development activities.\nWhen works for you in the next few days?\nIf there's someone else I should be speaking with, I'd really appreciate if you could point me in the right direction.\nThanks!\n--\nAshley Jones | Account Manager\nEmail: ash...@boomsourcing.com\n2940 W Maple Loop Lehi, UT 84043"
    },
    {
     "page": "A little help?",
     "name": "Ashley Jones",
     "body": "Hey Peter,\nI know you are probably busy but I was wondering if you saw the email I sent you a few days ago.\nHere it is again:\n-----------------------\nHey Peter, Your profile stood out to me as the person to speak with about the current strategy for sales development over at The Open Source Project for a Network Data Access Protocol. I could be way off base here but would love to get on your calendar for a 10 minute phone call to see if we can provide a boost to your sales pipeline.\nOur clients are seeing an average of 60% cost savings by having Boomsourcing supplement their current sales development strategies.\nWhen works for you in the next few days? If there's someone else I should be speaking with, I'd really appreciate if you could point me in the right direction.\nCheers!"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Build Your DockerCon Agenda: 5 Sessions Not To Miss",
     "name": "DockerCon",
     "body": "DockerCon EU is where industry leading companies come to learn how they can leverage Docker’s enterprise container platform to modernize their legacy apps, embrace digital transformation, and fund their innovation efforts. This year, GlaxoSmithKline, Splunk, MetLife and others will be sharing their stories. Build your schedule today, highlights include:\nLetting Science Drive Technology at GlaxoSmithKline\nRanjith Raghunath, GlaxoSmithKline\nLindsay Edwards, GlaxoSmithKline\nCilium - Kernel Native Security and DDOS Mitigation for Microservices with BPF\nCynthia Thomas, Covalent IO\nUsing Docker to Scale Operational Intelligence at Splunk\nMike Dickey, Splunk\nHarish Jayakumar, Docker\nContainerizing Hardware Accelerated Applications\nChelsea Mafrica, Intel\nHow Docker helps Open Doors at Assa Abloy\nJan Hëdstrom, Assa Abloy\nPatrick van der Bleek, Docker\nRegister Today\nDocker, Inc.\n144 Townsend Street\nSan Francisco, CA 94107\nConnect with us:\n© Docker 2017\nYou have received this email because you signed up to receive information and offers from the Docker ®.\nIf you would like to be excluded from further e‐mail communications, unsubscribe here."
    }
   ]
  },
  {
   "name": [
    {
     "page": "hyrax-docker",
     "name": "Gareth....@csiro.au",
     "body": "Greetings.\nI’ve returned to updating our OPeNDAP setup in CSIRO, intending to use a dockerised deployment and found that you’ve made great progress https://github.com/OPENDAP/hyrax-docker and published containers on docker hub https://hub.docker.com/r/opendap/hyrax_ncwms/ (also without ncwms). Congratulations!\nThe containers work nicely for me so far and I have some feedback that I can summarise here in point form. Possibly some can be converted into issues for either the hyrax-docker project or the upstream software component projects. I’ll conform to whatever suits you best and potentially fork hyrax-docker and submit changes back as pull requests.\nPoints follow:\n. I see tomcat is run by root in opendap/hyrax_ncwms – that should probably be changed\n. I’ll be looking for how to tune java – not asking just yet but making a note… Perhaps I need to inject a javaopts.sh\n. ncWMS performance has been slow for me. I’d value help\n. I have some experience with the travis-ci github integration as done in thredds-docker – I’d recommend adding that to the project\n. Do you plan to release separate besd, olfs and ncWMS images on dockerhub? – I guess I’d prefer to compose them – but the combined image is nice too (I should probably move to building/testing myself and using my own registry for blessed images but I’m not quite there yet).\n. Where does the custom help email (SERVER_HELP_EMAIL) get published? I see my customization hits /etc/bes/bes.conf but can’t see it in the presented web pages anywhere.\nRegards,\nGareth Williams\nCSIRO IMT Scientific Computing Services\nPrivate Bag 10\nClayton South Vic 3170"
    },
    {
     "page": "hyrax-docker",
     "name": "Nathan Potter",
     "body": "> On Aug 30, 2017, at 11:15 PM, <Gareth.Williams@csiro.au> <Gareth.Williams@csiro.au> wrote:\n>\n> Greetings.\n>\n> I’ve returned to updating our OPeNDAP setup in CSIRO, intending to use a dockerised deployment and found that you’ve made great progresshttps://github.com/OPENDAP/hyrax-docker and published containers on docker hub https://hub.docker.com/r/opendap/hyrax_ncwms/ (also without ncwms). Congratulations!\n>\n> The containers work nicely for me so far and I have some feedback that I can summarise here in point form. Possibly some can be converted into issues for either the hyrax-docker project or the upstream software component projects. I’ll conform to whatever suits you best and potentially fork hyrax-docker and submit changes back as pull requests.\n>\n> Points follow:\n>\n> . I see tomcat is run by root in opendap/hyrax_ncwms – that should probably be changed\nI tried, but gave up in frustration. It seems the nuances of getting multiple users configured in Docker land has so far escaped me. The Hyrax combined image requires the root user to launch the BES. And, as you pointed out, a different, less privileged user, to run Tomcat. Now it is of interest that although the Tomcat in the combined image is a YUM installed Tomcat from some CentOS YUM repo none of the system “service” stuff is available so I was unable to launch using:\nservice tomcat start;\nAnd had to utilize a direct invocation of Tomcat:\n/usr/libexec/tomcat/server start > /var/log/tomcat/console.log 2>&1 &\nWhich runs it as root. There is a “tomcat” user:\n[-bash: ~] docker exec --user tomcat hyrax_container whom\ntomcat\nHowever, if in the the startup script “entry_point.sh”, I try this:\nsu tomcat /usr/libexec/tomcat/server start > /var/log/tomcat/console.log 2>&1 &\nIt generates the error: \"This account is currently not available.”\nWhich seems right because the tomcat user has a nologin shell in /etc/passwd:\ntomcat:x:91:91:Apache Tomcat:/usr/share/tomcat:/sbin/nologin\nAnd I am not inclined to change the tomcat user to one with a login shell…\nThoughts?\nI also tried using the UNIDATA “secure” Tomcat (which looks a lot like a simple YUM installed Tomcat run in secure mode) but they have moved to Tomcat 8.5+ and somewhere between 8.32 and now Tomcat updated the default rules for security manager and now neither the OLFS or ncWMS are able to run when Tomcat is in secure mode: Both applications utilize calls to System.getProperty() which is now verboten. In order for the OLFS and ncWMS to run in secured Tomcat we will need to develop the appropriate permissions entries and poke them into $CATALINA_BASE/conf/catalina.policy during the build.\n>\n> . I’ll be looking for how to tune java – not asking just yet but making a note… Perhaps I need to inject a javaopts.sh\nI have not looked into this at all so I don’t have much to say here beyond that we should do it and make it easy :) Probably something we could add to the “local.env” (a.k.a. a localized version of hyrax_docker/hyrax-1.13.5/local.env.orig) which is already employed by the docker-compose stuff…\n>\n> . ncWMS performance has been slow for me. I’d value help\nCan you tell me more? I’m not sure how to help…\nIt may be resource starved (see previous point) but beyond that I’m not sure what to suggest.\n>\n> . I have some experience with the travis-ci github integration as done in thredds-docker – I’d recommend adding that to the project\nWe would like that too, but time/$$/time/$$ and all that… Pull request?\nNote: If it matters w.r.t. Travis, my plan is to organize the hyrax-docker project and published images the way Tomcat does\nhttps://hub.docker.com/r/_/tomcat/\nhttps://github.com/docker-library/tomcat\n>\n> . Do you plan to release separate besd, olfs and ncWMS images on dockerhub? – I guess I’d prefer to compose them – but the combined image is nice too (I should probably move to building/testing myself and using my own registry for blessed images but I’m not quite there yet).\nWe discussed this, but for the moment are leaving the building of the these images as an exercise for the user :)\nWe are not opposed to publishing them, again a question of time/$$/time/$$...\nFor now they should build easily from:\nhttps://github.com/OPENDAP/hyrax-docker/tree/master/hyrax-1.13.5/olfs\nhttps://github.com/OPENDAP/hyrax-docker/tree/master/hyrax-1.13.5/bes\nhttps://github.com/OPENDAP/hyrax-docker/tree/master/hyrax-1.13.5/ncWMS\nAnd there are example YML files here:\nhttps://github.com/OPENDAP/hyrax-docker/tree/master/hyrax-1.13.5\n>\n> . Where does the custom help email (SERVER_HELP_EMAIL) get published? I see my customization hits /etc/bes/bes.conf but can’t see it in the presented web pages anywhere.\nThat’s good! Seriously - That address gets returned in error messages and error pages so if you aren’t seeing it that’s a good thing. Try asking the server for something it doesn’t have - you should see the address at the end of the 404 page.\nSincerely,\nNathan\n>\n> Regards,\n>\n> Gareth Williams\n> CSIRO IMT Scientific Computing Services\n> Private Bag 10\n> Clayton South Vic 3170\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "hyrax-docker",
     "name": "Gareth....@csiro.au",
     "body": "Thanks Nathan.\nIt is late here and I'll just respond on root and services.\nA school of thought (a good one I think) says docker containers are for microservices with one per container and no init/service setup. Each container should just run one process (pid=1!) in the foreground, preferably not as root. It is not a full VM - that is part of the benefit, with a smaller range of vulnerabilities. This does not suit some software but bes and tomcat run fine in a foreground mode so can match well. Having the multi-component hyrax container breaks this ideal but it doesn't really matter (and simplifies things in one way, but requiring backgrounding and sleep loop and so on).\nSetting the user is a bit tricky, especially if you have an entrypoint that needs to do some things as root then drop to non-root. There are a number of non-su options available - one is gosu https://github.com/tianon/gosu - and there are many docker projects that use it. It should be easy enough to incorporate it to run tomcat as user tomcat.\nRegards,\nGareth\n- show quoted text -"
    },
    {
     "page": "hyrax-docker",
     "name": "gaj...@gmail.com",
     "body": "following up - I found that setting JAVA_OPTS in the environment (in docker run options) is picked up by tomcat. It would be nice if tuning java was more than guess-work. Do you know how I might determine if too little or too much memory is being used? I'm currently passing: JAVA_OPTS=\"-Djava.security.egd=file:/dev/urandom -Djava.awt.headless=true -Xms2G -Xms128m -XX:+UseConcMarkSweepGC\" Probably not all of that is useful.\nThis (probably just the 2G) has helped to avoid an issue where selecting the 'viewers' link gives an empty response in some cases. This seems to be repeatable with some of the testdata nc content (and the current opendap/hyrax_ncwms:latest from docker hub\nMy test setup is not publicly visible but the URLs are like: http://hyrax.csiro.au/opendap/viewers/viewers?dapService=/opendap/hyrax&datasetID=/test-data/nc/coads_climatology.nc as presented on http://hyrax.csiro.au/opendap/test-data/nc/contents.html (That site works fine but also has slow wms. it is the site I'm working on replacing).\nThe 'empty' response yields 200 and <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nNo errors are logged - but the request is logged and the bes logs making a ddx response.\nI'll submit an issue to hyrax-docker regarding tuning java when ncwms is wanted.\nThe java 'tuning' has helped with the wms performance - I think I'll go and assign more memory to the old setup now!\nregards,\nGareth"
    },
    {
     "page": "hyrax-docker",
     "name": "gaj...@gmail.com",
     "body": "That should read -Xmx2G I noticed that as I drafted this post and fixed it in my environment before posting, but did not fix the post!\nGareth\ncross-ref: https://github.com/OPENDAP/hyrax-docker/issues/2\nOn Friday, September 8, 2017 at 5:16:29 PM UTC+10, gaj...@gmail.com wrote:\nfollowing up - I found that setting JAVA_OPTS in the environment (in docker run options) is picked up by tomcat. It would be nice if tuning java was more than guess-work. Do you know how I might determine if too little or too much memory is being used? I'm currently passing: JAVA_OPTS=\"-Djava.security.egd=file:/dev/urandom -Djava.awt.headless=true -Xms2G -Xms128m -XX:+UseConcMarkSweepGC\" Probably not all of that is useful.\n-snip-"
    },
    {
     "page": "hyrax-docker",
     "name": "jgallagher",
     "body": "On Sep 8, 2017, at 01:40, gaj...@gmail.com wrote:\nThat should read -Xmx2G I noticed that as I drafted this post and fixed it in my environment before posting, but did not fix the post!\nThat looks pretty interesting. is the first part of the JAVA_OPTS '-Djava.security.egd=file:/dev/urandom’?\nJames\nGareth\ncross-ref: https://github.com/OPENDAP/hyrax-docker/issues/2\nOn Friday, September 8, 2017 at 5:16:29 PM UTC+10, gaj...@gmail.com wrote:\nfollowing up - I found that setting JAVA_OPTS in the environment (in docker run options) is picked up by tomcat. It would be nice if tuning java was more than guess-work. Do you know how I might determine if too little or too much memory is being used? I'm currently passing: JAVA_OPTS=\"-Djava.security.egd=file:/dev/urandom -Djava.awt.headless=true -Xms2G -Xms128m -XX:+UseConcMarkSweepGC\" Probably not all of that is useful.\n-snip-\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "hyrax-docker",
     "name": "jgallagher",
     "body": "On Sep 8, 2017, at 01:16, gaj...@gmail.com wrote:\nfollowing up - I found that setting JAVA_OPTS in the environment (in docker run options) is picked up by tomcat. It would be nice if tuning java was more than guess-work. Do you know how I might determine if too little or too much memory is being used? I'm currently passing: JAVA_OPTS=\"-Djava.security.egd=file:/dev/urandom -Djava.awt.headless=true -Xms2G -Xms128m -XX:+UseConcMarkSweepGC\" Probably not all of that is useful.\nLooking into these options a bit I found (and I’m summarizing these to make a record, because I didn’t know about them):\n_-Djava.security.egd=file:/dev/urandom_ is a way to speed up some security -related operations. See the info below. You probably know that weak randomness is one (of the best?) way to break encryption, but the crypto might not be that important.\nhttps://stackoverflow.com/questions/137212/how-to-solve-slow-java-securerandom\nYou should be able to select the faster-but-slightly-less-secure /dev/urandom on Linux using:\n-Djava.security.egd=file:/dev/urandom\nHowever, this doesn't work with Java 5 and later (Java Bug 6202721). The suggested work-around is to use:\n-Djava.security.egd=file:/dev/./urandom\n(note the extra /./)\nFor _java.awt.headless=true_\nHeadless mode is a system configuration in which the display device, keyboard, or mouse is lacking. Sounds unexpected, but actually you can perform different operations in this mode, even with graphic data.\nWhere it is applicable? Let's say that your application repeatedly generates a certain image, for example, a graphical authorization code that must be changed every time a user logs in to the system. When creating an image, your application needs neither the display nor the keyboard. Let's assume now that you have a mainframe or dedicated server on your project that has no display device, keyboard, or mouse. The ideal decision is to use this environment's substantial computing power for the visual as well as the nonvisual features. An image that was generated in the headless mode system then can be passed to the headful system for further rendering.\n_UseConcMarkSweepGC_ There’s lots of talk about garbage collection. Always has been… But this option combines two techniques that (seem) to make good use of parallelism.\nSee http://www.fasterj.com/articles/oraclecollectors1.shtml and generally Google about it.\nThis (probably just the 2G) has helped to avoid an issue where selecting the 'viewers' link gives an empty response in some cases. This seems to be repeatable with some of the testdata nc content (and the current opendap/hyrax_ncwms:latest from docker hub\nMy test setup is not publicly visible but the URLs are like: http://hyrax.csiro.au/opendap/viewers/viewers?dapService=/opendap/hyrax&datasetID=/test-data/nc/coads_climatology.nc as presented on http://hyrax.csiro.au/opendap/test-data/nc/contents.html (That site works fine but also has slow wms. it is the site I'm working on replacing).\nThe 'empty' response yields 200 and <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nNo errors are logged - but the request is logged and the bes logs making a ddx response.\nI'll submit an issue to hyrax-docker regarding tuning java when ncwms is wanted.\nGreat. Thanks.\nI think you’re right, it’s probably the memory size that makes the most difference. I would be cautious about the weaker randomness.\nJames\nThe java 'tuning' has helped with the wms performance - I think I'll go and assign more memory to the old setup now!\nregards,\nGareth\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "hyrax-docker",
     "name": "gaj...@gmail.com",
     "body": "Thanks James,\nI think I took most of the options from previous recommendations or 'discoveries' - mostly from working with TDS/thredds. Some may be defaults - but defaults may change over time. It always seems tricky to decide when to be explicit (And think you help to control stability) and when to only specify what you need (and hope to get incidental improvements for free, and test for and/or hope not to get bitten by troublesome upstream default changes).\nWith the current thredds-docker using /dev/urandom made a major startup time difference. It did not need /dev/./urandom - that may not apply to openjdk. I've not actually had startup time problems with hyrax and probably should have left that out of my post - but there was some point in noting that this is a generic method for passing java options.\nRegarding security, there is very mixed advice on the current merits or random vs urandom. I think in my case it doesn't matter but can appreciate the need to be cautious in general and specifically when giving general advice on an open forum.\nMostly I wanted to share what I'd learned and to get documentation improvements into hyrax-docker - the containerization offers a nest way of delivering environment customization alongside specific software. Documenting use of java options 'as code' would be a valuable improvement.\nGareth\n- show quoted text -"
    },
    {
     "page": "hyrax-docker",
     "name": "jgallagher",
     "body": "On Sep 11, 2017, at 01:58, gaj...@gmail.com wrote:\nThanks James,\nI think I took most of the options from previous recommendations or 'discoveries' - mostly from working with TDS/thredds. Some may be defaults - but defaults may change over time. It always seems tricky to decide when to be explicit (And think you help to control stability) and when to only specify what you need (and hope to get incidental improvements for free, and test for and/or hope not to get bitten by troublesome upstream default changes).\nWith the current thredds-docker using /dev/urandom made a major startup time difference. It did not need /dev/./urandom - that may not apply to openjdk. I've not actually had startup time problems with hyrax and probably should have left that out of my post - but there was some point in noting that this is a generic method for passing java options.\nRegarding security, there is very mixed advice on the current merits or random vs urandom. I think in my case it doesn't matter but can appreciate the need to be cautious in general and specifically when giving general advice on an open forum.\nMostly I wanted to share what I'd learned and to get documentation improvements into hyrax-docker - the containerization offers a nest way of delivering environment customization alongside specific software. Documenting use of java options 'as code' would be a valuable improvement.\nAgree. Thanks for send that info along.\nAre you going to fork our hyrax-docker repo and work with it?\nJames\n- show quoted text -\n--\nJames Gallagher\njgall...@opendap.org"
    },
    {
     "page": "hyrax-docker",
     "name": "Gareth....@csiro.au",
     "body": "+ Agree. Thanks for send that info along.\n+ Are you going to fork our hyrax-docker repo and work with it?\n+ James\nI have no immediate plans to fork. It is working sufficiently for me as is and I can maintain my own environment customizations. It would be good to have the documentation/guidance updated for others but I don’t think there is particular value in me forking just for that.\nPerhaps add text like:\nIf JAVA_OPTS is defined in the container runtime environment, tomcat/olfs/ncWMS will include those options in the service start up. There are many options that could be passed. Of particular note is –Xmx which sets the amount of memory available. ncWMS will not work properly with low memory limits. JAVA_OPTS can be set by normal methods: on the docker run command line, or in docker-compose configuration or in your own container layer if you build on the provided containers.\nSorry I’ve not been very precise in that test but hopefully it is a good start.\nYou might also add an env setting to the Dockerfile(s) as implicit embedded documentation that the JAVA_OPTS setting is useful and expected to be tuned.\nGareth"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Open Source Project For A Network Introduction to Flipcause",
     "name": "Joan Ramirez",
     "body": "Hi [not, I hope you’re having a great day! I came across Open Source Project For A Network a few days ago and I think we can help your organization with fundraising.\nI work for Flipcause. We’re a new tech service specifically built for small and volunteer-run nonprofits. We help you save time and money by automating your fundraising, in one place, with no technical work required on your end.\nYour Flipcause subscription comes with every feature and service we have to offer (see below) which includes a dedicated, four-person Success Team who will set up your fundraising campaigns, create banners and images, modernize your website, and much, much more...\nYour Success Team will do unlimited tasks for you, on demand, at no additional cost, so you can spend more time on your mission and less time on technology!\nWould you be willing to chat with one of our community development reps to learn more? Nothing too crazy. If you are open to a quick conversation, please take a look at our demo calendar and select the best time for you by clicking HERE.\nIf you are not interested, please let me know and you will never hear from me again. :)\nThank you for your time today and I hope to hear from you soon!\nJoan Ramirez\nOutreach Representative\n311 Oak Street Ste 110\nOakland, CA 94607\nHere are the features that come with your Flipcause subscription…\nOne time and recurring online donations\nAutomated tax-deductible donation receipts\nEvent registration/ticketing\nPeer to peer fundraising\nTeam fundraising\nMembership sign up\nRaffle ticketing\nVolunteer sign up\nCrowdfunding with dynamic progress meter\nSponsorship registration\nOnline store\nMerchant Partnerships\nA dedicated, four-person Success Team with free fundraising and technical assistance\nFree, modernized website with social media integration\nUnlimited, fully customizable fundraising pages to market your campaigns\nDonor dashboard to keep track of your supporters in one place\nEasy to transfer fundraising activity into your accounting system\nSortable donor data for targeted mass email communication\n--\nJoan Ramirez | Flipcause\nCommunity Outreach Representative\n311 Oak Street Ste 110\nOakland, CA 94607\noffice: (800) 523-1950 ext. 3\nwww.flipcause.com"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Fwd: Error server OPENDATD",
     "name": "Javier Peña",
     "body": "Dear Mr./Mrs,\nI am having problems to access to the server OPENDAP to download the following subsets of MERRA2 data:\n- inst6_3d_ana_Nv: https://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2I6NVANA.5.12.4/contents.html\n- inst6_3d_ana_Np: https://goldsmr5.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2I6NPANA.5.12.4/contents.html\nThe problem is when I try to access I get the following prompt:\nOUCH!\nSomething Bad Happened On This Server.\nThe specific error message associated with your request was:\nBES Client Failed To Start. Message: 'Could not connect to host localhost on port 10022. Connection refused (Connection refused)'\nIf you think that the server is broken (that the URL you submitted should have worked), then please contact the OPeNDAP user support coordinator at: admin.ema...@your.domain.name\nNevertheless, I am able to access to another servers without problem:\n- tavg1_2d_slv_Nx: https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/contents.html\n- tavg1_2d_ocn_Nx: https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXOCN.5.12.4/contents.html\nI hope you can help me to solve this problem.\nThank you very much in advance for your help.\nYours sincerely,\nJavier Peña\n--\nJavier Peña\nEREDA S.L.U.\nPaseo del Marqués de Monistrol 7, 28011 Madrid\nTel: +34.915014755\nFax: +34.915014756\nwww.ereda.com\nEn cumplimiento de la Ley Orgánica 15/1999 de Protección de Datos Personales, le informamos que sus datos están incorporados a nuestros ficheros, con la finalidad de mantener relaciones profesionales y/o comerciales. Si lo desea puede ejercer los derechos de acceso, rectificación, cancelación y oposición de sus datos dirigiéndose a: EREDA S.L.U., Paseo Marqués de Monistrol 7, 28011 Madrid, o bien escribiendo a la dirección de correo electrónico ereda_info@ereda.com, poniendo en el asunto “BAJA”. El contenido de este correo electrónico/fax y sus documentos adjuntos va dirigido únicamente a la persona o entidad que se muestra como destinatario y puede contener datos confidenciales o privilegiados. Si Ud. no es el destinatario y recibe este mail/fax por error, rogamos se ponga en contacto con nosotros y lo destruya con todos sus documentos adjuntos sin leerlos ni hacer ningún uso de los datos que en ellos figuren, ateniéndose a las consecuencias que de un uso indebido de dichos datos puedan derivarse.\nIn compliance with the provisions of Organic Law 15/1999 of 13 December 1999, relating to the Protection of Personal Data, EREDA S.L.U. guarantees the security and confidentiality of the data provided. The aim of these personal details collection is the commercial contact procedure, advertising and commercial research, all of them coincident purposes with those declared before the Information Protection Spanish Agency. Consequently, you, as information holder, give permission and authorisation to the File Responsible to include them in the ut supra detailed file. You declare that you are informed about the detailed conditions in the present clause, as well as you promise to update your details, and, in any case, you will be able to freely exercise the rights of access, rectification, cancellation and opposition (always in accordance with the assumption considered with the legislation currently in force) by writing to ereda_info@ereda.com. If you are not the intended recipient, you are hereby notified that any disclosure, copying, distribution, or use of the information contained herein (including any reliance thereon) is STRICTLY PROHIBITED. If you received this transmission in error, please immediately contact the sender and destroy the material in its entirety, whether in electronic or hard copy format.\nLibre de virus. www.avast.com\n--\nJavier Peña\nEREDA S.L.U.\nPaseo del Marqués de Monistrol 7, 28011 Madrid\nTel: +34.915014755\nFax: +34.915014756\nwww.ereda.com\nEn cumplimiento de la Ley Orgánica 15/1999 de Protección de Datos Personales, le informamos que sus datos están incorporados a nuestros ficheros, con la finalidad de mantener relaciones profesionales y/o comerciales. Si lo desea puede ejercer los derechos de acceso, rectificación, cancelación y oposición de sus datos dirigiéndose a: EREDA S.L.U., Paseo Marqués de Monistrol 7, 28011 Madrid, o bien escribiendo a la dirección de correo electrónico ereda_info@ereda.com, poniendo en el asunto “BAJA”. El contenido de este correo electrónico/fax y sus documentos adjuntos va dirigido únicamente a la persona o entidad que se muestra como destinatario y puede contener datos confidenciales o privilegiados. Si Ud. no es el destinatario y recibe este mail/fax por error, rogamos se ponga en contacto con nosotros y lo destruya con todos sus documentos adjuntos sin leerlos ni hacer ningún uso de los datos que en ellos figuren, ateniéndose a las consecuencias que de un uso indebido de dichos datos puedan derivarse.\nIn compliance with the provisions of Organic Law 15/1999 of 13 December 1999, relating to the Protection of Personal Data, EREDA S.L.U. guarantees the security and confidentiality of the data provided. The aim of these personal details collection is the commercial contact procedure, advertising and commercial research, all of them coincident purposes with those declared before the Information Protection Spanish Agency. Consequently, you, as information holder, give permission and authorisation to the File Responsible to include them in the ut supra detailed file. You declare that you are informed about the detailed conditions in the present clause, as well as you promise to update your details, and, in any case, you will be able to freely exercise the rights of access, rectification, cancellation and opposition (always in accordance with the assumption considered with the legislation currently in force) by writing to ereda_info@ereda.com. If you are not the intended recipient, you are hereby notified that any disclosure, copying, distribution, or use of the information contained herein (including any reliance thereon) is STRICTLY PROHIBITED. If you received this transmission in error, please immediately contact the sender and destroy the material in its entirety, whether in electronic or hard copy format."
    }
   ]
  },
  {
   "name": [
    {
     "page": "Need help to get the compatible version of opendap for the matlab 2013a",
     "name": "serge tomety",
     "body": "To whom it may concern,\nI am Serge Tomety, PhD student at UCT, I would like to use opendap to download data.\nfor that I Dowloaded loaddap-3.7.3 and install it on the computer. But I got the message that :\n<<loaddap that matlab tried to run was not compatible with your computer.>>\nCould you help me to get the compatible version.\nThank you in advanced.\nSerge"
    }
   ]
  },
  {
   "name": [
    {
     "page": "Developer documentation for writing data handlers",
     "name": "Chris Piker",
     "body": "Hi OPeNDAP\nI have an in-house data system (das2.org) for streaming planetary\nmagnetosphere data sets that is based off of small server side programs\nwhich supply time-reduced data streams. It's primarily used to\nprovide variable-resolution datasets to Autoplot. (http://autoplot.org)\nI'm pondering upgrades to my current system and see no reason to invent\nyet another data server if I don't have to. The Hyrax server looks\ninteresting and I am wondering what kind of glue code I would need to\nwrite in order to re-use my reader programs as a back end for Hyrax.\nAlso, have you ever kicked around providing a simple multi-server\ncatalog service to allow for easier collaboration across groups, or\nis that far the scope of OPeNDAP ?\nThanks!\n--\nChris Piker"
    },
    {
     "page": "Developer documentation for writing data handlers",
     "name": "jgallagher",
     "body": "On Aug 18, 2017, at 00:45, Chris Piker <chris...@uiowa.edu> wrote:\nHi OPeNDAP\nI have an in-house data system (das2.org) for streaming planetary\nmagnetosphere data sets that is based off of small server side programs\nwhich supply time-reduced data streams. It's primarily used to\nprovide variable-resolution datasets to Autoplot. (http://autoplot.org)\nI'm pondering upgrades to my current system and see no reason to invent\nyet another data server if I don't have to. The Hyrax server looks\ninteresting and I am wondering what kind of glue code I would need to\nwrite in order to re-use my reader programs as a back end for Hyrax.\nSorry for the delay in getting back to you…\nIt really depends on the way you’re storing the data now. Generally, it would be C++ code, but not very hard code. You’d need to fill in values in a C++ object and the server takes care of the rest. Until you get to reading actual values, then you’ll need to provide a handful of methods.\nLet us know a bit more about your data and it’s existing access tools and we’ll help you out.\nAlso, have you ever kicked around providing a simple multi-server\ncatalog service to allow for easier collaboration across groups, or\nis that far the scope of OPeNDAP ?\nWe have, but so far it’s been out of scope. It would be possible to write THREDDS catalog files for several servers and host them on some HTTPd.\nNathan, can you chime in? Am I way off base here?\nJames\nThanks!\n--\nChris Piker\n--\nJames Gallagher\njgall...@opendap.org"
    }
   ]
  },
  {
   "name": [
    {
     "page": "ProblemAccessing OpenDap Data",
     "name": "Bailey Palmer",
     "body": "Hello,\nI'd like to access the following data :https://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_L3/TRMM_3B43/1998/3B43.19980101.7.HDF?precipitation[0:1:1439][0:1:399],relativeError[0:1:1439][0:1:399],gaugeRelativeWeighting[0:1:1439][0:1:399],nlon[0:1:1439],nlat[0:1:399]\nI registered, and followed the instructions to use my email as the username and password when prompted by the system after requesting ASCII data, but it simply reopens the username window each time I type the username and password. Could you assist?\nThank you,\nBailey Palmer\nFoundation Fellow, University of Georgia '18\nEconomics"
    },
    {
     "page": "ProblemAccessing OpenDap Data",
     "name": "Nathan Potter",
     "body": "Hi Bailey,\nIn my browser the URL you provided fails because it attempts to download the underlying HDF file and that access is disabled on the server.\nWhat software did you use to access this URL, a browser? Or something else?\nIf you provide a DAP protocol suffix to the URL, for example “.dds”, you should get back a response. Here’s a curl example:\ncurl \"https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3B43/1998/3B43.19980101.7.HDF.dds?precipitation%5B0:1:1439%5D%5B0:1:399%5D,relativeError%5B0:1:1439%5D%5B0:1:399%5D,gaugeRelativeWeighting%5B0:1:1439%5D%5B0:1:399%5D,nlon%5B0:1:1439%5D,nlat%5B0:1:399%5D\"\nDataset {\nFloat32 precipitation[nlon = 1440][nlat = 400];\nFloat32 relativeError[nlon = 1440][nlat = 400];\nInt32 gaugeRelativeWeighting[nlon = 1440][nlat = 400];\nFloat32 nlon[nlon = 1440];\nFloat32 nlat[nlat = 400];\n} 3B43.19980101.7.HDF;\nNote the suffix “.dds” applied to the end if the URL path, just before the question mark. Also - to use curl you have to follow the configuration steps detailed here:\nhttp://docs.opendap.org/index.php/DAP_Clients_-_Authentication\nIf that doesn’t work then something may be going on with your credentials or your “Approved Applications” in Earthdata Login.\nYou can specify how you want the data returned by changing the URL path suffix:\n.dds - Syntactic Metadata (See above example)\n.das - Semantic Metadata\n.dods - DAP2 Binary Response\n.nc - Netcdf-3\n.nc4 - Netcdf-4\nThere are more formats available if none of these suit you.\nSincerely,\nNathan\n- show quoted text -\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "ProblemAccessing OpenDap Data",
     "name": "Bailey Palmer",
     "body": "Hi Nathan,\nThanks for your quick response! I wasn't very clear with my question, so I'll explain a bit more.\nI went to the following webpage to download the global rainfall data:\nhttps://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3B43/1998/3B43.19980101.7.HDF.html\nI clicked all of the possible variables (precipitation, lat, long etc) and tried to download the ASCII data by clicking on 'get ASCII'. It then brings up a new tab with an authentication pop-up, asking for my username and password. According to this page, I should use my registered email as the username and password. I created an account and verified my email address, so I'm not sure whats wrong. Am I accessing the data wrong?\nThank you!\nBailey Palmer\nFoundation Fellow, University of Georgia '18\nArabic and Economics\nFrom: Nathan Potter <n...@opendap.org>\nSent: Tuesday, August 22, 2017 10:01:32 AM\nTo: Bailey Palmer\nCc: Nathan Potter; sup...@opendap.org\nSubject: Re: [support] ProblemAccessing OpenDap Data\nHi Bailey,\nIn my browser the URL you provided fails because it attempts to download the underlying HDF file and that access is disabled on the server.\nWhat software did you use to access this URL, a browser? Or something else?\nIf you provide a DAP protocol suffix to the URL, for example “.dds”, you should get back a response. Here’s a curl example:\ncurl \"https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3B43/1998/3B43.19980101.7.HDF.dds?precipitation%5B0:1:1439%5D%5B0:1:399%5D,relativeError%5B0:1:1439%5D%5B0:1:399%5D,gaugeRelativeWeighting%5B0:1:1439%5D%5B0:1:399%5D,nlon%5B0:1:1439%5D,nlat%5B0:1:399%5D\"\nDataset {\nFloat32 precipitation[nlon = 1440][nlat = 400];\nFloat32 relativeError[nlon = 1440][nlat = 400];\nInt32 gaugeRelativeWeighting[nlon = 1440][nlat = 400];\nFloat32 nlon[nlon = 1440];\nFloat32 nlat[nlat = 400];\n} 3B43.19980101.7.HDF;\nNote the suffix “.dds” applied to the end if the URL path, just before the question mark. Also - to use curl you have to follow the configuration steps detailed here:\nhttp://docs.opendap.org/index.php/DAP_Clients_-_Authentication\nDAP Clients - Authentication - OPeNDAP Documentation\ndocs.opendap.org\n1 Overview. Many users access DAP servers using a browser as their primary software interface. However there is also a growing group of users that utilize either:\nIf that doesn’t work then something may be going on with your credentials or your “Approved Applications” in Earthdata Login.\nYou can specify how you want the data returned by changing the URL path suffix:\n.dds - Syntactic Metadata (See above example)\n.das - Semantic Metadata\n.dods - DAP2 Binary Response\n.nc - Netcdf-3\n.nc4 - Netcdf-4\nThere are more formats available if none of these suit you.\nSincerely,\nNathan\n> On Aug 21, 2017, at 8:06 PM, Bailey Palmer <bailey...@uga.edu> wrote:\n>\n> Hello,\n>\n> I'd like to access the following data :https://disc2.gesdisc.eosdis.nasa.gov:443/opendap/TRMM_L3/TRMM_3B43/1998/3B43.19980101.7.HDF?precipitation[0:1:1439][0:1:399],relativeError[0:1:1439][0:1:399],gaugeRelativeWeighting[0:1:1439][0:1:399],nlon[0:1:1439],nlat[0:1:399]\n>\n> I registered, and followed the instructions to use my email as the username and password when prompted by the system after requesting ASCII data, but it simply reopens the username window each time I type the username and password. Could you assist?\n>\n> Thank you,\n>\n> Bailey Palmer\n> Foundation Fellow, University of Georgia '18\n> Economics\n= = =\nNathan Potter ndp at opendap.org\nOPeNDAP, Inc. +1.541.231.3317"
    },
    {
     "page": "ProblemAccessing OpenDap Data",
     "name": "Nathan Potter",
     "body": "Hi Bailey,\nI think the issue is that you have not yet \"authorized the application”. In this context what that means is that you have to login to your Earthdata Login account using a browser and specifically permit this data service: \"NASA GESDISC DATA ARCHIVE” to access your login profile. Really. It’s a thing.\nWhat you do:\nWith your browser:\n- Go here: https://urs.earthdata.nasa.gov/profile\n- Click the link/tab called “My Applications”\n- Click the “Approve More Applications” button.\n- Search for \"NASA GESDISC DATA ARCHIVE”\n- Approve that application.\n- Wait 5 minutes.\n- Try to do all the stuff you did that led you to the authentication loop again.\nHopefully this changes something! Please let me know.\nSincerely,\nNathan\n> On Aug 22, 2017, at 8:23 AM, Bailey Palmer <bailey...@uga.edu> wrote:\n>\n> Hi Nathan,\n>\n> Thanks for your quick response! I wasn't very clear with my question, so I'll explain a bit more.\n>\n> I went to the following webpage to download the global rainfall data:\n> https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L3/TRMM_3B43/1998/3B43.19980101.7.HDF.html\n> I clicked all of the possible variables (precipitation, lat, long etc) and tried to download the ASCII data by clicking on 'get ASCII'. It then brings up a new tab with an authentication pop-up, asking for my username and password. According to this page, I should use my registered email as the username and password. I created an account and verified my email address, so I'm not sure whats wrong. Am I accessing the data wrong?\n>\n> Thank you!\n>\n> Bailey Palmer\n> Foundation Fellow, University of Georgia '18\n> Arabic and Economics\n- show quoted text -"
    }
   ]
  },
  {
   "name": [
    {
     "page": "opendap/hyrax is now public",
     "name": "no-r...@notify.docker.com",
     "body": "Your repository opendap/hyrax was just made public. Thank you for sharing it on Docker Hub. If this was done by mistake, please fix the repository privacy setting.\nThank you,\nDocker Hub"
    }
   ]
  },
  {
  },
  {
  }
 ]
}